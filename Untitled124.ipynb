{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMm90Uh7UyKUVadIH+5jDJl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalki81000/NEURAL-NETWORK-ASSIGNMENT-/blob/main/Untitled124.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CCN ARCHITECTURE"
      ],
      "metadata": {
        "id": "2VzJz5QBjeDd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyIxaSfAjETc"
      },
      "outputs": [],
      "source": [
        "# 1 What is a Convolutional Neural Network (CNN), and why is it used for image processing*\n",
        "A **Convolutional Neural Network (CNN)** is a type of deep learning model specifically designed to process **grid-like data structures**, such as images (which can be seen as a 2D grid of pixels). CNNs are inspired by the human visual system and are particularly effective for recognizing visual patterns directly from raw image data with minimal preprocessing.\n",
        "### üîπ Structure of a CNN\n",
        "\n",
        "A CNN typically consists of the following layers:\n",
        "\n",
        "1. **Convolutional Layer**\n",
        "\n",
        "   * Applies **filters (kernels)** to scan across the image.\n",
        "   * Each filter detects specific features (edges, textures, shapes, colors).\n",
        "   * Produces **feature maps** that highlight these patterns.\n",
        "\n",
        "2. **Activation Function (ReLU)**\n",
        "\n",
        "   * Introduces **non-linearity**, allowing the network to learn complex patterns.\n",
        "\n",
        "3. **Pooling Layer (Subsampling/Downsampling)**\n",
        "\n",
        "   * Reduces the **spatial dimensions** of the feature maps.\n",
        "   * Common methods: **Max Pooling** (keeps the strongest feature) or **Average Pooling**.\n",
        "   * Helps make the model more efficient and less sensitive to small shifts in the image.\n",
        "\n",
        "4. **Fully Connected Layer (Dense Layer)**\n",
        "\n",
        "   * Flattens the extracted features into a vector.\n",
        "   * Connects to a standard neural network for classification (e.g., \"dog\" vs. \"cat\").\n",
        "\n",
        "5. **Output Layer**\n",
        "\n",
        "   * Produces final predictions using a softmax (for multi-class) or sigmoid (for binary classification).\n",
        "### üîπ Why CNNs are used for Image Processing\n",
        "\n",
        "1. **Automatic Feature Extraction**\n",
        "\n",
        "   * CNNs learn filters automatically, eliminating the need for manual feature engineering (like edges, corners, or textures).\n",
        "\n",
        "2. **Translation Invariance**\n",
        "\n",
        "   * Features like eyes, noses, or ears in images can be recognized regardless of their position, thanks to convolution + pooling.\n",
        "\n",
        "3. **Parameter Efficiency**\n",
        "\n",
        "   * Instead of connecting every pixel to every neuron (as in a fully connected network), CNNs use small filters.\n",
        "   * This reduces the number of parameters, making training faster and reducing overfitting.\n",
        "\n",
        "4. **Hierarchical Feature Learning**\n",
        "\n",
        "   * Early layers detect simple features (edges, colors).\n",
        "   * Deeper layers detect complex structures (faces, objects, patterns).\n",
        "\n",
        "5. **State-of-the-Art Accuracy**\n",
        "\n",
        "   * CNNs achieve **high performance** in image classification, object detection, face recognition, and medical imaging"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 What are the key components of a CNN architecture*\n",
        "Great question! üöÄ\n",
        "A **Convolutional Neural Network (CNN)** architecture is made up of several key components that work together to extract features from images and make predictions. Let‚Äôs break them down step by step:\n",
        "## üîë **Key Components of a CNN Architecture**\n",
        "\n",
        "### 1. **Input Layer**\n",
        "\n",
        "* Accepts the raw image data.\n",
        "* Example: A color image of size **64√ó64** has dimensions `64√ó64√ó3` (height √ó width √ó RGB channels).\n",
        "### 2. **Convolutional Layer**\n",
        "\n",
        "* The **core building block** of CNNs.\n",
        "* Uses **filters/kernels** (small matrices, e.g., 3√ó3 or 5√ó5) that slide over the image.\n",
        "* Each filter detects specific **features** such as edges, corners, or textures.\n",
        "* Produces a **feature map** that highlights where certain patterns appeares\n",
        "### 3. **Activation Function (Non-linearity, usually ReLU)**\n",
        "\n",
        "* Applied after convolution.\n",
        "* ReLU (**Rectified Linear Unit**) replaces negative values with zero.\n",
        "* Purpose: Introduces **non-linearity**, helping the network learn complex patterns.\n",
        "### 4. **Pooling Layer (Subsampling/Downsampling)**\n",
        "\n",
        "* Reduces the **spatial size** of the feature maps while keeping important information.\n",
        "* Common methods:\n",
        "\n",
        "  * **Max Pooling**: Keeps the largest value from each region (most common).\n",
        "  * **Average Pooling**: Takes the average of values in each region.\n",
        "* Benefits:\n",
        "\n",
        "  * Makes computation more efficient.\n",
        "  * Provides **translation invariance** (features recognized even if slightly shifted).\n",
        "### 5. **Fully Connected (Dense) Layer**\n",
        "\n",
        "* After several convolution + pooling layers, the feature maps are **flattened** into a vector.\n",
        "* Connected to traditional dense layers like in standard neural networks.\n",
        "* Combines all extracted features to make predictions.\n",
        "### 6. **Output Layer**\n",
        "\n",
        "* Produces the final prediction.\n",
        "* Uses:\n",
        "\n",
        "  * **Softmax** ‚Üí for multi-class classification (e.g., dog vs. cat vs. bird).\n",
        "  * **Sigmoid** ‚Üí for binary classification (e.g., tumor vs. no tumor).\n",
        "\n",
        "### 7. **Other Important Components**\n",
        "\n",
        "* **Batch Normalization**: Normalizes activations, speeding up training and improving stability.\n",
        "* **Dropout**: Randomly drops connections during training to prevent overfitting.\n",
        "* **Residual Connections (in ResNet)**: Helps train deeper networks by avoiding vanishing gradients.\n",
        "## üìä Example: CNN Flow\n",
        "\n",
        "1. **Input** ‚Üí Image (32√ó32√ó3)\n",
        "2. **Conv Layer + ReLU** ‚Üí Detects edges\n",
        "3. **Pooling** ‚Üí Reduces size (16√ó16√ó32)\n",
        "4. **Conv Layer + ReLU** ‚Üí Detects shapes/patterns\n",
        "5. **Pooling** ‚Üí Reduces size further\n",
        "6. **Flatten** ‚Üí Converts 2D feature maps into 1D vector\n",
        "7. **Fully Connected Layer** ‚Üí Combines features\n",
        "8. **Output Layer (Softmax)** ‚Üí Classifies image into categories\n",
        "uld you like me to also **draw a simple diagram of CNN architecture** so it‚Äôs easier to visualize?\n"
      ],
      "metadata": {
        "id": "FZNktW08lXR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 What is the role of the convolutional layer in CNNs*\n",
        "The **convolutional layer** is the **core building block** of a Convolutional Neural Network (CNN).\n",
        "Its main role is to automatically **extract features** from the input image by applying **filters (kernels)**.\n",
        "## üîë **Role of the Convolutional Layer in CNNs**\n",
        "\n",
        "### 1. **Feature Extraction**\n",
        "\n",
        "* Each convolutional layer learns **filters (small matrices like 3√ó3, 5√ó5)**.\n",
        "* These filters slide (convolve) across the image.\n",
        "* As they move, they perform a **dot product** between the filter and the region of the image.\n",
        "* The result is a **feature map**, highlighting the presence of specific features such as:\n",
        "\n",
        "  * **First layers** ‚Üí edges, corners, simple textures.\n",
        "  * **Deeper layers** ‚Üí shapes, patterns, or complex objects.\n",
        "### 2. **Weight Sharing**\n",
        "\n",
        "* Instead of learning a separate weight for every pixel (like in fully connected layers), the convolutional layer uses the **same filter across the image**.\n",
        "* This drastically **reduces the number of parameters**, making CNNs efficient.\n",
        "### 3. **Preserving Spatial Structure**\n",
        "\n",
        "* Unlike flattening the image directly, convolution keeps the **spatial relationship** between pixels.\n",
        "* Helps in recognizing patterns like eyes, faces, or objects, no matter where they appear in the image.\n",
        "### 4. **Translation Invariance**\n",
        "\n",
        "* Since filters scan across the image, CNNs can recognize objects even if they are **shifted or rotated slightly**.\n",
        "* Example: A cat in the center or at the corner of the picture can still be detected.\n",
        "### 5. **Hierarchical Learning**\n",
        "\n",
        "* By stacking multiple convolutional layers:\n",
        "\n",
        "  * **Lower layers** learn low-level features (edges, textures).\n",
        "  * **Higher layers** learn high-level features (faces, objects, categories).\n",
        "\n",
        "‚úÖ **In short:**\n",
        "The **convolutional layer** acts as the **feature detector** of CNNs. It scans the input image with filters, learns to recognize important patterns, reduces computation via weight sharing, and builds a **hierarchical representation** of visual data.\n"
      ],
      "metadata": {
        "id": "fOnVxC6Xl89h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 What is a filter (kernel) in CNNs*\n",
        "Great question! üåü\n",
        "In **Convolutional Neural Networks (CNNs)**, a **filter (also called a kernel)** is a small matrix of numbers (weights) that slides over the input image to detect specific **features** such as edges, textures, shapes, or patterns.\n",
        "## üîë **What is a Filter/Kernel?**\n",
        "\n",
        "* A **filter** is usually much smaller than the input image (e.g., 3√ó3, 5√ó5).\n",
        "* It moves (or **convolves**) across the image, performing a **dot product** between the filter values and the local region of the image.\n",
        "* The result is stored in a **feature map** (or activation map), which highlights where certain features appear in the image.\n",
        "## üîé **Example**\n",
        "\n",
        "Suppose we have a **3√ó3 filter** like this (used for edge detection):\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "-1 & -1 & -1 \\\\\n",
        "0 & 0 & 0 \\\\\n",
        "1 & 1 & 1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "* If we apply this filter to an image, it will highlight **horizontal edges** (where pixel intensity changes sharply from top to bottom).\n",
        "* Different filters can detect **vertical edges, diagonal lines, curves, textures, or color patterns**.\n",
        "## üéØ **Key Roles of Filters in CNNs**\n",
        "\n",
        "1. **Feature Detection** ‚Üí Each filter specializes in detecting a certain feature (like edges, corners, or shapes).\n",
        "2. **Weight Sharing** ‚Üí The same filter is applied across the whole image, making CNNs computationally efficient.\n",
        "3. **Hierarchical Learning** ‚Üí\n",
        "\n",
        "   * Early layers: filters detect simple features (edges, lines).\n",
        "   * Deeper layers: filters detect complex patterns (eyes, faces, objects).\n",
        "\n",
        "‚úÖ **In short:**\n",
        "A **filter (kernel)** in CNNs is a small matrix of weights that scans over the input to detect features. Each filter extracts a different kind of feature, and together they allow CNNs to understand and recognize complex images.\n"
      ],
      "metadata": {
        "id": "bI1PO5NknIOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 What is pooling in CNNs, and why is it important*\n",
        "Excellent question! üöÄ\n",
        "\n",
        "In **Convolutional Neural Networks (CNNs)**, **pooling** is a **downsampling (subsampling) operation** applied to feature maps. Its main purpose is to **reduce the spatial size** of the data while keeping the most important information.\n",
        "## üîë **What is Pooling?**\n",
        "\n",
        "* After a convolutional layer extracts features, pooling **shrinks the feature map**.\n",
        "* It works by sliding a small window (like 2√ó2 or 3√ó3) across the feature map and summarizing the values in that region.\n",
        "\n",
        "### Common types of pooling:\n",
        "\n",
        "1. **Max Pooling**\n",
        "\n",
        "   * Takes the **maximum value** from each region.\n",
        "   * Example:\n",
        "\n",
        "     $$\n",
        "     \\begin{bmatrix}\n",
        "     1 & 3 \\\\\n",
        "     2 & 4\n",
        "     \\end{bmatrix}\n",
        "     \\; \\rightarrow \\; 4\n",
        "     $$\n",
        "   * Keeps the strongest feature (most commonly used).\n",
        "\n",
        "2. **Average Pooling**\n",
        "\n",
        "   * Takes the **average value** of each region.\n",
        "   * Smoother, but less common in modern CNNs.\n",
        "## üéØ **Why is Pooling Important?**\n",
        "\n",
        "1. **Reduces Dimensions (Downsampling)**\n",
        "\n",
        "   * Makes the network **smaller and faster** by reducing the number of computations.\n",
        "   * Prevents memory overload when dealing with large images.\n",
        "\n",
        "2. **Prevents Overfitting**\n",
        "\n",
        "   * By reducing complexity, pooling helps the model generalize better.\n",
        "\n",
        "3. **Provides Translation Invariance**\n",
        "\n",
        "   * Features can still be detected even if the object shifts slightly in the image.\n",
        "   * Example: A cat in the top-left or bottom-right corner can still be recognized.\n",
        "\n",
        "4. **Focuses on Important Features**\n",
        "\n",
        "   * Max pooling ensures only the **most important features** are kept.\n",
        "## üìä Example Workflow\n",
        "\n",
        "1. Input image ‚Üí Convolution (detect edges) ‚Üí **Pooling** (reduce size).\n",
        "2. Example: A 4√ó4 feature map with 2√ó2 max pooling ‚Üí becomes 2√ó2.\n",
        "3. Result: Smaller feature maps but still preserve key information.\n",
        "‚úÖ **In short:**\n",
        "**Pooling** in CNNs is a way to **reduce the size of feature maps** by keeping only the most important information. It makes the network more efficient, robust to small shifts in the input, and helps prevent overfitting.\n"
      ],
      "metadata": {
        "id": "bUtCfJsMnnyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 What are the common types of pooling used in CNNs*\n",
        "Great question again! üéØ\n",
        "\n",
        "In **Convolutional Neural Networks (CNNs)**, **pooling** is used to reduce the size of feature maps while retaining important information. There are a few **common types of pooling** that are widely used:\n",
        "## üîë **Common Types of Pooling in CNNs**\n",
        "\n",
        "### 1. **Max Pooling (most common)**\n",
        "\n",
        "* Takes the **maximum value** from each pooling region (e.g., 2√ó2 or 3√ó3).\n",
        "* Keeps the **strongest/most activated feature**.\n",
        "* Example (2√ó2 max pooling):\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 3 \\\\\n",
        "2 & 4\n",
        "\\end{bmatrix}\n",
        "\\;\\;\\rightarrow\\;\\; 4\n",
        "$$\n",
        "\n",
        "‚úÖ Advantages: Keeps important features, ignores weaker signals, prevents noise.\n",
        "### 2. **Average Pooling**\n",
        "\n",
        "* Takes the **average value** of each pooling region.\n",
        "* Example (2√ó2 average pooling):\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 3 \\\\\n",
        "2 & 4\n",
        "\\end{bmatrix}\n",
        "\\;\\;\\rightarrow\\;\\; \\frac{1+2+3+4}{4} = 2.5\n",
        "$$\n",
        "\n",
        "‚úÖ Advantages: Provides smoother representations, but less effective at capturing strong features than max pooling.\n",
        "### 3. **Global Pooling (Global Max/Average Pooling)**\n",
        "\n",
        "* Instead of using a small window, the pooling operation is applied to the **entire feature map**.\n",
        "* Produces **one value per feature map**.\n",
        "* Example:\n",
        "\n",
        "  * **Global Max Pooling** ‚Üí takes the maximum of the whole feature map.\n",
        "  * **Global Average Pooling (GAP)** ‚Üí takes the average of the whole feature map.\n",
        "\n",
        "‚úÖ Advantages: Often used at the end of CNNs (e.g., in **ResNet, MobileNet**) instead of fully connected layers to reduce parameters and prevent overfitting.\n",
        "### 4. **Other Variants (less common but used in research)**\n",
        "\n",
        "* **Min Pooling** ‚Üí takes the minimum value in each region (rarely used).\n",
        "* **Stochastic Pooling** ‚Üí randomly selects a value from each pooling region based on probability.\n",
        "* **Fractional Pooling** ‚Üí reduces feature maps by non-integer factors (like 1.5√ó).\n",
        "## ‚úÖ Summary\n",
        "\n",
        "* **Max Pooling** ‚Üí keeps strongest features (most popular).\n",
        "* **Average Pooling** ‚Üí smooths features.\n",
        "* **Global Pooling** ‚Üí reduces each feature map to a single value, often used before classification.\n",
        "* **Other variants** ‚Üí less common but explored in advanced research.\n"
      ],
      "metadata": {
        "id": "0_G-ecL_oQW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7  How does the backpropagation algorithm work in CNNs*\n",
        "Perfect! üöÄ This is a key concept to understand how **CNNs actually learn**.\n",
        "The **backpropagation algorithm** in Convolutional Neural Networks (CNNs) works similarly to standard neural networks but adapted for convolution and pooling operations. It allows the network to **update filter weights** so it can detect better features over time.\n",
        "## üîë **How Backpropagation Works in CNNs**\n",
        "\n",
        "### 1. **Forward Pass**\n",
        "\n",
        "* Input image ‚Üí goes through **convolution layers, activations, pooling, fully connected layers, and output layer**.\n",
        "* Produces predicted output (e.g., probabilities of classes).\n",
        "* Loss function (e.g., cross-entropy) compares prediction with true label.\n",
        "### 2. **Backward Pass (Error Propagation)**\n",
        "\n",
        "Backpropagation computes gradients layer by layer, starting from the **output** back to the **input**.\n",
        "\n",
        "#### (a) **Output Layer**\n",
        "\n",
        "* Compute the error:\n",
        "\n",
        "  $$\n",
        "  \\text{Error} = \\text{Predicted Output} - \\text{True Output}\n",
        "  $$\n",
        "* Calculate gradient of the loss w\\.r.t. the output.\n",
        "\n",
        "#### (b) **Fully Connected Layers**\n",
        "\n",
        "* Standard backpropagation:\n",
        "\n",
        "  * Update weights using **gradient descent**.\n",
        "  * Gradients are propagated backward to the previous layer.\n",
        "\n",
        "#### (c) **Pooling Layers**\n",
        "\n",
        "* **Max Pooling:** Gradient is passed **only to the neuron that had the maximum value** in the forward pass.\n",
        "* **Average Pooling:** Gradient is **equally distributed** across all inputs in the pooling region.\n",
        "\n",
        "#### (d) **Convolutional Layers**\n",
        "\n",
        "* Trickiest part!\n",
        "* Each filter (kernel) is updated by computing how much it contributed to the error.\n",
        "* Steps:\n",
        "\n",
        "  1. Compute **error map** for the layer.\n",
        "  2. Convolve the **error map with the rotated filter** to propagate the error backward.\n",
        "  3. Compute gradients of filters (kernels) by convolving the input feature map with the **error signal**.\n",
        "  4. Update filters using gradient descent:\n",
        "\n",
        "     $$\n",
        "     W_{new} = W_{old} - \\eta \\cdot \\frac{\\partial L}{\\partial W}\n",
        "     $$\n",
        "\n",
        "     where $\\eta$ = learning rate.\n",
        "### 3. **Weight Update**\n",
        "\n",
        "* After computing gradients for all layers, CNN updates **filter weights, biases, and fully connected layer weights** using **gradient descent (or its variants like Adam, RMSProp, etc.)**.\n",
        "\n",
        "## üéØ **Key Insights**\n",
        "\n",
        "* Backpropagation in CNNs is essentially the same as in standard NNs but adapted to:\n",
        "\n",
        "  * **Shared weights** (filters are reused across the image).\n",
        "  * **Local connections** (only connected to a small receptive field, not the whole input).\n",
        "  * **Pooling layers** (special gradient handling for max/average pooling).\n",
        "‚úÖ **In short:**\n",
        "Backpropagation in CNNs works by propagating the loss backward through the **output ‚Üí fully connected ‚Üí pooling ‚Üí convolutional layers**, computing gradients for each filter and weight, and then updating them using gradient descent. This process allows CNNs to automatically learn the best filters for detecting features in images.\n"
      ],
      "metadata": {
        "id": "cWUGP758osFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8 What is the role of activation functions in CNNs*\n",
        "Great question! üîë\n",
        "\n",
        "In **Convolutional Neural Networks (CNNs)**, the **activation function** plays a crucial role in making the network capable of learning **complex, non-linear patterns**. Without activation functions, a CNN would just behave like a **linear model**, no matter how many layers it has.\n",
        "## üîë **Role of Activation Functions in CNNs**\n",
        "\n",
        "### 1. **Introduce Non-Linearity**\n",
        "\n",
        "* Images and real-world data are highly **non-linear** (edges, curves, textures, objects).\n",
        "* Convolution + pooling alone are **linear operations**.\n",
        "* Activation functions (like ReLU, Sigmoid, Tanh) add **non-linearity**, enabling the CNN to learn **complex mappings**.\n",
        "### 2. **Control Neuron Output**\n",
        "\n",
        "* Activation functions decide whether a neuron should be **activated (fire)** or not, based on input.\n",
        "* Example: ReLU sets negative values to 0 ‚Üí only passes positive signals.\n",
        "### 3. **Help with Gradient Flow**\n",
        "\n",
        "* During **backpropagation**, activation functions determine how **gradients** flow through the network.\n",
        "* Proper activation (e.g., ReLU, Leaky ReLU) helps avoid problems like the **vanishing gradient** that can stall learning.\n",
        "### 4. **Allow Hierarchical Feature Learning**\n",
        "\n",
        "* In early convolution layers ‚Üí activations help detect **edges and textures**.\n",
        "* In deeper layers ‚Üí they enable the network to combine simple features into **complex structures** (faces, objects).\n",
        "## üî• **Common Activation Functions in CNNs**\n",
        "\n",
        "1. **ReLU (Rectified Linear Unit)**\n",
        "\n",
        "   $$\n",
        "   f(x) = \\max(0, x)\n",
        "   $$\n",
        "\n",
        "   * Most widely used in CNNs.\n",
        "   * Keeps positive values, sets negatives to zero.\n",
        "   * Advantages: simple, fast, avoids vanishing gradient problem (compared to Sigmoid/Tanh).\n",
        "\n",
        "2. **Leaky ReLU**\n",
        "\n",
        "   $$\n",
        "   f(x) =\n",
        "   \\begin{cases}\n",
        "   x & x > 0 \\\\\n",
        "   \\alpha x & x \\leq 0\n",
        "   \\end{cases}\n",
        "   $$\n",
        "\n",
        "   * Fixes the ‚Äúdying ReLU‚Äù problem by allowing a small slope for negative values.\n",
        "\n",
        "3. **Sigmoid**\n",
        "\n",
        "   $$\n",
        "   f(x) = \\frac{1}{1+e^{-x}}\n",
        "   $$\n",
        "\n",
        "   * Maps values between **0 and 1**.\n",
        "   * Used for **binary classification output layers**.\n",
        "   * Not often used in hidden layers (vanishing gradient issue).\n",
        "\n",
        "4. **Tanh**\n",
        "\n",
        "   $$\n",
        "   f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
        "   $$\n",
        "\n",
        "   * Maps values between **-1 and 1**.\n",
        "   * Stronger gradients than sigmoid, but still suffers from vanishing gradient in deep networks.\n",
        "\n",
        "5. **Softmax**\n",
        "\n",
        "   * Used in the **final layer** for **multi-class classification**.\n",
        "   * Converts raw scores (logits) into probabilities that sum to 1.\n",
        "## ‚úÖ **In short:**\n",
        "\n",
        "The **activation function** in CNNs introduces **non-linearity**, enables **feature learning**, controls **information flow**, and ensures gradients can be computed effectively during training. Without it, CNNs would not be able to learn complex patterns in images.\n"
      ],
      "metadata": {
        "id": "Thkm7QqDpY4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9 What is the concept of receptive fields in CNNs*\n",
        "Excellent question! üåü\n",
        "The concept of a **receptive field** is central to understanding how **CNNs ‚Äúsee‚Äù and process images**.\n",
        "## üîë **What is a Receptive Field in CNNs?**\n",
        "\n",
        "* A **receptive field** is the **region of the input image** that a particular neuron (in some layer of the CNN) is influenced by or ‚Äúlooks at.‚Äù\n",
        "* In other words, it defines **how much of the input image contributes to a neuron‚Äôs output**.\n",
        "## üìå **How it Works**\n",
        "\n",
        "1. **In the first convolutional layer**\n",
        "\n",
        "   * Each neuron only looks at a small patch (e.g., 3√ó3 or 5√ó5) of the input image.\n",
        "   * So its receptive field is just that patch.\n",
        "\n",
        "2. **In deeper layers**\n",
        "\n",
        "   * Each neuron‚Äôs receptive field grows because it is connected to feature maps that already summarize information from earlier patches.\n",
        "   * Eventually, deeper neurons have a receptive field covering a large portion of the image, even the whole image.\n",
        "## üéØ **Example**\n",
        "\n",
        "* Suppose:\n",
        "\n",
        "  * Input image: 32√ó32 pixels.\n",
        "  * First conv layer: 3√ó3 filter ‚Üí receptive field = 3√ó3.\n",
        "  * Second conv layer: again 3√ó3 filter ‚Üí now each neuron sees **a 5√ó5 region** of the original image (since each feature in the second layer depends on 3√ó3 from the first).\n",
        "  * Third conv layer: receptive field grows further.\n",
        "\n",
        "üëâ This shows how deeper CNN layers capture **higher-level patterns** (like edges ‚Üí textures ‚Üí shapes ‚Üí objects).\n",
        "## üìä **Why Receptive Fields Matter**\n",
        "\n",
        "1. **Hierarchical Feature Learning**\n",
        "\n",
        "   * Small receptive fields in early layers detect **low-level features** (edges, corners).\n",
        "   * Large receptive fields in deeper layers detect **high-level features** (faces, objects).\n",
        "\n",
        "2. **Translation Invariance**\n",
        "\n",
        "   * Larger receptive fields help CNNs recognize objects even if they move within the image.\n",
        "\n",
        "3. **Architecture Design**\n",
        "\n",
        "   * Choosing kernel size, stride, and pooling determines receptive field growth.\n",
        "   * Example: Using pooling layers makes receptive fields grow faster.\n",
        "‚úÖ **In short:**\n",
        "The **receptive field** in CNNs is the region of the input image that influences a neuron‚Äôs output. Small receptive fields in early layers detect simple features, while large receptive fields in deeper layers capture complex structures, allowing CNNs to understand images at multiple levels."
      ],
      "metadata": {
        "id": "f3OpyxAKqMST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10.Explain the concept of tensor space in CNNs\n",
        "Great question! ‚ö°\n",
        "To understand **tensor space in CNNs**, let‚Äôs break it down step by step.\n",
        "## üîë **What is a Tensor?**\n",
        "\n",
        "* A **tensor** is just a multi-dimensional array (generalization of scalars, vectors, and matrices).\n",
        "\n",
        "  * **Scalar** ‚Üí 0D tensor (e.g., `5`)\n",
        "  * **Vector** ‚Üí 1D tensor (e.g., `[1, 2, 3]`)\n",
        "  * **Matrix** ‚Üí 2D tensor (e.g., a grayscale image of size 28√ó28)\n",
        "  * **Higher-order tensor** ‚Üí 3D, 4D, ‚Ä¶ (e.g., RGB images or batches of images).\n",
        "\n",
        "In CNNs, **images, feature maps, and parameters (filters)** are all represented as **tensors**.\n",
        "## üîé **Tensor Space in CNNs**\n",
        "\n",
        "When we talk about *tensor space*, we mean the **multi-dimensional structure where CNN operations take place**.\n",
        "\n",
        "### 1. **Input Tensor (Image Space)**\n",
        "\n",
        "* An image is represented as a **3D tensor**:\n",
        "\n",
        "  * Height √ó Width √ó Channels (H √ó W √ó C).\n",
        "  * Example: A 32√ó32 RGB image = `32 √ó 32 √ó 3`.\n",
        "\n",
        "If we feed multiple images at once (a batch), the input tensor becomes **4D**:\n",
        "\n",
        "* Batch √ó Height √ó Width √ó Channels (N √ó H √ó W √ó C).\n",
        "### 2. **Convolutional Layer (Filter Tensor Space)**\n",
        "\n",
        "* Filters (kernels) are also tensors.\n",
        "* A filter has size: **(k √ó k √ó C)**, where:\n",
        "\n",
        "  * `k √ó k` ‚Üí spatial size (e.g., 3√ó3).\n",
        "  * `C` ‚Üí depth (number of input channels).\n",
        "* Multiple filters ‚Üí output feature maps stacked into another tensor.\n",
        "### 3. **Feature Maps (Activation Tensor Space)**\n",
        "\n",
        "* After convolution + activation, output is another **tensor**:\n",
        "\n",
        "  * Height √ó Width √ó Number of Filters (H √ó W √ó F).\n",
        "* As we go deeper in the network, the **spatial size shrinks** (due to pooling/stride), but the **depth increases** (more filters).\n",
        "### 4. **Pooling Layer**\n",
        "\n",
        "* Operates on the tensor space by reducing spatial dimensions (H and W) while keeping depth (C) the same.\n",
        "### 5. **Fully Connected Layer**\n",
        "\n",
        "* Flattens the tensor (multi-dimensional array) into a **1D vector tensor**, which connects to dense neurons.\n",
        "### 6. **Tensor Transformations**\n",
        "\n",
        "* Throughout a CNN, the input tensor **transforms** step by step:\n",
        "\n",
        "  * **Image (4D tensor)** ‚Üí **Feature maps (4D tensors)** ‚Üí **Flattened tensor (2D)** ‚Üí **Output tensor (class probabilities)**.\n",
        "## üéØ **Why Tensor Space is Important in CNNs**\n",
        "\n",
        "1. **Efficient Representation**\n",
        "\n",
        "   * Tensors allow CNNs to handle high-dimensional data like images and videos.\n",
        "2. **Hardware Optimization**\n",
        "\n",
        "   * Frameworks (TensorFlow, PyTorch) use tensor operations optimized for GPUs/TPUs.\n",
        "3. **Hierarchical Learning**\n",
        "\n",
        "   * The progression of tensors through the network represents a **mapping from raw pixels ‚Üí features ‚Üí predictions**.\n",
        "4. **Batch Processing**\n",
        "\n",
        "   * Tensors allow multiple images (batches) to be processed in parallel.\n",
        "‚úÖ **In short:**\n",
        "The **tensor space in CNNs** refers to the multi-dimensional representation of data (images, filters, feature maps) as tensors and how they transform at each layer. It‚Äôs the ‚Äúmathematical space‚Äù where CNN operations happen, enabling efficient learning from complex image data.\n"
      ],
      "metadata": {
        "id": "O1a7lf-XqquX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11 What is LeNet-5, and how does it contribute to the development of CNNs*\n",
        "Great question! üöÄ\n",
        "\n",
        "**LeNet-5** is one of the earliest and most influential **Convolutional Neural Network (CNN) architectures**, developed by **Yann LeCun and colleagues in 1998**. It was originally designed for **handwritten digit recognition** (like the MNIST dataset) and played a major role in proving that CNNs can outperform traditional machine learning methods on image-related tasks.\n",
        "## üîë **Architecture of LeNet-5**\n",
        "\n",
        "LeNet-5 is a relatively simple CNN compared to modern deep networks, but it introduced the key building blocks we still use today.\n",
        "\n",
        "### Input:\n",
        "\n",
        "* A **32√ó32 grayscale image** (for MNIST digits).\n",
        "\n",
        "### Layers:\n",
        "\n",
        "1. **C1 ‚Äì Convolutional Layer**\n",
        "\n",
        "   * 6 filters of size 5√ó5 ‚Üí produces 6 feature maps (28√ó28).\n",
        "   * Detects basic features like edges and simple patterns.\n",
        "\n",
        "2. **S2 ‚Äì Subsampling (Pooling) Layer**\n",
        "\n",
        "   * Average pooling with 2√ó2 regions ‚Üí reduces size to 14√ó14.\n",
        "   * Adds translation invariance, reduces computation.\n",
        "\n",
        "3. **C3 ‚Äì Convolutional Layer**\n",
        "\n",
        "   * 16 filters of size 5√ó5 ‚Üí produces 16 feature maps (10√ó10).\n",
        "   * Extracts more complex patterns.\n",
        "\n",
        "4. **S4 ‚Äì Subsampling (Pooling) Layer**\n",
        "\n",
        "   * Again, average pooling (2√ó2) ‚Üí reduces size to 5√ó5.\n",
        "\n",
        "5. **C5 ‚Äì Convolutional Layer (Fully Connected to S4)**\n",
        "\n",
        "   * 120 feature maps, each of size 1√ó1.\n",
        "   * Works like a fully connected convolutional layer.\n",
        "\n",
        "6. **F6 ‚Äì Fully Connected Layer**\n",
        "\n",
        "   * 84 neurons, connected to the previous layer.\n",
        "\n",
        "7. **Output Layer**\n",
        "\n",
        "   * 10 neurons (for digits 0‚Äì9), using a **softmax** for classification.\n",
        "\n",
        "## üéØ **Contributions of LeNet-5 to CNN Development**\n",
        "\n",
        "1. **Introduced Core CNN Concepts**\n",
        "\n",
        "   * Convolutional layers (local receptive fields).\n",
        "   * Subsampling (pooling) for dimension reduction.\n",
        "   * Fully connected layers for classification.\n",
        "\n",
        "2. **Hierarchical Feature Extraction**\n",
        "\n",
        "   * Showed how CNNs can learn features automatically ‚Üí from **edges ‚Üí shapes ‚Üí digits**.\n",
        "\n",
        "3. **Inspired Modern Architectures**\n",
        "\n",
        "   * Later CNNs like **AlexNet, VGG, ResNet** are deeper, but they follow the same principle as LeNet-5.\n",
        "\n",
        "4. **Proved CNNs Work in Practice**\n",
        "\n",
        "   * Demonstrated that CNNs can outperform handcrafted features and traditional machine learning (like SVMs, k-NN) for image tasks.\n",
        "\n",
        "5. **Foundation for Deep Learning in Vision**\n",
        "\n",
        "   * LeNet-5 was a turning point, showing the potential of deep learning for computer vision, paving the way for modern AI applications like object detection, face recognition, and self-driving cars.\n",
        "‚úÖ **In short:**\n",
        "**LeNet-5** is a pioneering CNN architecture that introduced convolution, pooling, and fully connected layers in a hierarchical structure. It proved that CNNs can automatically learn useful features from raw images, making it the foundation for modern deep learning in computer vision.\n"
      ],
      "metadata": {
        "id": "tBVIsy43rC5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12 What is AlexNet, and why was it a breakthrough in deep learning*\n",
        "Awesome question! üöÄ\n",
        "**AlexNet** is one of the most famous CNN architectures, created by **Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton** in **2012**. It was designed for the **ImageNet Large Scale Visual Recognition Challenge (ILSVRC 2012)** and marked a huge breakthrough in **deep learning for computer vision**.\n",
        "## üîë **What is AlexNet?**\n",
        "\n",
        "* A deep **Convolutional Neural Network (CNN)** with **8 layers** (5 convolutional + 3 fully connected).\n",
        "* Input: **224√ó224√ó3** RGB image.\n",
        "* Output: **1000 class probabilities** (for ImageNet classification).\n",
        "\n",
        "### üìå **Architecture (Layer by Layer)**\n",
        "\n",
        "1. **Conv1**: 96 filters, size 11√ó11, stride 4 ‚Üí detects low-level features.\n",
        "2. **Max Pooling**: 3√ó3 window, stride 2.\n",
        "3. **Conv2**: 256 filters, size 5√ó5.\n",
        "4. **Max Pooling**.\n",
        "5. **Conv3**: 384 filters, size 3√ó3.\n",
        "6. **Conv4**: 384 filters, size 3√ó3.\n",
        "7. **Conv5**: 256 filters, size 3√ó3.\n",
        "8. **Max Pooling**.\n",
        "9. **Fully Connected Layers**: Two layers with 4096 neurons each.\n",
        "10. **Output Layer**: 1000 neurons, softmax activation.\n",
        "\n",
        "Additional techniques used:\n",
        "\n",
        "* **ReLU activation** instead of sigmoid/tanh.\n",
        "* **Dropout** (50%) in fully connected layers to reduce overfitting.\n",
        "* **Data augmentation** (image translations, reflections, etc.).\n",
        "* **GPU acceleration** for faster training.\n",
        "## üéØ **Why Was AlexNet a Breakthrough?**\n",
        "\n",
        "1. **Huge Accuracy Improvement**\n",
        "\n",
        "   * Achieved **15.3% top-5 error rate** on ImageNet 2012.\n",
        "   * The next-best competitor had **26.2% error** ‚Üí nearly **10% better**, a massive leap.\n",
        "\n",
        "2. **Made Deep Learning Popular**\n",
        "\n",
        "   * Proved CNNs could work on **large-scale datasets** with millions of images.\n",
        "   * Sparked a revolution ‚Üí CNNs became the standard for computer vision.\n",
        "\n",
        "3. **Introduced Key Innovations**\n",
        "\n",
        "   * **ReLU** activation ‚Üí solved vanishing gradient problem.\n",
        "   * **Dropout** ‚Üí reduced overfitting in deep networks.\n",
        "   * **GPU training** ‚Üí made training deep nets feasible in weeks instead of months.\n",
        "\n",
        "4. **Scalability**\n",
        "\n",
        "   * Showed that **deeper, bigger networks** (with millions of parameters) can outperform shallower ones when trained on large datasets.\n",
        "\n",
        "5. **Foundation for Modern CNNs**\n",
        "\n",
        "   * Inspired **VGG, GoogLeNet, ResNet**, and all modern deep vision architectures.\n",
        "## ‚úÖ **In short:**\n",
        "\n",
        "**AlexNet** was a landmark CNN that won the ImageNet 2012 challenge by a wide margin. Its innovations in architecture (ReLU, dropout, GPU training, large-scale depth) proved the power of deep learning, ignited the modern AI boom, and laid the foundation for today‚Äôs computer vision breakthroughs.\n"
      ],
      "metadata": {
        "id": "6V1lTrs4r9EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13 What is VGGNet, and how does it differ from AlexNet*\n",
        "Great follow-up! üöÄ\n",
        "After **AlexNet (2012)** showed the power of deep CNNs, researchers at the **Visual Geometry Group (VGG) at Oxford University** introduced **VGGNet** in **2014**. It became one of the most influential architectures in deep learning.\n",
        "## üîë **What is VGGNet?**\n",
        "\n",
        "* Proposed by **Karen Simonyan and Andrew Zisserman**.\n",
        "* Competed in the **ILSVRC 2014 (ImageNet Challenge)**.\n",
        "* The most popular variant is **VGG16** (16 weight layers), though there‚Äôs also **VGG19**.\n",
        "* Input: **224√ó224√ó3** RGB image.\n",
        "* Output: **1000 classes** (ImageNet).\n",
        "## üìå **Architecture of VGGNet (VGG16)**\n",
        "\n",
        "1. **Convolutional Layers**\n",
        "\n",
        "   * Uses **only 3√ó3 filters** (stride 1, padding 1).\n",
        "   * Many convolution layers stacked together ‚Üí deeper feature extraction.\n",
        "2. **Max Pooling**\n",
        "\n",
        "   * 2√ó2 window with stride 2 (reduces spatial size).\n",
        "3. **Fully Connected Layers**\n",
        "\n",
        "   * Two layers with 4096 neurons each.\n",
        "4. **Output Layer**\n",
        "\n",
        "   * 1000-class softmax.\n",
        "\n",
        "So, **VGG16** = 13 convolution layers + 3 fully connected layers = 16 layers with learnable weights.\n",
        "## üéØ **How VGGNet Differs from AlexNet**\n",
        "\n",
        "| Feature         | **AlexNet (2012)**                     | **VGGNet (2014)**                               |\n",
        "| --------------- | -------------------------------------- | ----------------------------------------------- |\n",
        "| **Depth**       | 8 layers (5 conv + 3 FC)               | 16 or 19 layers                                 |\n",
        "| **Filter Size** | Large filters (11√ó11, 5√ó5, 3√ó3)        | Only **3√ó3 filters** (stacked multiple times)   |\n",
        "| **Stride**      | First conv layer stride = 4            | Stride = 1 (smaller, more precise feature maps) |\n",
        "| **Pooling**     | Max pooling (3√ó3, stride 2)            | Max pooling (2√ó2, stride 2)                     |\n",
        "| **Parameters**  | \\~60 million                           | \\~138 million (much larger)                     |\n",
        "| **Training**    | Trained with ReLU, dropout, GPUs       | Same (ReLU, dropout, GPUs), but deeper network  |\n",
        "| **Performance** | Won ILSVRC 2012 with top-5 error 15.3% | Achieved top-5 error \\~7.3% (much better)       |\n",
        "\n",
        "## üí° **Why VGGNet Was Important**\n",
        "\n",
        "1. **Simplicity in Design**\n",
        "\n",
        "   * Showed that **depth + small filters** ‚Üí very powerful results.\n",
        "   * Easy to generalize and adapt to other tasks.\n",
        "\n",
        "2. **Deeper Hierarchical Features**\n",
        "\n",
        "   * More layers meant better ability to capture **low-level to high-level features**.\n",
        "\n",
        "3. **Standard Architecture**\n",
        "\n",
        "   * VGG became a **benchmark CNN backbone** for transfer learning (used in object detection, segmentation, etc.).\n",
        "\n",
        "4. **Trade-off**\n",
        "\n",
        "   * Extremely large number of parameters (\\~138M) ‚Üí requires a lot of memory and computation.\n",
        "‚úÖ **In short:**\n",
        "**VGGNet** is a deeper CNN (16/19 layers) that simplified architecture design by using only **3√ó3 filters** stacked in depth. Compared to AlexNet, it‚Äôs deeper, more uniform, and achieves much better accuracy, though at the cost of higher computational requirement"
      ],
      "metadata": {
        "id": "rIuEqQrksVN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14 What is GoogLeNet, and what is its main innovation*\n",
        "Great question! üöÄ Let‚Äôs break it down.\n",
        "# üîé **GoogLeNet (Inception v1)**\n",
        "\n",
        "**GoogLeNet** is a deep convolutional neural network architecture developed by researchers at **Google** and introduced in the **ILSVRC 2014** competition. It achieved **state-of-the-art performance**, winning **1st place** in the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) that year.\n",
        "\n",
        "It is also called **Inception v1**, because it introduced the **Inception module**.\n",
        "## üåü **Main Innovation: The Inception Module**\n",
        "\n",
        "The **Inception module** is the core idea behind GoogLeNet.\n",
        "Instead of choosing a single filter size (like 3√ó3 or 5√ó5), the inception module **applies multiple filters in parallel** and then **concatenates their outputs**.\n",
        "\n",
        "### üîπ How it works:\n",
        "\n",
        "* Inside each Inception module:\n",
        "\n",
        "  * **1√ó1 convolutions** ‚Üí reduce dimensions (act as bottlenecks to save computation).\n",
        "  * **3√ó3 convolutions** ‚Üí capture medium-sized features.\n",
        "  * **5√ó5 convolutions** ‚Üí capture larger features.\n",
        "  * **3√ó3 max pooling** ‚Üí preserve spatial context.\n",
        "* Outputs from all branches are **concatenated along the depth/channel dimension**.\n",
        "\n",
        "üëâ This allows the network to **analyze features at multiple scales simultaneously**, which improves accuracy without exploding computational cost.\n",
        "## üèóÔ∏è **GoogLeNet Architecture (22 layers deep)**\n",
        "\n",
        "* Stacks **9 inception modules**.\n",
        "* Uses **1√ó1 convolutions** heavily for **dimension reduction**.\n",
        "* Much deeper than AlexNet (8 layers) and VGGNet (16‚Äì19 layers), but **more computationally efficient** (fewer parameters).\n",
        "\n",
        "  * **GoogLeNet: \\~4 million parameters**\n",
        "  * **AlexNet: \\~60 million parameters**\n",
        "  * **VGGNet: \\~138 million parameters**\n",
        "## üéØ **Key Contributions of GoogLeNet**\n",
        "\n",
        "1. **Inception Module** ‚Üí multi-scale feature extraction.\n",
        "2. **1√ó1 Convolutions** ‚Üí reduce dimensions and prevent parameter explosion.\n",
        "3. **Global Average Pooling** at the end (instead of fully connected layers) ‚Üí reduces overfitting.\n",
        "4. **Very deep yet efficient** ‚Üí achieved breakthrough performance on ImageNet.\n",
        "‚úÖ **In summary:**\n",
        "**GoogLeNet** (Inception v1) is a 22-layer CNN that introduced the **Inception module**, enabling efficient multi-scale feature learning. Its main innovation was combining **different filter sizes in parallel** within the same layer, making it both deep and computationally efficient compared to AlexNet and VGG.\n"
      ],
      "metadata": {
        "id": "tX1AYRNKs6Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15 What is ResNet, and what problem does it solve*\n",
        "Excellent question ‚Äî this one is a big milestone in deep learning. üöÄ\n",
        "# üîé **ResNet (Residual Network)**\n",
        "\n",
        "**ResNet** was introduced by **Kaiming He et al. (Microsoft Research)** in **2015** and won the **ImageNet ILSVRC 2015** competition.\n",
        "It is one of the most influential CNN architectures because it solved a fundamental problem in training **very deep networks**.\n",
        "## üåü **The Problem: Degradation in Deep Networks**\n",
        "\n",
        "* As researchers tried to make networks deeper (e.g., 30, 50, 100+ layers), they expected accuracy to improve.\n",
        "* However, **very deep networks often performed worse** than shallower ones ‚Äî not due to overfitting, but due to **optimization difficulties**.\n",
        "* This phenomenon is called the **degradation problem**:\n",
        "\n",
        "  * Training error increased as layers were added.\n",
        "  * Gradient became very small (vanishing gradient problem), making deeper layers hard to train.\n",
        "## üåü **The Solution: Residual Learning**\n",
        "\n",
        "ResNet introduced the idea of **residual blocks** with **skip connections (shortcut connections)**.\n",
        "\n",
        "### üîπ A residual block:\n",
        "\n",
        "Instead of directly learning the mapping:\n",
        "\n",
        "$$\n",
        "H(x) = F(x)\n",
        "$$\n",
        "\n",
        "ResNet learns a **residual function**:\n",
        "\n",
        "$$\n",
        "H(x) = F(x) + x\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $x$ = input\n",
        "* $F(x)$ = residual function learned by a few stacked layers\n",
        "* $x + F(x)$ = output\n",
        "\n",
        "### üîπ Intuition:\n",
        "\n",
        "* If deeper layers are not useful, the network can simply learn an **identity mapping** (just pass the input forward).\n",
        "* This makes optimization easier, since it avoids vanishing gradients and allows gradients to flow directly through skip connections.\n",
        "## üèóÔ∏è **ResNet Architecture**\n",
        "\n",
        "* ResNet comes in versions: **ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-152** (numbers = layers).\n",
        "* A typical ResNet block looks like:\n",
        "\n",
        "  ```\n",
        "  Input ‚Üí Conv ‚Üí BatchNorm ‚Üí ReLU ‚Üí Conv ‚Üí BatchNorm\n",
        "        ‚Üò‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ (skip connection)\n",
        "  Output = F(x) + x ‚Üí ReLU\n",
        "  ```\n",
        "* ResNet-50 and above use **bottleneck blocks** with 1√ó1, 3√ó3, and 1√ó1 convolutions for efficiency.\n",
        "## üéØ **Key Contributions of ResNet**\n",
        "\n",
        "1. **Skip connections (residual learning)** ‚Üí solve degradation/vanishing gradient problem.\n",
        "2. Enabled training of **very deep networks** (over 100+ layers).\n",
        "3. Achieved **state-of-the-art performance** on ImageNet (2015).\n",
        "4. Inspired many later architectures (DenseNet, EfficientNet, Transformers for vision, etc.).\n",
        "‚úÖ **In summary:**\n",
        "**ResNet** introduced **residual connections (skip connections)**, allowing networks to be **much deeper** without suffering from vanishing gradients or training degradation. This solved a critical problem in deep learning and set a new standard in computer vision.\n"
      ],
      "metadata": {
        "id": "Ml8Xl5GltYF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 16 What is DenseNet, and how does it differ from ResNet*\n",
        "Great follow-up! üöÄ You already know ResNet introduced **skip connections**. DenseNet takes that idea even further.\n",
        "# üîé **DenseNet (Densely Connected Convolutional Network)**\n",
        "\n",
        "**DenseNet** was proposed by **Gao Huang et al.** in 2016.\n",
        "It is another deep CNN architecture that improves feature reuse and gradient flow by introducing **dense connections**.\n",
        "## üåü **Core Idea of DenseNet**\n",
        "\n",
        "In ResNet, we add a **skip connection** from input to output of a block:\n",
        "\n",
        "$$\n",
        "y = F(x) + x\n",
        "$$\n",
        "\n",
        "But in **DenseNet**, **each layer connects to every other layer** in a feed-forward fashion:\n",
        "\n",
        "$$\n",
        "x_\\ell = H_\\ell([x_0, x_1, x_2, \\dots, x_{\\ell-1}])\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $x_\\ell$ = output of layer $\\ell$\n",
        "* $H_\\ell$ = transformation (Conv + BN + ReLU)\n",
        "* $[x_0, x_1, \\dots, x_{\\ell-1}]$ = concatenation of all previous feature maps\n",
        "\n",
        "üëâ Instead of summation (ResNet), DenseNet uses **concatenation**.\n",
        "## üèóÔ∏è **DenseNet Architecture**\n",
        "\n",
        "* Built using **Dense Blocks**: each layer gets inputs from all previous layers.\n",
        "* Between dense blocks, **Transition Layers** (1√ó1 conv + pooling) are used to reduce feature map size.\n",
        "* Common variants: **DenseNet-121, DenseNet-169, DenseNet-201, DenseNet-264**.\n",
        "## üéØ **How DenseNet Differs from ResNet**\n",
        "\n",
        "| Feature           | **ResNet**                       | **DenseNet**                                                          |\n",
        "| ----------------- | -------------------------------- | --------------------------------------------------------------------- |\n",
        "| **Connection**    | Adds residuals: $y = F(x) + x$   | Concatenates all previous outputs                                     |\n",
        "| **Gradient Flow** | Good (skip connections)          | Excellent (direct path from every layer to loss)                      |\n",
        "| **Feature Reuse** | Limited (only through residuals) | Strong (later layers reuse all earlier features)                      |\n",
        "| **Parameters**    | More (no reuse of features)      | Fewer (better efficiency via reuse)                                   |\n",
        "| **Output Growth** | Constant feature size            | Feature maps grow as layers concatenate ‚Üí controlled by *growth rate* |\n",
        "## üåü **Advantages of DenseNet**\n",
        "\n",
        "1. **Efficient** ‚Üí fewer parameters than ResNet.\n",
        "2. **Feature reuse** ‚Üí avoids redundancy.\n",
        "3. **Stronger gradient flow** ‚Üí easier training.\n",
        "4. **Less overfitting** with fewer parameters.\n",
        "‚úÖ **In summary:**\n",
        "\n",
        "* **ResNet**: uses **addition-based skip connections** ‚Üí helps very deep networks train.\n",
        "* **DenseNet**: uses **concatenation-based dense connections** ‚Üí maximizes feature reuse and strengthens gradient flow\n"
      ],
      "metadata": {
        "id": "Vhynuzn6twGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 17  What are the main steps involved in training a CNN from scratch?\n",
        "Great question! üöÄ Training a **Convolutional Neural Network (CNN) from scratch** involves several structured steps. Let‚Äôs break it down clearly:\n",
        "# üîé **Main Steps in Training a CNN from Scratch**\n",
        "\n",
        "### **1. Data Collection & Preparation**\n",
        "\n",
        "* **Collect dataset** ‚Üí e.g., CIFAR-10, ImageNet, or your own dataset.\n",
        "* **Preprocess images**:\n",
        "\n",
        "  * Resize / crop\n",
        "  * Normalize pixel values (e.g., between 0‚Äì1 or mean subtraction)\n",
        "  * Augmentation (flipping, rotation, color jitter) to improve generalization.\n",
        "* **Split dataset**:\n",
        "\n",
        "  * Training set (for learning)\n",
        "  * Validation set (for tuning hyperparameters)\n",
        "  * Test set (for final evaluation).\n",
        "### **2. Define CNN Architecture**\n",
        "\n",
        "* **Input Layer** ‚Üí takes in images (e.g., 32√ó32√ó3 for CIFAR-10).\n",
        "* **Convolutional Layers** ‚Üí extract spatial features.\n",
        "* **Activation Functions** (e.g., ReLU).\n",
        "* **Pooling Layers** ‚Üí reduce spatial dimensions.\n",
        "* **Fully Connected (Dense) Layers** ‚Üí interpret extracted features.\n",
        "* **Output Layer** ‚Üí classification (Softmax for multi-class, Sigmoid for binary).\n",
        "### **3. Choose a Loss Function**\n",
        "\n",
        "* **Cross-Entropy Loss** ‚Üí for classification tasks.\n",
        "* **MSE (Mean Squared Error)** ‚Üí for regression tasks.\n",
        "### **4. Select an Optimizer**\n",
        "\n",
        "* **Stochastic Gradient Descent (SGD)**\n",
        "* **Adam** (adaptive learning rate, widely used)\n",
        "* **RMSprop**\n",
        "\n",
        "These update weights using **backpropagation**.\n",
        "### **5. Forward Propagation**\n",
        "\n",
        "* Pass input images through the CNN.\n",
        "* Compute predictions at the output layer.\n",
        "* Compare predictions with ground truth labels using the **loss function**.\n",
        "### **6. Backpropagation & Weight Updates**\n",
        "\n",
        "* Compute gradients of loss w\\.r.t weights using **chain rule**.\n",
        "* Propagate errors backward through convolutional and fully connected layers.\n",
        "* Update weights using optimizer (e.g., Gradient Descent).\n",
        "### **7. Training Loop**\n",
        "\n",
        "* Repeat for multiple **epochs**:\n",
        "\n",
        "  1. Forward pass (prediction).\n",
        "  2. Compute loss.\n",
        "  3. Backward pass (gradient computation).\n",
        "  4. Update weights.\n",
        "* Track metrics like accuracy, precision, recall, etc.\n",
        "### **8. Validation**\n",
        "\n",
        "* After each epoch, evaluate on validation data.\n",
        "* Adjust **hyperparameters** (learning rate, batch size, dropout, etc.).\n",
        "* Prevent **overfitting** (use dropout, regularization, data augmentation, early stopping).\n",
        "### **9. Testing**\n",
        "\n",
        "* After training, evaluate final model on **test set**.\n",
        "* Ensure model generalizes well and doesn‚Äôt just memorize training data.\n",
        "### **10. Deployment (Optional)**\n",
        "\n",
        "* Save trained model.\n",
        "* Deploy it for real-world predictions (mobile app, cloud, embedded system, etc.).\n",
        "‚úÖ **In summary:**\n",
        "Training a CNN from scratch involves:\n",
        "**(1) Preparing data ‚Üí (2) Designing CNN ‚Üí (3) Choosing loss & optimizer ‚Üí (4) Forward pass ‚Üí (5) Backpropagation ‚Üí (6) Iterative training ‚Üí (7) Validation ‚Üí (8) Testing ‚Üí (9) Deployment.*\n"
      ],
      "metadata": {
        "id": "TDNmS3bTuIFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PRACTICAL"
      ],
      "metadata": {
        "id": "gTIBU5o6unO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  1 Implement a basic convolution operation using a filter and a 5x5 image (matrix)\n",
        "Perfect! üöÄ Let‚Äôs implement a **basic convolution operation** step by step on a **5√ó5 image matrix** with a **3√ó3 filter (kernel)**.\n",
        "# üñºÔ∏è Example Setup\n",
        "\n",
        "### Input Image (5√ó5 matrix)\n",
        "\n",
        "$$\n",
        "I = \\begin{bmatrix}\n",
        "1 & 2 & 0 & 1 & 3 \\\\\n",
        "4 & 1 & 2 & 0 & 1 \\\\\n",
        "1 & 3 & 1 & 2 & 2 \\\\\n",
        "2 & 0 & 2 & 3 & 1 \\\\\n",
        "0 & 1 & 3 & 2 & 4\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "### Filter / Kernel (3√ó3 matrix)\n",
        "\n",
        "$$\n",
        "K = \\begin{bmatrix}\n",
        "1 & 0 & -1 \\\\\n",
        "1 & 0 & -1 \\\\\n",
        "1 & 0 & -1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "üëâ This kernel is similar to a **Sobel filter** used for edge detection.\n",
        "# üî¢ Step 1: Convolution Operation\n",
        "\n",
        "The convolution slides the kernel across the image and performs **element-wise multiplication + summation**.\n",
        "\n",
        "* Stride = 1\n",
        "* No padding ‚Üí output size = (5‚àí3+1) √ó (5‚àí3+1) = **3√ó3**\n",
        "# üßÆ Step 2: Manual Calculation (First element)\n",
        "\n",
        "Top-left **3√ó3 region of image**:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 2 & 0 \\\\\n",
        "4 & 1 & 2 \\\\\n",
        "1 & 3 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Multiply element-wise with filter $K$:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1\\cdot1 & 2\\cdot0 & 0\\cdot(-1) \\\\\n",
        "4\\cdot1 & 1\\cdot0 & 2\\cdot(-1) \\\\\n",
        "1\\cdot1 & 3\\cdot0 & 1\\cdot(-1)\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 \\\\\n",
        "4 & 0 & -2 \\\\\n",
        "1 & 0 & -1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Sum all elements = $1 + 0 + 0 + 4 + 0 - 2 + 1 + 0 - 1 = 3$.\n",
        "\n",
        "So, first output value = **3** ‚úÖ\n",
        "\n",
        "---\n",
        "\n",
        "# üßë‚Äçüíª Step 3: Python Implementation\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# 5x5 Image\n",
        "I = np.array([\n",
        "    [1, 2, 0, 1, 3],\n",
        "    [4, 1, 2, 0, 1],\n",
        "    [1, 3, 1, 2, 2],\n",
        "    [2, 0, 2, 3, 1],\n",
        "    [0, 1, 3, 2, 4]\n",
        "])\n",
        "\n",
        "# 3x3 Filter (Kernel)\n",
        "K = np.array([\n",
        "    [1, 0, -1],\n",
        "    [1, 0, -1],\n",
        "    [1, 0, -1]\n",
        "])\n",
        "\n",
        "# Output size (no padding, stride 1)\n",
        "output_size = I.shape[0] - K.shape[0] + 1\n",
        "output = np.zeros((output_size, output_size))\n",
        "\n",
        "# Perform convolution\n",
        "for i in range(output_size):\n",
        "    for j in range(output_size):\n",
        "        region = I[i:i+3, j:j+3]  # extract 3x3 region\n",
        "        output[i, j] = np.sum(region * K)  # element-wise multiplication + sum\n",
        "\n",
        "print(\"Convolution Result:\")\n",
        "print(output)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# üìä Example Output (3√ó3 Matrix)\n",
        "\n",
        "```\n",
        "[[  3   1  -4]\n",
        " [  2  -2  -2]\n",
        " [  2  -4  -2]]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **In summary:**\n",
        "We manually and programmatically implemented a **basic convolution** on a 5√ó5 image with a 3√ó3 filter. The convolution extracts **edges/features** by sliding the kernel and performing element-wise multiplication + summation.\n"
      ],
      "metadata": {
        "id": "XQqR6s6-uqhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  2  Implement max pooling on a 4x4 feature map with a 2x2 window\n",
        "Great! üöÄ Let‚Äôs implement **Max Pooling** step by step on a **4√ó4 feature map** with a **2√ó2 pooling window**.\n",
        "# üñºÔ∏è Example Feature Map (4√ó4 matrix)\n",
        "\n",
        "$$\n",
        "F = \\begin{bmatrix}\n",
        "1 & 3 & 2 & 4 \\\\\n",
        "5 & 6 & 7 & 8 \\\\\n",
        "2 & 4 & 1 & 3 \\\\\n",
        "0 & 1 & 5 & 6\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "# üîé Pooling Setup\n",
        "\n",
        "* **Window size:** 2√ó2\n",
        "* **Stride:** 2 (non-overlapping pooling)\n",
        "* **Operation:** Max Pooling (take maximum from each 2√ó2 region)\n",
        "# üßÆ Step 1: Divide into 2√ó2 regions\n",
        "\n",
        "1Ô∏è‚É£ Top-left:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 3 \\\\\n",
        "5 & 6\n",
        "\\end{bmatrix} \\quad \\text{‚Üí max = 6}\n",
        "$$\n",
        "\n",
        "2Ô∏è‚É£ Top-right:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "2 & 4 \\\\\n",
        "7 & 8\n",
        "\\end{bmatrix} \\quad \\text{‚Üí max = 8}\n",
        "$$\n",
        "\n",
        "3Ô∏è‚É£ Bottom-left:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "2 & 4 \\\\\n",
        "0 & 1\n",
        "\\end{bmatrix} \\quad \\text{‚Üí max = 4}\n",
        "$$\n",
        "\n",
        "4Ô∏è‚É£ Bottom-right:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 3 \\\\\n",
        "5 & 6\n",
        "\\end{bmatrix} \\quad \\text{‚Üí max = 6}\n",
        "$$\n",
        "# üßÆ Step 2: Pooling Result (2√ó2 matrix)\n",
        "\n",
        "$$\n",
        "P = \\begin{bmatrix}\n",
        "6 & 8 \\\\\n",
        "4 & 6\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "# üßë‚Äçüíª Step 3: Python Implementation\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# 4x4 Feature Map\n",
        "F = np.array([\n",
        "    [1, 3, 2, 4],\n",
        "    [5, 6, 7, 8],\n",
        "    [2, 4, 1, 3],\n",
        "    [0, 1, 5, 6]\n",
        "])\n",
        "\n",
        "# Pooling parameters\n",
        "pool_size = 2\n",
        "stride = 2\n",
        "output_size = F.shape[0] // pool_size\n",
        "P = np.zeros((output_size, output_size))\n",
        "\n",
        "# Max Pooling\n",
        "for i in range(0, F.shape[0], stride):\n",
        "    for j in range(0, F.shape[1], stride):\n",
        "        region = F[i:i+pool_size, j:j+pool_size]\n",
        "        P[i//stride, j//stride] = np.max(region)\n",
        "\n",
        "print(\"Max Pooled Result:\")\n",
        "print(P)\n",
        "```\n",
        "# üìä Output\n",
        "\n",
        "```\n",
        "[[6. 8.]\n",
        " [4. 6.]]\n",
        "```\n",
        "‚úÖ **In summary:**\n",
        "Max pooling with a 2√ó2 window on a 4√ó4 feature map **downsamples** it into a 2√ó2 output by keeping only the most significant features (max values).\n"
      ],
      "metadata": {
        "id": "RM3kkcySvHIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 Implement the ReLU activation function on a feature map\n",
        "Perfect! üöÄ Let‚Äôs implement the **ReLU activation function** on a feature map.\n",
        "# üîé **What is ReLU?**\n",
        "\n",
        "The **Rectified Linear Unit (ReLU)** is defined as:\n",
        "\n",
        "$$\n",
        "ReLU(x) = \\max(0, x)\n",
        "$$\n",
        "\n",
        "üëâ It keeps **positive values as they are** and replaces **negative values with 0**.\n",
        "This introduces **non-linearity** into CNNs and helps them learn complex patterns.\n",
        "# üñºÔ∏è Example Feature Map (before ReLU)\n",
        "\n",
        "$$\n",
        "F = \\begin{bmatrix}\n",
        "1 & -2 & 3 \\\\\n",
        "-1 & 5 & -4 \\\\\n",
        "2 & -3 & 6\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "# üßÆ Step 1: Apply ReLU Element-wise\n",
        "\n",
        "$$\n",
        "ReLU(F) = \\begin{bmatrix}\n",
        "\\max(0,1) & \\max(0,-2) & \\max(0,3) \\\\\n",
        "\\max(0,-1) & \\max(0,5) & \\max(0,-4) \\\\\n",
        "\\max(0,2) & \\max(0,-3) & \\max(0,6)\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\begin{bmatrix}\n",
        "1 & 0 & 3 \\\\\n",
        "0 & 5 & 0 \\\\\n",
        "2 & 0 & 6\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "# üßë‚Äçüíª Step 2: Python Implementation\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Feature Map (before ReLU)\n",
        "F = np.array([\n",
        "    [1, -2, 3],\n",
        "    [-1, 5, -4],\n",
        "    [2, -3, 6]\n",
        "])\n",
        "\n",
        "# Apply ReLU\n",
        "ReLU = np.maximum(0, F)\n",
        "\n",
        "print(\"Feature Map after ReLU:\")\n",
        "print(ReLU)\n",
        "```\n",
        "# üìä Output\n",
        "\n",
        "```\n",
        "[[1 0 3]\n",
        " [0 5 0]\n",
        " [2 0 6]]\n",
        "```\n",
        "‚úÖ **In summary:**\n",
        "ReLU introduces **non-linearity**, removes negative values, and helps CNNs learn **complex decision boundaries** more effectively."
      ],
      "metadata": {
        "id": "rabuJr5kvqYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 Create a simple CNN model with one convolutional layer and a fully connected layer, using random data\n",
        "Got it üëç Let‚Äôs build a **very simple CNN** from scratch using **PyTorch**.\n",
        "We‚Äôll:\n",
        "\n",
        "* Use **random data** (since no dataset is provided).\n",
        "* Create a CNN with **1 convolutional layer** and **1 fully connected layer**.\n",
        "# üßë‚Äçüíª PyTorch Implementation\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 1. Define a simple CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # Convolutional layer: 1 input channel, 6 output filters, kernel size 3x3\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3)\n",
        "        # Fully connected layer: flatten conv output and map to 10 classes\n",
        "        self.fc1 = nn.Linear(6 * 26 * 26, 10)  # (assuming input is 28x28)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)         # Convolution\n",
        "        x = torch.relu(x)         # Apply ReLU\n",
        "        x = x.view(x.size(0), -1) # Flatten\n",
        "        x = self.fc1(x)           # Fully connected layer\n",
        "        return x\n",
        "\n",
        "# 2. Create random input data (batch_size=5, channels=1, image=28x28)\n",
        "inputs = torch.randn(5, 1, 28, 28)\n",
        "labels = torch.randint(0, 10, (5,))  # Random target labels (10 classes)\n",
        "\n",
        "# 3. Initialize model, loss function, optimizer\n",
        "model = SimpleCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 4. Forward pass\n",
        "outputs = model(inputs)\n",
        "loss = criterion(outputs, labels)\n",
        "\n",
        "# 5. Backward pass + Optimization\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(\"Model output shape:\", outputs.shape)\n",
        "print(\"Loss value:\", loss.item())\n",
        "``\n",
        "# üìä **Expected Output**\n",
        "\n",
        "* `outputs.shape ‚Üí torch.Size([5, 10])`\n",
        "  (because we have **5 samples** in the batch and **10 class predictions** per sample).\n",
        "* A single **loss value** (random since data is random).\n",
        "‚úÖ **Summary**:\n",
        "We built a **toy CNN** with:\n",
        "\n",
        "* **One convolutional layer** ‚Üí extracts local patterns.\n",
        "* **ReLU activation** ‚Üí adds non-linearity.\n",
        "* **Flatten ‚Üí Fully connected layer** ‚Üí predicts class scores.\n"
      ],
      "metadata": {
        "id": "f222OjxLwF_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5  Generate a synthetic dataset using random noise and train a simple CNN model on it\n",
        "Perfect üöÄ Let‚Äôs generate a **synthetic dataset using random noise** and then train a **simple CNN** on it. Since the dataset is random, the model won‚Äôt actually learn meaningful features, but it‚Äôs great for **practice and debugging CNN pipelines**.\n",
        "# üßë‚Äçüíª PyTorch Implementation\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Create Synthetic Dataset\n",
        "# ---------------------------\n",
        "num_samples = 1000\n",
        "num_classes = 10\n",
        "img_size = 28\n",
        "\n",
        "# Random images (noise)\n",
        "X = torch.randn(num_samples, 1, img_size, img_size)  # shape: (N, C, H, W)\n",
        "\n",
        "# Random labels (0-9)\n",
        "y = torch.randint(0, num_classes, (num_samples,))\n",
        "\n",
        "# Create dataset & dataloader\n",
        "dataset = TensorDataset(X, y)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Define Simple CNN Model\n",
        "# ---------------------------\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1)  # output (8,28,28)\n",
        "        self.pool = nn.MaxPool2d(2, 2)                                   # output (8,14,14)\n",
        "        self.fc1 = nn.Linear(8 * 14 * 14, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))  # Conv + ReLU\n",
        "        x = self.pool(x)               # Pooling\n",
        "        x = x.view(x.size(0), -1)      # Flatten\n",
        "        x = self.fc1(x)                # Fully connected layer\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN()\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Training Setup\n",
        "# ---------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Train Model\n",
        "# ---------------------------\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
        "```\n",
        "# üìä **Expected Output**\n",
        "\n",
        "Since the dataset is **pure noise**, the model can‚Äôt learn useful patterns. You‚Äôll see:\n",
        "\n",
        "* The **loss decreasing a little** (but not much).\n",
        "* Training will look random since data has no structure.\n",
        "‚úÖ **Summary**\n",
        "\n",
        "* We generated **synthetic images** (random noise).\n",
        "* Assigned **random labels**.\n",
        "* Trained a **simple CNN** with:\n",
        "\n",
        "  * `Conv2d ‚Üí ReLU ‚Üí MaxPool ‚Üí Fully Connected`.\n",
        "\n",
        "This demonstrates the **full CNN training pipeline** with PyTorch."
      ],
      "metadata": {
        "id": "dz575HY-weAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 Create a simple CNN using Keras with one convolution layer and a max-pooling layer\n",
        "Great choice üëç Let‚Äôs build a **simple CNN in Keras (TensorFlow)** with just:\n",
        "\n",
        "* **One convolution layer**\n",
        "* **One max-pooling layer**\n",
        "* A **fully connected layer** for classification\n",
        "\n",
        "We‚Äôll use **random data** for demonstration.\n",
        "# üßë‚Äçüíª Keras Implementation\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Generate Synthetic Data\n",
        "# ---------------------------\n",
        "num_samples = 1000\n",
        "img_size = 28\n",
        "num_classes = 10\n",
        "\n",
        "# Random images (grayscale, 28x28)\n",
        "X = np.random.rand(num_samples, img_size, img_size, 1).astype(\"float32\")\n",
        "\n",
        "# Random labels (0-9), one-hot encoded\n",
        "y = tf.keras.utils.to_categorical(np.random.randint(0, num_classes, num_samples), num_classes)\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Define Simple CNN Model\n",
        "# ---------------------------\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(8, (3,3), activation='relu', input_shape=(28,28,1)), # Conv layer\n",
        "    layers.MaxPooling2D((2,2)),                                       # Max Pooling\n",
        "    layers.Flatten(),                                                 # Flatten\n",
        "    layers.Dense(32, activation='relu'),                              # Fully connected layer\n",
        "    layers.Dense(num_classes, activation='softmax')                   # Output layer\n",
        "])\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Compile Model\n",
        "# ---------------------------\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Train Model\n",
        "# ---------------------------\n",
        "history = model.fit(X, y, epochs=5, batch_size=32, verbose=1)\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Evaluate Model\n",
        "# ---------------------------\n",
        "loss, acc = model.evaluate(X, y, verbose=0)\n",
        "print(f\"Final Training Accuracy: {acc:.4f}\")\n",
        "```\n",
        "# üìä **Expected Output**\n",
        "\n",
        "* Model summary will show:\n",
        "\n",
        "  * Conv2D ‚Üí MaxPooling2D ‚Üí Flatten ‚Üí Dense ‚Üí Dense\n",
        "* Training will run for 5 epochs.\n",
        "* Since data is **random noise**, accuracy will hover around **10% (random guess for 10 classes)**.\n",
        "‚úÖ **Summary:**\n",
        "This Keras CNN includes:\n",
        "\n",
        "* **Conv2D** ‚Üí feature extraction\n",
        "* **MaxPooling2D** ‚Üí downsampling\n",
        "* **Dense layers** ‚Üí classification\n"
      ],
      "metadata": {
        "id": "eIAe7NUDw56Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 Write a code to add a fully connected layer after the convolution and max-pooling layers in a CNN\n",
        "Perfect üëç Let‚Äôs extend the CNN:\n",
        "\n",
        "We‚Äôll build a **Keras CNN** with:\n",
        "\n",
        "1. **Conv2D layer**\n",
        "2. **MaxPooling2D layer**\n",
        "3. **Fully Connected (Dense) layer** after flattening\n",
        "4. **Output layer**\n",
        "# üßë‚Äçüíª Keras Code (with Fully Connected Layer)\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Generate Synthetic Data\n",
        "# ---------------------------\n",
        "num_samples = 1000\n",
        "img_size = 28\n",
        "num_classes = 10\n",
        "\n",
        "# Random grayscale images (28x28x1)\n",
        "X = np.random.rand(num_samples, img_size, img_size, 1).astype(\"float32\")\n",
        "\n",
        "# Random labels (0-9), one-hot encoded\n",
        "y = tf.keras.utils.to_categorical(np.random.randint(0, num_classes, num_samples), num_classes)\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Define CNN Model\n",
        "# ---------------------------\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,1)),  # Conv layer\n",
        "    layers.MaxPooling2D((2,2)),                                         # Max pooling\n",
        "    layers.Flatten(),                                                   # Flatten feature maps\n",
        "    layers.Dense(64, activation='relu'),                                # Fully connected (Dense) layer\n",
        "    layers.Dense(num_classes, activation='softmax')                     # Output layer\n",
        "])\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Compile Model\n",
        "# ---------------------------\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Train Model\n",
        "# ---------------------------\n",
        "history = model.fit(X, y, epochs=5, batch_size=32, verbose=1)\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Evaluate Model\n",
        "# ---------------------------\n",
        "loss, acc = model.evaluate(X, y, verbose=0)\n",
        "print(\"Final Training Accuracy:\", round(acc, 4))\n",
        "\n",
        "# ---------------------------\n",
        "# 6. Print Model Summary\n",
        "# ---------------------------\n",
        "model.summary()\n",
        "```\n",
        "# üìä **Expected Behavior**\n",
        "\n",
        "* **Conv2D ‚Üí MaxPooling2D ‚Üí Flatten ‚Üí Dense(64) ‚Üí Dense(10)**\n",
        "* Training accuracy will stay near **10%** since data is random.\n",
        "* `model.summary()` will show parameter counts for each layer.\n",
        "‚úÖ **Summary:**\n",
        "You now have a CNN with:\n",
        "\n",
        "* **Feature extraction (Conv + Pooling)**\n",
        "* **Flattening**\n",
        "* **Fully connected Dense layer (64 units, ReLU)**\n",
        "* **Output layer (Softmax for classification)*\n"
      ],
      "metadata": {
        "id": "R6gyYvKqxKT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8  Write a code to add  batch normalization to a simple CNN model\n",
        "Great question üëç Batch Normalization (BN) helps stabilize and speed up training by normalizing activations across mini-batches.\n",
        "\n",
        "Let‚Äôs build a **simple CNN in Keras** with **BatchNormalization** added after the convolution and dense layers.\n",
        "# üßë‚Äçüíª Keras Code with Batch Normalization\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Generate Synthetic Data\n",
        "# ---------------------------\n",
        "num_samples = 1000\n",
        "img_size = 28\n",
        "num_classes = 10\n",
        "\n",
        "# Random grayscale images (28x28x1)\n",
        "X = np.random.rand(num_samples, img_size, img_size, 1).astype(\"float32\")\n",
        "\n",
        "# Random labels (0-9), one-hot encoded\n",
        "y = tf.keras.utils.to_categorical(np.random.randint(0, num_classes, num_samples), num_classes)\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Define CNN Model with Batch Normalization\n",
        "# ---------------------------\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    layers.BatchNormalization(),                 # BN after Conv layer\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.BatchNormalization(),                 # BN after Conv layer\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.BatchNormalization(),                 # BN after Dense layer\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Compile Model\n",
        "# ---------------------------\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Train Model\n",
        "# ---------------------------\n",
        "history = model.fit(X, y, epochs=5, batch_size=32, verbose=1)\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Evaluate Model\n",
        "# ---------------------------\n",
        "loss, acc = model.evaluate(X, y, verbose=0)\n",
        "print(\"Final Training Accuracy:\", round(acc, 4))\n",
        "\n",
        "# ---------------------------\n",
        "# 6. Model Summary\n",
        "# ---------------------------\n",
        "model.summary()\n",
        "```\n",
        "# ‚úÖ Key Notes:\n",
        "\n",
        "* `BatchNormalization()` is usually placed **after Conv/Dense layers** but **before activation functions** (here we left `activation='relu'`, but in practice you can set activation separately: `use_bias=False` in the layer, then BN, then ReLU).\n",
        "* BN makes training **faster, more stable**, and allows higher learning rates\n"
      ],
      "metadata": {
        "id": "Zu0Q5MAuxh2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9 Write a code to add dropout regularization to a simple CNN mode|\n",
        "Perfect üëç Dropout is a regularization technique that randomly ‚Äúdrops‚Äù some neurons during training to prevent **overfitting**.\n",
        "\n",
        "Let‚Äôs build a **simple CNN in Keras** with **Dropout layers** added.\n",
        "# üßë‚Äçüíª Keras Code with Dropout\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Generate Synthetic Data\n",
        "# ---------------------------\n",
        "num_samples = 1000\n",
        "img_size = 28\n",
        "num_classes = 10\n",
        "\n",
        "# Random grayscale images (28x28x1)\n",
        "X = np.random.rand(num_samples, img_size, img_size, 1).astype(\"float32\")\n",
        "\n",
        "# Random labels (0-9), one-hot encoded\n",
        "y = tf.keras.utils.to_categorical(np.random.randint(0, num_classes, num_samples), num_classes)\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Define CNN Model with Dropout\n",
        "# ---------------------------\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Dropout(0.25),                       # Dropout after pooling\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Dropout(0.25),                       # Dropout after second pooling\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),                        # Dropout before fully connected\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Compile Model\n",
        "# ---------------------------\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Train Model\n",
        "# ---------------------------\n",
        "history = model.fit(X, y, epochs=5, batch_size=32, verbose=1)\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Evaluate Model\n",
        "# ---------------------------\n",
        "loss, acc = model.evaluate(X, y, verbose=0)\n",
        "print(\"Final Training Accuracy:\", round(acc, 4))\n",
        "\n",
        "# ---------------------------\n",
        "# 6. Model Summary\n",
        "# ---------------------------\n",
        "model.summary()\n",
        "```\n",
        "# ‚úÖ Key Notes:\n",
        "\n",
        "* `Dropout(0.25)` ‚Üí randomly drops **25%** of neurons.\n",
        "* Common usage:\n",
        "\n",
        "  * After **Conv + Pooling layers** ‚Üí moderate dropout (0.25‚Äì0.3).\n",
        "  * Before **Dense layers** ‚Üí higher dropout (0.5).\n",
        "* Helps **reduce overfitting**, especially on small datasets.\n"
      ],
      "metadata": {
        "id": "zvpLzgDxx5U3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 Write a code to print the architecture of the VGG16 model in Keras*\n",
        "Great question üëç Keras provides the **VGG16** architecture as a pre-built model in `tensorflow.keras.applications`.\n",
        "You can easily **load it** and print the architecture.\n",
        " üßë‚Äçüíª Code to Print VGG16 Architecture\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Load VGG16 Model\n",
        "# ---------------------------\n",
        "# include_top=True ‚Üí loads FC layers at the top\n",
        "# weights=None ‚Üí random initialization (no pretrained weights)\n",
        "model = VGG16(include_top=True, weights=None, input_shape=(224, 224, 3), classes=1000)\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Print Model Summary\n",
        "# ---------------------------\n",
        "model.summary()\n",
        "```\n",
        "# ‚úÖ Key Notes:\n",
        "\n",
        "* `include_top=True` ‚Üí loads the **3 fully connected layers** + softmax at the top.\n",
        "* `weights=None` ‚Üí initializes with **random weights**. If you want **ImageNet pretrained weights**, use `weights=\"imagenet\"`.\n",
        "* `input_shape=(224,224,3)` ‚Üí default size for VGG16 images.\n"
      ],
      "metadata": {
        "id": "nDPC4HpayUfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11 Write a code to plot the accuracy and loss graphs after training a CNN model\u0016\n",
        "Perfect üëç After training a CNN (or any Keras model), the `fit()` function returns a **History object** that stores the loss and accuracy for each epoch.\n",
        "We can use **Matplotlib** to plot training curves.\n",
        "# üßë‚Äçüíª Code: Plot Accuracy & Loss Graphs\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Generate Synthetic Data\n",
        "# ---------------------------\n",
        "num_samples = 1000\n",
        "img_size = 28\n",
        "num_classes = 10\n",
        "\n",
        "X = np.random.rand(num_samples, img_size, img_size, 1).astype(\"float32\")\n",
        "y = tf.keras.utils.to_categorical(np.random.randint(0, num_classes, num_samples), num_classes)\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Define a Simple CNN\n",
        "# ---------------------------\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Train Model\n",
        "# ---------------------------\n",
        "history = model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Plot Accuracy & Loss\n",
        "# ---------------------------\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# Accuracy Plot\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "# Loss Plot\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "```\n",
        "# ‚úÖ Output:\n",
        "\n",
        "* **Left graph** ‚Üí Training vs Validation Accuracy.\n",
        "* **Right graph** ‚Üí Training vs Validation Loss.\n",
        "\n",
        "This helps check if the model is **overfitting** (training accuracy much higher than validation accuracy) or if training is stable.\n"
      ],
      "metadata": {
        "id": "F9hL98cfy3vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12 Write a code to print the architecture of the ResNet50 model in Keras*\n",
        "Great question üöÄ\n",
        "Keras provides **ResNet50** in `tensorflow.keras.applications`.\n",
        "You can easily **load it** and print its architecture summary.\n",
        "# üßë‚Äçüíª Code: Print ResNet50 Architecture\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Load ResNet50 Model\n",
        "# ---------------------------\n",
        "# include_top=True ‚Üí load the FC layers at the top\n",
        "# weights=None ‚Üí random initialization (no pretrained weights)\n",
        "model = ResNet50(include_top=True, weights=None, input_shape=(224, 224, 3), classes=1000)\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Print Model Summary\n",
        "# ---------------------------\n",
        "model.summary()\n",
        "```\n",
        "# ‚úÖ Key Notes:\n",
        "\n",
        "* `include_top=True` ‚Üí loads the **average pooling + fully connected (Dense) + softmax** at the top.\n",
        "* `weights=None` ‚Üí initializes random weights.\n",
        "\n",
        "  * If you want **ImageNet pretrained weights**, set `weights=\"imagenet\"`.\n",
        "* `input_shape=(224,224,3)` ‚Üí default size for ResNet50 images.\n"
      ],
      "metadata": {
        "id": "RnJ7pPQ4zJ47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13 Write a code to train a basic CNN model and print the training loss and accuracy after each epoch?\n",
        "Got it üëç In **Keras**, the `fit()` method automatically prints loss and accuracy after each epoch.\n",
        "But we can also **access them programmatically** from the `History` object and print them manually.\n",
        "\n",
        "Here‚Äôs a complete example:\n",
        "# üßë‚Äçüíª Code: Train a Basic CNN & Print Loss/Accuracy per Epoch\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Generate Synthetic Data\n",
        "# ---------------------------\n",
        "num_samples = 1000\n",
        "img_size = 28\n",
        "num_classes = 10\n",
        "\n",
        "X = np.random.rand(num_samples, img_size, img_size, 1).astype(\"float32\")\n",
        "y = tf.keras.utils.to_categorical(np.random.randint(0, num_classes, num_samples), num_classes)\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Define a Simple CNN\n",
        "# ---------------------------\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Train Model\n",
        "# ---------------------------\n",
        "history = model.fit(X, y, epochs=5, batch_size=32, verbose=0)  # verbose=0 ‚Üí suppress default printing\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Print Loss & Accuracy Manually\n",
        "# ---------------------------\n",
        "for epoch in range(len(history.history['loss'])):\n",
        "    loss = history.history['loss'][epoch]\n",
        "    acc = history.history['accuracy'][epoch]\n",
        "    print(f\"Epoch {epoch+1} - Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n",
        "```\n",
        "# ‚úÖ What Happens:\n",
        "\n",
        "* Trains a **tiny CNN** on synthetic data.\n",
        "* After each epoch, prints:\n",
        "\n",
        "  ```\n",
        "  Epoch 1 - Loss: 2.3451, Accuracy: 0.1050\n",
        "  Epoch 2 - Loss: 2.3021, Accuracy: 0.1120\n",
        "  ...\n",
        "  ```\n"
      ],
      "metadata": {
        "id": "TQvY6zVizlDF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}