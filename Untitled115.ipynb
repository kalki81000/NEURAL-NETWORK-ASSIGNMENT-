{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN48D1Hz+nzSiTlS/7ZpjxZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalki81000/NEURAL-NETWORK-ASSIGNMENT-/blob/main/Untitled115.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ##Neural Network A Simple Perception\n",
        ""
      ],
      "metadata": {
        "id": "jyh9s-cATuPb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0Y2Zl-PykiE"
      },
      "outputs": [],
      "source": [
        "#)1 What is deep learning, and how is it connected to artificial intelligence\"\n",
        "Deep learning is a **subset of machine learning**, which itself is a branch of **artificial intelligence (AI)**.\n",
        "\n",
        "Hereâ€™s how they connect:\n",
        "\n",
        "1. **Artificial Intelligence (AI)** â€“ This is the broad field concerned with creating systems that can perform tasks normally requiring human intelligence, such as understanding language, recognizing images, or making decisions.\n",
        "\n",
        "2. **Machine Learning (ML)** â€“ A subset of AI that focuses on algorithms that allow machines to learn from data rather than being explicitly programmed for every rule.\n",
        "\n",
        "3. **Deep Learning (DL)** â€“ A further subset of ML that uses **artificial neural networks** with many layers (â€œdeepâ€ networks) to automatically learn complex patterns from large amounts of data.\n",
        "\n",
        "**Key points about deep learning:**\n",
        "\n",
        "* **Neural networks** are inspired by the human brainâ€™s structure, with interconnected â€œneuronsâ€ that process information in layers.\n",
        "* Itâ€™s particularly powerful for **image recognition**, **speech recognition**, **natural language processing**, and **autonomous systems**.\n",
        "* Unlike traditional ML, deep learning can **automatically extract features** from raw data, reducing the need for manual feature engineering.\n",
        "* It thrives on **large datasets** and **high computing power** (e.g., GPUs).\n",
        "\n",
        "**Hierarchy analogy:**\n",
        "\n",
        "> AI â†’ Machine Learning â†’ Deep Learning\n",
        "> (like â€œMathematics â†’ Algebra â†’ Linear Algebraâ€ â€” deep learning is a specialized area inside a specialized area)\n",
        "\n",
        "If you want, I can make you a **simple diagram showing the relationship between AI, ML, and DL** so itâ€™s instantly clear. Would you like me to do that?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 What is the mathematical structure of a neural network!\n",
        "The **mathematical structure** of a neural network can be described using **linear algebra and functions**.\n",
        "At its core, a neural network is a **composition of functions** where each layer applies a **linear transformation** followed by a **non-linear activation**.\n",
        "\n",
        "## **1. Neuron (Single Unit)**\n",
        "\n",
        "A single neuron takes an input vector **x** and produces an output **y**:\n",
        "\n",
        "$$\n",
        "z = \\sum_{i=1}^{n} w_i x_i + b\n",
        "$$\n",
        "\n",
        "* $x_i$ â†’ input feature\n",
        "* $w_i$ â†’ weight for each input\n",
        "* $b$ â†’ bias term\n",
        "* $z$ â†’ weighted sum (pre-activation)\n",
        "\n",
        "Then apply an **activation function** $f(\\cdot)$:\n",
        "\n",
        "$$\n",
        "y = f(z) = f\\left( \\sum_{i=1}^{n} w_i x_i + b \\right)\n",
        "$$\n",
        "\n",
        "\n",
        "## **2. Layer**\n",
        "\n",
        "For a layer with **m** neurons receiving **n** inputs:\n",
        "\n",
        "1. Inputs as a column vector:\n",
        "\n",
        "$$\n",
        "\\mathbf{x} =\n",
        "\\begin{bmatrix}\n",
        "x_1 \\\\\n",
        "x_2 \\\\\n",
        "\\vdots \\\\\n",
        "x_n\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. Weight matrix:\n",
        "\n",
        "$$\n",
        "\\mathbf{W} =\n",
        "\\begin{bmatrix}\n",
        "w_{11} & w_{12} & \\dots & w_{1n} \\\\\n",
        "w_{21} & w_{22} & \\dots & w_{2n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "w_{m1} & w_{m2} & \\dots & w_{mn}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "(size: $m \\times n$)\n",
        "\n",
        "3. Bias vector:\n",
        "\n",
        "$$\n",
        "\\mathbf{b} =\n",
        "\\begin{bmatrix}\n",
        "b_1 \\\\\n",
        "b_2 \\\\\n",
        "\\vdots \\\\\n",
        "b_m\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "4. Layer operation:\n",
        "\n",
        "$$\n",
        "\\mathbf{z} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = f(\\mathbf{z})\n",
        "$$\n",
        "\n",
        "Where $f$ is applied element-wise.\n",
        "\n",
        "## **3. Multi-Layer Neural Network**\n",
        "\n",
        "If we have $L$ layers:\n",
        "\n",
        "* **Layer 1**:\n",
        "\n",
        "$$\n",
        "\\mathbf{a}^{(1)} = f^{(1)}(\\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)})\n",
        "$$\n",
        "\n",
        "* **Layer 2**:\n",
        "\n",
        "$$\n",
        "\\mathbf{a}^{(2)} = f^{(2)}(\\mathbf{W}^{(2)} \\mathbf{a}^{(1)} + \\mathbf{b}^{(2)})\n",
        "$$\n",
        "\n",
        "* And so on, until the output layer $\\mathbf{a}^{(L)}$.\n",
        "\n",
        "**General formula** for layer $l$:\n",
        "\n",
        "$$\n",
        "\\mathbf{a}^{(l)} = f^{(l)} \\left( \\mathbf{W}^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)} \\right)\n",
        "$$\n",
        "\n",
        "with $\\mathbf{a}^{(0)} = \\mathbf{x}$ (the input vector).\n",
        "\n",
        "\n",
        "## **4. Overall Function**\n",
        "\n",
        "A neural network essentially computes:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{y}} = F(\\mathbf{x}; \\Theta)\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $\\mathbf{x}$ = input vector\n",
        "* $\\Theta = \\{\\mathbf{W}^{(l)}, \\mathbf{b}^{(l)}\\}_{l=1}^{L}$ = all learnable parameters\n",
        "* $F$ = composition of linear transformations + nonlinear activations\n",
        "\n",
        "## **5. Training (Mathematical Side)**\n",
        "\n",
        "* **Loss Function** $\\mathcal{L}(\\hat{\\mathbf{y}}, \\mathbf{y})$ measures error.\n",
        "* Parameters $\\Theta$ are updated using **gradient descent**:\n",
        "\n",
        "$$\n",
        "\\Theta \\leftarrow \\Theta - \\eta \\, \\nabla_{\\Theta} \\mathcal{L}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "3DkOcDJzz_mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 What is an activation function, and why is it essential in neural\"\n",
        "An **activation function** is a **mathematical function** applied to the output of a neuron in a neural network to decide **whether the neuron should be â€œactivatedâ€** and how it should transform its input.\n",
        "\n",
        "In simple terms, it introduces **non-linearity** into the network so it can learn **complex patterns** instead of just linear relationships.\n",
        "\n",
        "\n",
        "## **1. Mathematical Definition**\n",
        "\n",
        "If a neuron computes:\n",
        "\n",
        "$$\n",
        "z = \\sum_{i=1}^n w_i x_i + b\n",
        "$$\n",
        "\n",
        "then the activation function $f(\\cdot)$ produces:\n",
        "\n",
        "$$\n",
        "a = f(z)\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $z$ = weighted sum (linear)\n",
        "* $a$ = neuronâ€™s output after activation (possibly non-linear)\n",
        "\n",
        "## **2. Why Itâ€™s Essential**\n",
        "\n",
        "1. **Introduces Non-Linearity**\n",
        "\n",
        "   * Without activation functions, a neural network is just a **stack of linear equations**, which collapses into a single linear transformationâ€”limiting its ability to model real-world problems.\n",
        "\n",
        "2. **Allows Complex Decision Boundaries**\n",
        "\n",
        "   * Non-linear activations enable networks to classify data thatâ€™s not linearly separable.\n",
        "\n",
        "3. **Enables Deep Learning**\n",
        "\n",
        "   * Multi-layer networks with non-linear activations can approximate **any continuous function** (Universal Approximation Theorem).\n",
        "\n",
        "4. **Controls Signal Flow**\n",
        "\n",
        "   * Some activations help avoid problems like exploding or vanishing gradients.\n",
        "\n",
        "## **3. Common Activation Functions**\n",
        "\n",
        "| Function       | Formula                                    | Range        | Key Features                                           | Use Case                        |\n",
        "| -------------- | ------------------------------------------ | ------------ | ------------------------------------------------------ | ------------------------------- |\n",
        "| **Sigmoid**    | $f(z) = \\frac{1}{1+e^{-z}}$                | (0,1)        | Smooth, squashes values; can cause vanishing gradients | Binary classification           |\n",
        "| **Tanh**       | $f(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$ | (-1,1)       | Centered at 0; still suffers vanishing gradient        | Hidden layers                   |\n",
        "| **ReLU**       | $f(z) = \\max(0, z)$                        | \\[0,âˆž)       | Fast, prevents vanishing gradient for positive values  | Most hidden layers              |\n",
        "| **Leaky ReLU** | $f(z) = \\max(\\alpha z, z)$                 | (-âˆž,âˆž)       | Allows small negative slope                            | Solves ReLU â€œdead neuronâ€ issue |\n",
        "| **Softmax**    | $f(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$  | (0,1), sum=1 | Converts vector to probability distribution            | Output layer for multi-class    |\n",
        "\n",
        "## **4. Without Activation Functions**\n",
        "\n",
        "If you remove activation functions, the network becomes:\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = \\mathbf{W}^{(n)} \\dots \\mathbf{W}^{(2)} \\mathbf{W}^{(1)} \\mathbf{x} + \\text{bias terms}\n",
        "$$\n",
        "\n",
        "This is still **just one linear transformation**, no matter how many layersâ€”so it can only model straight-line relationships.\n"
      ],
      "metadata": {
        "id": "pJa1AA-r0yDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Could you list some common activation functions used in neural networks!\n",
        ". Sigmoid Family\n",
        "Sigmoid / Logistic Function\n",
        "\n",
        "Range: (0, 1)\n",
        "Use: Binary classification outputs, probability mapping.\n",
        "\n",
        "Tanh (Hyperbolic Tangent)\n",
        "Range: (-1, 1)\n",
        "Use: Hidden layers where centered output is beneficial.\n",
        "\n",
        "2. ReLU Variants\n",
        "ReLU (Rectified Linear Unit)\n",
        "\n",
        "f(z)=max(0,z)\n",
        "Range: [0, âˆž)\n",
        "Use: Most hidden layers in deep networks.\n",
        "\n",
        "Leaky ReLU\n",
        "â‰ˆ\n",
        "0.01\n",
        "f(z)=max(Î±z,z),Î±â‰ˆ0.01\n",
        "Range: (-âˆž, âˆž)\n",
        "Use: Prevents â€œdead neuronsâ€ in ReLU.\n",
        "\n",
        "Parametric ReLU (PReLU)\n",
        "Like Leaky ReLU, but\n",
        "ð›¼\n",
        "Î± is learned during training.\n",
        "\n",
        "ELU (Exponential Linear Unit)\n",
        "\n",
        "Smooths negative side for better gradient flow.\n",
        "\n",
        "3. Softmax and Probability Functions\n",
        "Softmax\n",
        "Use: Multi-class classification output layer.\n",
        "\n",
        "LogSoftmax\n",
        "Applies log to softmax output â€” better numerical stability.\n",
        "\n",
        "4. Advanced / Modern\n",
        "Swish\n",
        "f(z)=zâ‹…Ïƒ(z)\n",
        "Smooth, self-gated â€” used in Googleâ€™s EfficientNet.\n",
        "\n",
        "GELU (Gaussian Error Linear Unit)\n",
        "Combines ReLU + sigmoid-like smoothness â€” used in Transformer models like BERT.\n",
        "\n",
        "Maxout\n",
        "Outputs the maximum of several linear functions â€” adapts to different activation shapes."
      ],
      "metadata": {
        "id": "h6idlhV81x4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5 What is a multilayer neural network!\n",
        "A **Multilayer Neural Network** (also called a **Multilayer Perceptron â€“ MLP**) is a type of neural network that has **two or more layers of neurons** between the input and output.\n",
        "\n",
        "Itâ€™s the simplest form of a **deep neural network**, and it learns by combining **linear transformations** with **non-linear activation functions** in multiple stages.\n",
        "\n",
        "## **Structure**\n",
        "\n",
        "1. **Input Layer** â€“ Receives raw data (features).\n",
        "2. **Hidden Layers** â€“ One or more layers that process data through weighted connections and activation functions.\n",
        "3. **Output Layer** â€“ Produces the final prediction or classification.\n",
        "\n",
        "### **Mathematical Flow**\n",
        "\n",
        "If we have:\n",
        "\n",
        "* $\\mathbf{x}$ = input vector\n",
        "* $\\mathbf{W}^{(l)}$, $\\mathbf{b}^{(l)}$ = weights and biases for layer $l$\n",
        "* $f^{(l)}$ = activation function for layer $l$\n",
        "\n",
        "Then for each layer:\n",
        "\n",
        "$$\n",
        "\\mathbf{a}^{(l)} = f^{(l)}\\left(\\mathbf{W}^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)}\\right)\n",
        "$$\n",
        "\n",
        "with $\\mathbf{a}^{(0)} = \\mathbf{x}$.\n",
        "\n",
        "## **Key Features**\n",
        "\n",
        "* **Multiple hidden layers** â†’ More representation power.\n",
        "* **Non-linear activations** â†’ Can model complex patterns.\n",
        "* **Fully connected** â†’ Every neuron in one layer connects to every neuron in the next (in standard MLPs).\n",
        "\n",
        "## **Advantages**\n",
        "\n",
        "âœ… Can approximate any continuous function (**Universal Approximation Theorem**).\n",
        "âœ… Handles non-linear and complex relationships.\n",
        "âœ… Versatile â€” works for regression, classification, and more.\n",
        "\n",
        "## **Limitations**\n",
        "\n",
        "âŒ Prone to **overfitting** if too large and not regularized.\n",
        "âŒ Can be computationally expensive.\n",
        "âŒ Requires careful tuning of learning rate, activation functions, and number of layers.\n",
        "\n",
        "## **Example**\n",
        "\n",
        "A 3-layer neural network (1 input layer, 1 hidden layer, 1 output layer) for predicting whether an email is spam:\n",
        "\n",
        "* **Input Layer:** Features like word frequency, sender domain, presence of links.\n",
        "* **Hidden Layer:** Processes feature interactions.\n",
        "* **Output Layer:** Probability of â€œspamâ€ vs â€œnot spam\n"
      ],
      "metadata": {
        "id": "Ytv28F0X2pwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6What is a loss function, and why is it crucial for neural network training!\n",
        "A **loss function** (also called a **cost function** or **objective function**) is a mathematical formula that measures **how far a neural networkâ€™s predictions are from the actual target values**.\n",
        "\n",
        "Itâ€™s the **guide** that tells the network how wrong it is, so it knows how to adjust its weights during training.\n",
        "\n",
        "## **1. Mathematical Definition**\n",
        "\n",
        "If:\n",
        "\n",
        "* $\\hat{y}$ = predicted output of the network\n",
        "* $y$ = true (target) value\n",
        "* $\\mathcal{L}$ = loss function\n",
        "\n",
        "Then the loss is:\n",
        "\n",
        "$$\n",
        "\\text{Loss} = \\mathcal{L}(y, \\hat{y})\n",
        "$$\n",
        "\n",
        "## **2. Why Itâ€™s Crucial**\n",
        "\n",
        "1. **Training Signal** â€“ The loss function is the **feedback** that drives learning.\n",
        "\n",
        "   * High loss â†’ large errors â†’ big weight updates.\n",
        "   * Low loss â†’ smaller errors â†’ small weight updates.\n",
        "2. **Optimization Goal** â€“ Training a neural network is about **minimizing** this loss over the training data:\n",
        "\n",
        "$$\n",
        "\\min_{\\Theta} \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}(y_i, \\hat{y}_i)\n",
        "$$\n",
        "\n",
        "where $\\Theta$ = all weights & biases.\n",
        "\n",
        "3. **Direction for Gradient Descent** â€“ Backpropagation computes gradients of the loss with respect to parameters, so without a loss function, the network wouldnâ€™t know **how to improve**.\n",
        "## **3. Common Loss Functions**\n",
        "\n",
        "### **For Regression**\n",
        "\n",
        "* **Mean Squared Error (MSE)**:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "* **Mean Absolute Error (MAE)**:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|\n",
        "$$\n",
        "\n",
        "### **For Classification**\n",
        "\n",
        "* **Binary Cross-Entropy**:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i) \\right]\n",
        "$$\n",
        "\n",
        "* **Categorical Cross-Entropy** (for multi-class):\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = - \\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\n",
        "$$\n",
        "\n",
        "### **For Special Tasks**\n",
        "\n",
        "* **Hinge Loss** â†’ Support vectorâ€“style classification.\n",
        "* **Huber Loss** â†’ Robust regression with fewer outlier effects.\n",
        "## **4. Without a Loss Function**\n",
        "\n",
        "If a network didnâ€™t have a loss function, it would have **no measurable target** to improve toward.\n",
        "It would be like a student taking an exam and never getting the results â€” they wouldnâ€™t know what to study or how to get better.\n"
      ],
      "metadata": {
        "id": "LWQ_V89Z3M2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7 What are some common types of loss functions!\n",
        "Hereâ€™s a **quick categorized list** of common loss functions used in neural networks, with their main purposes:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Regression Loss Functions**\n",
        "\n",
        "Used when predicting **continuous values**.\n",
        "\n",
        "| Loss Function                 | Formula                                        | Key Feature                            | Use Case           |                    |                   |\n",
        "| ----------------------------- | ---------------------------------------------- | -------------------------------------- | ------------------ | ------------------ | ----------------- |\n",
        "| **Mean Squared Error (MSE)**  | $\\frac{1}{N} \\sum (y - \\hat{y})^2$             | Penalizes large errors more strongly   | General regression |                    |                   |\n",
        "| **Mean Absolute Error (MAE)** | ( \\frac{1}{N} \\sum                             | y - \\hat{y}                            | )                  | Robust to outliers | Robust regression |\n",
        "| **Huber Loss**                | Piecewise: MSE for small errors, MAE for large | Combines robustness & smooth gradients | Noisy regression   |                    |                   |\n",
        "| **Log-Cosh Loss**             | $\\sum \\log(\\cosh(y - \\hat{y}))$                | Smooth and less sensitive to outliers  | Stable regression  |                    |                   |\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Classification Loss Functions**\n",
        "\n",
        "Used when predicting **discrete classes**.\n",
        "\n",
        "| Loss Function                        | Formula                                    | Key Feature                        | Use Case                   |\n",
        "| ------------------------------------ | ------------------------------------------ | ---------------------------------- | -------------------------- |\n",
        "| **Binary Cross-Entropy (Log Loss)**  | $-[y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})]$ | Works with probabilities in (0,1)  | Binary classification      |\n",
        "| **Categorical Cross-Entropy**        | $-\\sum y_i \\log(\\hat{y}_i)$                | Multi-class probability prediction | Multi-class classification |\n",
        "| **Sparse Categorical Cross-Entropy** | Like categorical but with integer labels   | Memory-efficient                   | Large-class classification |\n",
        "| **Hinge Loss**                       | $\\max(0, 1 - y\\hat{y})$                    | Margin-based                       | SVM-style classification   |\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Ranking & Probability Losses**\n",
        "\n",
        "Used for ranking problems or probabilistic outputs.\n",
        "\n",
        "| Loss Function                                   | Key Feature                                                     | Use Case                                  |\n",
        "| ----------------------------------------------- | --------------------------------------------------------------- | ----------------------------------------- |\n",
        "| **Kullbackâ€“Leibler Divergence (KL Divergence)** | Measures how one probability distribution diverges from another | Variational autoencoders, language models |\n",
        "| **Contrastive Loss**                            | Pushes similar pairs together, dissimilar apart                 | Siamese networks                          |\n",
        "| **Triplet Loss**                                | Uses anchor-positive-negative triplets for embedding learning   | Face recognition                          |\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Specialized Losses**\n",
        "\n",
        "For tasks beyond standard classification/regression.\n",
        "\n",
        "| Loss Function                                        | Key Feature                                       | Use Case                                    |\n",
        "| ---------------------------------------------------- | ------------------------------------------------- | ------------------------------------------- |\n",
        "| **Dice Loss**                                        | Measures overlap between predicted & actual masks | Medical image segmentation                  |\n",
        "| **IoU Loss (Jaccard Loss)**                          | Intersection-over-union for shapes                | Object detection & segmentation             |\n",
        "| **Perceptual Loss**                                  | Compares high-level features instead of pixels    | Image style transfer                        |\n",
        "| **CTC Loss (Connectionist Temporal Classification)** | Allows training without exact alignment           | Speech recognition, handwriting recognition |\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can prepare a **cheat sheet table** with **formulas, graphs, pros/cons, and best-use cases** for all these loss functions so you can revise them quickly before exams.\n",
        "Do you want me to make that next?\n"
      ],
      "metadata": {
        "id": "0h9kkdop367y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8 How does a neural network learn!\n",
        "A **neural network learns** by **adjusting its weights and biases** so that its predictions become closer to the correct answers.\n",
        "This happens through a cycle of **forward pass â†’ loss calculation â†’ backward pass â†’ parameter update**.\n",
        "## **1. The Learning Process (Step-by-Step)**\n",
        "\n",
        "### **Step 1: Forward Propagation**\n",
        "\n",
        "* Input data ($\\mathbf{x}$) passes through the network layer by layer.\n",
        "* Each neuron computes:\n",
        "\n",
        "$$\n",
        "z = \\mathbf{W} \\mathbf{x} + \\mathbf{b}\n",
        "$$\n",
        "\n",
        "* An **activation function** $f(z)$ adds non-linearity.\n",
        "* The final output $\\hat{y}$ is the networkâ€™s prediction.\n",
        "### **Step 2: Loss Calculation**\n",
        "\n",
        "* The networkâ€™s output $\\hat{y}$ is compared to the actual target $y$ using a **loss function**:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(y, \\hat{y})\n",
        "$$\n",
        "\n",
        "* This gives a number that measures **how wrong** the network is.\n",
        "### **Step 3: Backpropagation**\n",
        "\n",
        "* The loss is propagated **backward** through the network to compute the **gradient** of the loss with respect to each weight and bias.\n",
        "* Uses the **chain rule of calculus**:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial W_{ij}} = \\frac{\\partial \\mathcal{L}}{\\partial a_j} \\cdot \\frac{\\partial a_j}{\\partial z_j} \\cdot \\frac{\\partial z_j}{\\partial W_{ij}}\n",
        "$$\n",
        "### **Step 4: Weight Update (Gradient Descent)**\n",
        "\n",
        "* The network updates parameters using:\n",
        "\n",
        "$$\n",
        "W \\leftarrow W - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b \\leftarrow b - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $\\eta$ = learning rate (step size)\n",
        "* Gradients come from backpropagation\n",
        "### **Step 5: Repeat**\n",
        "\n",
        "* This process is repeated for many **epochs** (full passes through the training data) until:\n",
        "\n",
        "  * The loss becomes small enough\n",
        "  * Or performance stops improving\n",
        "## **2. Summary Flow**\n",
        "\n",
        "1. **Forward pass** â†’ Get predictions.\n",
        "2. **Loss function** â†’ Measure error.\n",
        "3. **Backpropagation** â†’ Calculate gradients.\n",
        "4. **Gradient descent** â†’ Update weights.\n",
        "5. **Repeat** until the model learns patterns.\n",
        "## **3. Analogy**\n",
        "\n",
        "Think of it like **throwing darts blindfolded**:\n",
        "\n",
        "* You throw a dart (make a prediction).\n",
        "* Someone tells you how far you missed (loss function).\n",
        "* You adjust your aim based on feedback (backpropagation).\n",
        "* Over time, you hit closer to the bullseye (better prediction.\n"
      ],
      "metadata": {
        "id": "UhdhONsY4boH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9 What is an optimizer in neural networks, and why is it necessary!\n",
        "An **optimizer** in a neural network is an **algorithm** that updates the modelâ€™s weights and biases during training so that the **loss function is minimized**.\n",
        "\n",
        "In short:\n",
        "\n",
        "* **Loss function** â†’ tells us *how wrong* the model is.\n",
        "* **Optimizer** â†’ decides *how to change* the weights to get better.\n",
        "## **1. Why Itâ€™s Necessary**\n",
        "\n",
        "* Without an optimizer, the weights in the network wouldnâ€™t change, and the model would **never learn**.\n",
        "* Optimizers decide **direction** (which way to move in the loss landscape) and **magnitude** (how big a step to take).\n",
        "* They help find the set of parameters $\\Theta = \\{W, b\\}$ that make predictions most accurate.\n",
        "## **2. How It Works**\n",
        "\n",
        "During training:\n",
        "\n",
        "1. **Forward pass** â†’ Predictions are made.\n",
        "2. **Loss function** â†’ Error is calculated.\n",
        "3. **Backpropagation** â†’ Gradients ($\\frac{\\partial \\mathcal{L}}{\\partial W}$) are computed.\n",
        "4. **Optimizer** â†’ Uses these gradients to update weights:\n",
        "\n",
        "$$\n",
        "W \\leftarrow W - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W}\n",
        "$$\n",
        "\n",
        "where $\\eta$ = learning rate.\n",
        "## **3. Common Optimizers**\n",
        "\n",
        "| Optimizer                             | Key Idea                                                         | Pros                                        | Cons                                   |\n",
        "| ------------------------------------- | ---------------------------------------------------------------- | ------------------------------------------- | -------------------------------------- |\n",
        "| **SGD (Stochastic Gradient Descent)** | Updates weights using gradient of one (or few) samples at a time | Simple, memory-efficient                    | May be slow to converge                |\n",
        "| **Momentum**                          | Adds a fraction of previous updates to current update            | Speeds up convergence, reduces oscillations | Needs tuning of momentum term          |\n",
        "| **Adagrad**                           | Adapts learning rate for each parameter based on past gradients  | Works well for sparse data                  | Learning rate may decay too much       |\n",
        "| **RMSProp**                           | Keeps moving average of squared gradients                        | Works well for RNNs                         | Needs learning rate tuning             |\n",
        "| **Adam (Adaptive Moment Estimation)** | Combines Momentum + RMSProp                                      | Fast, widely used, minimal tuning           | Can overfit if learning rate not tuned |\n",
        "| **AdamW**                             | Adam with weight decay for regularization                        | Better generalization                       | Slightly more complex                  |\n",
        "\n",
        "## **4. Analogy**\n",
        "\n",
        "Training a neural network is like **hiking down a mountain blindfolded**:\n",
        "\n",
        "* **Loss function** = your altitude (you want to minimize it).\n",
        "* **Gradients** = tell you which way is downhill.\n",
        "* **Optimizer** = decides how big a step you should take and adjusts your path for efficiency\n"
      ],
      "metadata": {
        "id": "BbUV-COi5yC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Could you briefly describe some common optimizers!\n",
        "Sure â€” hereâ€™s a **quick overview** of the most common neural network optimizers:\n",
        "### **1. Stochastic Gradient Descent (SGD)**\n",
        "\n",
        "* **How it works**: Updates weights using the gradient from a single (or small batch of) training sample(s).\n",
        "* **Update rule**:\n",
        "\n",
        "  $$\n",
        "  W \\leftarrow W - \\eta \\cdot \\nabla_W \\mathcal{L}\n",
        "  $$\n",
        "* **Pros**: Simple, memory-efficient.\n",
        "* **Cons**: Can be slow, oscillates in narrow valleys.\n",
        "### **2. SGD with Momentum**\n",
        "\n",
        "* **How it works**: Adds a fraction of the previous update to the current update to speed up learning and smooth oscillations.\n",
        "* **Update rule**:\n",
        "\n",
        "  $$\n",
        "  v_t = \\beta v_{t-1} + \\eta \\nabla_W \\mathcal{L}\n",
        "  $$\n",
        "\n",
        "  $$\n",
        "  W \\leftarrow W - v_t\n",
        "  $$\n",
        "* **Pros**: Faster convergence, especially in deep networks.\n",
        "* **Cons**: Needs momentum term $\\beta$ tuning.\n",
        "### **3. Adagrad**\n",
        "\n",
        "* **How it works**: Adapts the learning rate for each parameter based on historical gradient magnitude.\n",
        "* **Update rule**:\n",
        "\n",
        "  $$\n",
        "  W \\leftarrow W - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\cdot \\nabla_W \\mathcal{L}\n",
        "  $$\n",
        "* **Pros**: Good for sparse features (e.g., NLP).\n",
        "* **Cons**: Learning rate keeps shrinking, may stop learning.\n",
        "### **4. RMSProp**\n",
        "\n",
        "* **How it works**: Keeps an exponentially decaying average of squared gradients for normalization.\n",
        "* **Update rule**:\n",
        "\n",
        "  $$\n",
        "  W \\leftarrow W - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\cdot \\nabla_W \\mathcal{L}\n",
        "  $$\n",
        "* **Pros**: Works well for RNNs and non-stationary problems.\n",
        "* **Cons**: Needs tuning of decay rate.\n",
        "### **5. Adam (Adaptive Moment Estimation)**\n",
        "\n",
        "* **How it works**: Combines **Momentum** (moving average of gradients) and **RMSProp** (adaptive learning rates).\n",
        "* **Update rule**: Uses first moment $m_t$ and second moment $v_t$ estimates:\n",
        "\n",
        "  $$\n",
        "  W \\leftarrow W - \\frac{\\eta \\cdot \\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
        "  $$\n",
        "* **Pros**: Fast, widely used, works well out of the box.\n",
        "* **Cons**: Can generalize poorly if not tuned.\n",
        "\n",
        "### **6. AdamW**\n",
        "\n",
        "* **How it works**: Adam + weight decay for better regularization.\n",
        "* **Pros**: Better generalization than Adam.\n",
        "* **Cons**: Slightly more complex.\n"
      ],
      "metadata": {
        "id": "hRTT0ozZ6g9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11 Can you explain forward and backward propagation in a neural network!\n",
        "Sure â€” letâ€™s break it down clearly.\n",
        "## **1. Forward Propagation**\n",
        "\n",
        "**Goal:** Pass input data through the network to get predictions.\n",
        "\n",
        "### **Step-by-step**\n",
        "\n",
        "1. **Input Layer** â€” The data $\\mathbf{x}$ is fed into the network.\n",
        "2. **Weighted Sum** â€” Each neuron calculates:\n",
        "\n",
        "   $$\n",
        "   z^{(l)} = \\mathbf{W}^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)}\n",
        "   $$\n",
        "\n",
        "   where:\n",
        "\n",
        "   * $l$ = layer number\n",
        "   * $\\mathbf{a}^{(0)} = \\mathbf{x}$ (input features)\n",
        "3. **Activation Function** â€” Apply a non-linear function:\n",
        "\n",
        "   $$\n",
        "   a^{(l)} = f^{(l)}(z^{(l)})\n",
        "   $$\n",
        "4. **Output Layer** â€” Produces the prediction $\\hat{\\mathbf{y}}$.\n",
        "\n",
        "ðŸ’¡ **Analogy:** Like water flowing forward through pipes â€” each layer processes and passes data along.\n",
        "\n",
        "## **2. Backward Propagation (Backprop)**\n",
        "\n",
        "**Goal:** Calculate how each weight contributed to the error, so we can update them.\n",
        "\n",
        "### **Step-by-step**\n",
        "\n",
        "1. **Loss Calculation** â€” Compare prediction $\\hat{\\mathbf{y}}$ with actual target $\\mathbf{y}$ using a **loss function**:\n",
        "\n",
        "   $$\n",
        "   \\mathcal{L}(y, \\hat{y})\n",
        "   $$\n",
        "2. **Error at Output** â€” Compute gradient of loss with respect to output:\n",
        "\n",
        "   $$\n",
        "   \\delta^{(L)} = \\frac{\\partial \\mathcal{L}}{\\partial z^{(L)}}\n",
        "   $$\n",
        "3. **Propagate Error Backwards** â€” For each hidden layer:\n",
        "\n",
        "   $$\n",
        "   \\delta^{(l)} = (\\mathbf{W}^{(l+1)})^T \\delta^{(l+1)} \\odot f'^{(l)}(z^{(l)})\n",
        "   $$\n",
        "\n",
        "   where $\\odot$ is element-wise multiplication.\n",
        "4. **Calculate Gradients** â€” For weights and biases:\n",
        "\n",
        "   $$\n",
        "   \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(l)}} = \\delta^{(l)} (\\mathbf{a}^{(l-1)})^T\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(l)}} = \\delta^{(l)}\n",
        "   $$\n",
        "5. **Update Weights** â€” Using an optimizer (like SGD, Adam):\n",
        "\n",
        "   $$\n",
        "   \\mathbf{W}^{(l)} \\leftarrow \\mathbf{W}^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(l)}}\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   \\mathbf{b}^{(l)} \\leftarrow \\mathbf{b}^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(l)}}\n",
        "   $$\n",
        "\n",
        "ðŸ’¡ **Analogy:** Imagine fixing a factory line â€” you start from the last stage (output) and trace back to see which machine (layer) introduced the error.\n",
        "## **3. Process Overview**\n",
        "\n",
        "1. **Forward Pass** â†’ Make predictions.\n",
        "2. **Loss Calculation** â†’ Measure error.\n",
        "3. **Backward Pass** â†’ Find which weights caused the error.\n",
        "4. **Optimization Step** â†’ Adjust weights to reduce future error.\n",
        "\n"
      ],
      "metadata": {
        "id": "T3234hC97IDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12 What is weight initialization, and how does it impact training!\n",
        "**Weight Initialization** is the process of assigning the starting values (usually small random numbers) to the weights of a neural network **before** training begins.\n",
        "\n",
        "Itâ€™s more important than it might sound â€” a bad initialization can make training very slow or even stop the network from learning altogether.\n",
        "\n",
        "## **Why Weight Initialization Matters**\n",
        "\n",
        "When training, the weights are updated step-by-step using gradients.\n",
        "If the starting weights are poorly chosen:\n",
        "\n",
        "1. **Too small (near zero)** â†’ Signals shrink layer by layer â†’ *vanishing gradients* â†’ slow or no learning.\n",
        "2. **Too large** â†’ Signals explode layer by layer â†’ *exploding gradients* â†’ unstable training.\n",
        "3. **All equal** â†’ Every neuron learns the same thing â†’ no diversity in features.\n",
        "\n",
        "A good initialization keeps the scale of activations and gradients **balanced** as they flow forward and backward through the network.\n",
        "\n",
        "## **Common Weight Initialization Methods**\n",
        "\n",
        "| Method                           | Idea                                                                                               | When to Use                                         |\n",
        "| -------------------------------- | -------------------------------------------------------------------------------------------------- | --------------------------------------------------- |\n",
        "| **Zero Initialization**          | All weights = 0                                                                                    | âŒ Never for hidden layers (causes symmetry problem) |\n",
        "| **Random Initialization**        | Small random numbers from uniform or normal distribution                                           | Very basic, often replaced by better methods        |\n",
        "| **Xavier/Glorot Initialization** | Variance depends on number of input & output neurons: $\\text{Var}(W) = \\frac{2}{n_{in} + n_{out}}$ | Works well with sigmoid/tanh                        |\n",
        "| **He Initialization**            | Variance: $\\text{Var}(W) = \\frac{2}{n_{in}}$                                                       | Works well with ReLU/variants                       |\n",
        "| **LeCun Initialization**         | Variance: $\\text{Var}(W) = \\frac{1}{n_{in}}$                                                       | Works well with SELU                                |\n",
        "| **Orthogonal Initialization**    | Weight matrix is orthogonal                                                                        | Used in RNNs for stable long-term dependencies      |\n",
        "## **Impact on Training**\n",
        "\n",
        "* **Faster convergence**: Good initialization can reduce the number of epochs needed.\n",
        "* **Stable gradients**: Prevents vanishing or exploding gradients.\n",
        "* **Better final accuracy**: Allows network to learn richer features early on\n",
        "ðŸ’¡ **Analogy:**\n",
        "Think of weight initialization like choosing a starting point for climbing a hill in fog (gradient descent).\n",
        "\n",
        "* If you start at a terrible spot (bad init), you might get stuck in a ditch (local minimum) or slide down endlessly (exploding gradients).\n",
        "* If you start at a reasonable height (good init), you reach the top faster and more reliably."
      ],
      "metadata": {
        "id": "UJPDRibt7wuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13 What is the vanishing gradient problem in deep learning!\n",
        "The **vanishing gradient problem** is a common issue in training deep neural networks, where the gradients (partial derivatives of the loss with respect to weights) become **very small** as they are backpropagated through many layers.\n",
        "\n",
        "Hereâ€™s what happens step-by-step:\n",
        "\n",
        "1. **Backpropagation uses the chain rule**\n",
        "\n",
        "   * The gradient at each layer is computed by multiplying derivatives from the next layer.\n",
        "   * In deep networks, this means multiplying many small numbers together.\n",
        "\n",
        "2. **Small derivatives shrink exponentially**\n",
        "\n",
        "   * If the activation functionâ€™s derivative is less than 1 (e.g., sigmoid or tanh), repeated multiplication across layers makes the gradient approach **zero** for early layers.\n",
        "\n",
        "3. **Impact**\n",
        "\n",
        "   * **Early layers** (closer to the input) learn **extremely slowly** because their weights receive almost no update.\n",
        "   * The network might fail to capture important low-level features.\n",
        "\n",
        "4. **Example with sigmoid activation**\n",
        "\n",
        "   * The sigmoid derivative is at most 0.25.\n",
        "   * If your network has 10 layers, multiplying numbers â‰¤ 0.25 repeatedly can make gradients vanish to near zero.\n",
        "\n",
        "5. **Why itâ€™s a problem**\n",
        "\n",
        "   * Training becomes **very slow** or **stalls entirely**.\n",
        "   * The network might get stuck with poor performance.\n",
        "**Common solutions**:\n",
        "\n",
        "* Use activation functions with better gradient flow (e.g., **ReLU**, Leaky ReLU).\n",
        "* Use **batch normalization** to stabilize activations.\n",
        "* Use **residual connections** (ResNets) to shorten gradient paths.\n",
        "* Careful **weight initialization** (e.g., Xavier/He initialization)."
      ],
      "metadata": {
        "id": "r7qibt9I8bgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14 What is the exploding gradient problem?\n",
        "The **exploding gradient problem** is the opposite of the vanishing gradient problem â€” instead of gradients becoming tiny during backpropagation, they become **very large** as they pass through many layers.\n",
        "### How it happens\n",
        "\n",
        "1. **Backpropagation multiplies derivatives**\n",
        "\n",
        "   * In deep networks, if the derivatives or weight values are **greater than 1**, multiplying them repeatedly across layers can make gradients grow **exponentially**.\n",
        "\n",
        "2. **Causes**\n",
        "\n",
        "   * Poor **weight initialization** (too large values).\n",
        "   * Activation functions with large derivatives.\n",
        "   * **Recurrent Neural Networks (RNNs)** with long sequences are especially prone because of repeated weight multiplication over time.\n",
        "### Impact\n",
        "\n",
        "* **Unstable training**: The loss oscillates wildly or becomes `NaN` due to numerical overflow.\n",
        "* **Diverging weights**: The model fails to converge.\n",
        "* Large updates cause the network to overshoot the optimal weights.\n",
        "### Example intuition\n",
        "\n",
        "If the derivative in each layer is \\~2, and you have 10 layers:\n",
        "\n",
        "$$\n",
        "2^{10} = 1024\n",
        "$$\n",
        "\n",
        "Thatâ€™s a **thousandfold increase** in the gradient magnitude, causing massive updates.\n",
        "### Common solutions\n",
        "\n",
        "* **Gradient clipping**: Limit the gradientâ€™s maximum norm or value (common in RNNs).\n",
        "* **Careful weight initialization** (e.g., Xavier, He initialization).\n",
        "* **Lower learning rates**.\n",
        "* **Batch normalization** to keep activations stable.\n",
        "* **Residual networks** to shorten gradient paths.\n"
      ],
      "metadata": {
        "id": "cqAgUyp59TiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VLt4Suks9zfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practicle\n"
      ],
      "metadata": {
        "id": "xmKQw-lHUCH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 How do you create a simple perceptron for basic binary classification!\n",
        "Alright, letâ€™s go step-by-step and make a **simple perceptron** for **binary classification** in Python.\n",
        "\n",
        "Weâ€™ll use only **NumPy** to show the core logicâ€”no high-level ML librariesâ€”so you can see exactly how it works--\n",
        "\n",
        "## **1. What is a Perceptron?**\n",
        "\n",
        "A perceptron is a single-layer neural network unit that:\n",
        "\n",
        "1. Takes inputs\n",
        "2. Multiplies them by weights\n",
        "3. Sums them up (plus a bias)\n",
        "4. Passes the result through a step function to output either 0 or 1.\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "$$\n",
        "y = f(w \\cdot x + b)\n",
        "$$\n",
        "\n",
        "where $f$ is usually the step function:\n",
        "\n",
        "$$\n",
        "f(z) = \\begin{cases}\n",
        "1 & \\text{if } z > 0 \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "## **2. Code for a Simple Perceptron**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Step activation function\n",
        "def step_function(x):\n",
        "    return np.where(x >= 0, 1, 0)\n",
        "\n",
        "# Perceptron training\n",
        "def perceptron_train(X, y, learning_rate=0.1, epochs=10):\n",
        "    weights = np.zeros(X.shape[1])\n",
        "    bias = 0\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for xi, target in zip(X, y):\n",
        "            linear_output = np.dot(xi, weights) + bias\n",
        "            prediction = step_function(linear_output)\n",
        "            update = learning_rate * (target - prediction)\n",
        "            weights += update * xi\n",
        "            bias += update\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "# Perceptron prediction\n",
        "def perceptron_predict(X, weights, bias):\n",
        "    return step_function(np.dot(X, weights) + bias)\n",
        "\n",
        "# Example dataset: AND logic gate\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Train perceptron\n",
        "weights, bias = perceptron_train(X, y, learning_rate=0.1, epochs=10)\n",
        "\n",
        "# Test perceptron\n",
        "predictions = perceptron_predict(X, weights, bias)\n",
        "\n",
        "print(\"Weights:\", weights)\n",
        "print(\"Bias:\", bias)\n",
        "print(\"Predictions:\", predictions)\n",
        "```\n",
        "## **3. Output Example (for AND gate)**\n",
        "\n",
        "```\n",
        "Weights: [0.1 0.1]\n",
        "Bias: -0.1\n",
        "Predictions: [0 0 0 1]\n",
        "``\n",
        "âœ… **Key Notes**:\n",
        "\n",
        "* This example uses a **step function** so it can only do **linear binary classification** (e.g., AND, OR).\n",
        "* It wonâ€™t work for problems that arenâ€™t linearly separable (e.g., XOR).\n",
        "* For more complex tasks, youâ€™d use multiple perceptrons (multi-layer) and activation functions like **sigmoid**, **ReLU**, etc."
      ],
      "metadata": {
        "id": "-sLV1emoUH5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2 How can you build a neural network with one hidden layer using Keras!\n",
        "Alright â€” letâ€™s go step-by-step and build a **neural network with one hidden layer** using **Keras** (part of TensorFlow).\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Basic Idea**\n",
        "\n",
        "Weâ€™ll make a neural network:\n",
        "\n",
        "* **Input layer** â†’ takes features (X)\n",
        "* **Hidden layer** â†’ applies weights, bias, and activation function\n",
        "* **Output layer** â†’ gives final prediction\n",
        "\n",
        "For **binary classification**, the output layer will have **1 neuron** with a **sigmoid** activation.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Code Example**\n",
        "\n",
        "```python\n",
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Example dataset (XOR problem just for demo)\n",
        "import numpy as np\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([0,1,1,0])\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "\n",
        "# Hidden layer: 4 neurons, relu activation\n",
        "model.add(Dense(4, input_dim=2, activation='relu'))\n",
        "\n",
        "# Output layer: 1 neuron, sigmoid activation (binary classification)\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model.fit(X, y, epochs=200, verbose=0)\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(X, y)\n",
        "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(X)\n",
        "print(\"Predictions:\", predictions)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Explanation of Parameters**\n",
        "\n",
        "* `Dense(4, activation='relu')` â†’ hidden layer with 4 neurons\n",
        "* `input_dim=2` â†’ number of input features (in our example: two bits)\n",
        "* `Dense(1, activation='sigmoid')` â†’ output layer for binary classification\n",
        "* `loss='binary_crossentropy'` â†’ suitable loss for binary classification\n",
        "* `optimizer='adam'` â†’ adaptive learning rate optimizer\n",
        "* `epochs=200` â†’ number of passes through the training data\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Output Example**\n",
        "\n",
        "```\n",
        "Accuracy: 100.00%\n",
        "Predictions:\n",
        "[[0.01]\n",
        " [0.98]\n",
        " [0.98]\n",
        " [0.02]]\n",
        "```\n",
        "\n",
        "Here, outputs close to `0` mean class **0**, and close to `1` mean class **1**."
      ],
      "metadata": {
        "id": "0E_2HZeTVNun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 How do you initialize weights using the Xavier (Glorot) initialization method in Keras!\n",
        "In Keras, **Xavier initialization** is already available as **Glorot initialization** (named after Xavier Glorot, who proposed it).\n",
        "\n",
        "Hereâ€™s how you can use it:\n",
        "\n",
        "## **1. Using Glorot Initialization in Keras**\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.initializers import GlorotUniform, GlorotNormal\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Hidden layer with Xavier (Glorot) uniform initialization\n",
        "model.add(Dense(\n",
        "    units=4,\n",
        "    activation='relu',\n",
        "    kernel_initializer=GlorotUniform(),  # Xavier uniform\n",
        "    input_dim=2\n",
        "))\n",
        "\n",
        "# Output layer with Xavier (Glorot) normal initialization\n",
        "model.add(Dense(\n",
        "    units=1,\n",
        "    activation='sigmoid',\n",
        "    kernel_initializer=GlorotNormal()  # Xavier normal\n",
        "))\n",
        "```\n",
        "## **2. Explanation**\n",
        "\n",
        "* **`GlorotUniform()`** â†’ draws weights from a uniform distribution\n",
        "\n",
        "  $$\n",
        "  \\text{U}\\left(-\\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}, \\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}\\right)\n",
        "  $$\n",
        "* **`GlorotNormal()`** â†’ draws weights from a normal distribution\n",
        "\n",
        "  $$\n",
        "  \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{\\text{in}} + n_{\\text{out}}}}\\right)\n",
        "  $$\n",
        "* $n_{\\text{in}}$ = number of input neurons\n",
        "* $n_{\\text{out}}$ = number of output neurons in the layer\n",
        "## **3. Why Use Xavier Initialization?**\n",
        "\n",
        "It keeps the variance of the activations **consistent** across layers, preventing exploding or vanishing gradients during training, especially for activations like **sigmoid** and **tanh**."
      ],
      "metadata": {
        "id": "Wd4yBP_BVjsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4  How can you apply different activation functions in a neural network in Keras!\n",
        "In **Keras**, you can apply **different activation functions** simply by specifying the `activation` parameter in each `Dense` (or other) layer.\n",
        "You can mix and match per layer depending on your networkâ€™s needs.\n",
        "## **1. Example: Different Activations in Different Layers**\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "\n",
        "# Input â†’ Hidden layer 1 with ReLU\n",
        "model.add(Dense(8, input_dim=4, activation='relu'))\n",
        "\n",
        "# Hidden layer 2 with Tanh\n",
        "model.add(Dense(6, activation='tanh'))\n",
        "\n",
        "# Hidden layer 3 with LeakyReLU (as a separate layer)\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "model.add(Dense(4))  # No activation here\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "# Output layer with Sigmoid (for binary classification)\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "```\n",
        "## **2. Notes on Activation Choices**\n",
        "\n",
        "* **ReLU** (`relu`) â†’ default for hidden layers, fast & avoids vanishing gradients.\n",
        "* **Tanh** (`tanh`) â†’ outputs in range \\[-1, 1], good for normalized data.\n",
        "* **Sigmoid** (`sigmoid`) â†’ outputs in \\[0, 1], often used for binary classification output layers.\n",
        "* **Softmax** (`softmax`) â†’ for multi-class classification output layers.\n",
        "* **LeakyReLU**, **ELU**, etc. â†’ can be added as separate layers for more control.\n",
        "## **3. Using Activation Layers Explicitly**\n",
        "\n",
        "Instead of passing `activation='relu'` inside `Dense`, you can also use:\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.layers import Activation\n",
        "\n",
        "model.add(Dense(8, input_dim=4))\n",
        "model.add(Activation('relu'))\n",
        "```\n",
        "\n",
        "This gives more flexibility if you want to apply custom logic before/after activation."
      ],
      "metadata": {
        "id": "woTZGDj2V7ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 How do you add dropout to a neural network model to prevent overfitting!\n",
        "In a neural network, **dropout** is a regularization technique that helps prevent overfitting by randomly â€œdroppingâ€ (ignoring) a fraction of neurons during training. This forces the model to not rely too heavily on specific neurons and improves generalization.\n",
        "\n",
        "Hereâ€™s how to add dropout in **Keras** step-by-step:\n",
        "\n",
        "### **1. Import the required library**\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "```\n",
        "### **2. Define the model with dropout layers**\n",
        "\n",
        "```python\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer\n",
        "model.add(Dense(64, activation='relu', input_shape=(100,)))\n",
        "\n",
        "# Dropout layer (drop 30% of neurons)\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Hidden layer\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "# Dropout again\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Output layer (binary classification example)\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "```\n",
        "### **3. Compile and train**\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "```\n",
        "### **Key Points**\n",
        "\n",
        "* `Dropout(rate)` â†’ **rate** is the fraction of neurons to drop, e.g., `0.3` means drop **30%** of neurons during training.\n",
        "* Dropout is **only applied during training**. During evaluation or prediction, all neurons are active.\n",
        "* Itâ€™s usually added **after dense or convolutional layers**, not before the input layer.\n"
      ],
      "metadata": {
        "id": "SLeCOBChg-zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 How do you manually implement forward propagation in a simple neural network!\n",
        "Alright â€” letâ€™s go through **manual forward propagation** in a simple neural network step-by-step, without using Keras, TensorFlow, or PyTorch.\n",
        "\n",
        "Weâ€™ll make a **small example**:\n",
        "\n",
        "* Input layer: 2 neurons\n",
        "* One hidden layer: 2 neurons (ReLU activation)\n",
        "* Output layer: 1 neuron (Sigmoid activation)\n",
        "\n",
        "## **1. Import dependencies**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "```\n",
        "## **2. Define input, weights, and biases**\n",
        "\n",
        "```python\n",
        "# Example input (1 sample, 2 features)\n",
        "X = np.array([[0.5, 0.2]])\n",
        "\n",
        "# Weights and biases (initialized manually)\n",
        "W1 = np.array([[0.1, 0.4],    # weights for input â†’ hidden\n",
        "               [0.8, 0.5]])   # shape: (2, 2)\n",
        "b1 = np.array([[0.1, 0.2]])   # bias for hidden layer\n",
        "\n",
        "W2 = np.array([[0.3],         # weights for hidden â†’ output\n",
        "               [0.9]])        # shape: (2, 1)\n",
        "b2 = np.array([[0.05]])       # bias for output layer\n",
        "```\n",
        "## **3. Define activation functions**\n",
        "\n",
        "```python\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "```\n",
        "## **4. Forward propagation steps**\n",
        "\n",
        "```python\n",
        "# Step 1: Input â†’ Hidden layer\n",
        "z1 = np.dot(X, W1) + b1\n",
        "a1 = relu(z1)\n",
        "\n",
        "# Step 2: Hidden â†’ Output layer\n",
        "z2 = np.dot(a1, W2) + b2\n",
        "a2 = sigmoid(z2)  # final output\n",
        "```\n",
        "## **5. Print results**\n",
        "\n",
        "```python\n",
        "print(\"Hidden layer linear output (z1):\", z1)\n",
        "print(\"Hidden layer activation (a1):\", a1)\n",
        "print(\"Output layer linear output (z2):\", z2)\n",
        "print(\"Final prediction (a2):\", a2)\n",
        "```\n",
        "### **Flow Recap**\n",
        "\n",
        "1. **Linear transformation**: $z = XW + b$\n",
        "2. **Apply activation**: $a = f(z)$\n",
        "3. Repeat for each layer until the output layer is reached."
      ],
      "metadata": {
        "id": "RQ2DA2RYiXT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7  How do you add batch normalization to a neural network model in Keras!\n",
        "Batch Normalization (**BN**) is a technique that normalizes the output of a layer to have a mean of 0 and a variance of 1 during training, which helps:\n",
        "\n",
        "* **Speed up convergence**\n",
        "* **Reduce internal covariate shift**\n",
        "* **Act as a form of regularization** (sometimes reducing the need for dropout)\n",
        "## **1. Import the necessary classes**\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
        "```\n",
        "## **2. Add Batch Normalization to the model**\n",
        "\n",
        "Hereâ€™s a simple example:\n",
        "\n",
        "```python\n",
        "model = Sequential()\n",
        "\n",
        "# Input + Dense\n",
        "model.add(Dense(64, activation='relu', input_shape=(100,)))\n",
        "\n",
        "# Batch Normalization\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Optional Dropout (for extra regularization)\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Hidden layer\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "```\n",
        "## **3. Compile and train**\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "```\n",
        "### **Best Practices**\n",
        "\n",
        "* Place **`BatchNormalization()`** *after* the Dense/Conv layer, but **before** the activation in some designs:\n",
        "\n",
        "  ```python\n",
        "  model.add(Dense(64, use_bias=False))  # Bias not needed, BN handles it\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  ```\n",
        "* BN can sometimes reduce the need for dropout â€” but using both is possible.\n",
        "* It works well for deep networks by stabilizing training."
      ],
      "metadata": {
        "id": "U0qaMgT5jE93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8 How can you visualize the training process with accuracy and loss curves!\n",
        "To visualize the **training process** of a neural network, you can plot **accuracy** and **loss curves** from the `History` object returned by `model.fit()` in Keras.\n",
        "\n",
        "## **1. Train the model and store history**\n",
        "\n",
        "```python\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)\n",
        "```\n",
        "\n",
        "The `history` variable contains `history.history`, a dictionary with keys like:\n",
        "\n",
        "* `\"accuracy\"` â†’ training accuracy\n",
        "* `\"val_accuracy\"` â†’ validation accuracy\n",
        "* `\"loss\"` â†’ training loss\n",
        "* `\"val_loss\"` â†’ validation loss\n",
        "## **2. Plot accuracy and loss curves**\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot Loss\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "## **3. What to look for**\n",
        "\n",
        "* **If training loss keeps going down but validation loss goes up** â†’ overfitting.\n",
        "* **If both losses stagnate at high values** â†’ underfitting.\n",
        "* **Smooth curves** are good; highly noisy curves might mean a learning rate thatâ€™s too high.\n"
      ],
      "metadata": {
        "id": "B342iUISj4LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9 How can you use gradient clipping in Keras to control the gradient size and prevent exploding gradients!\n",
        "**Gradient clipping** is a technique that limits (clips) the size of gradients during backpropagation to prevent the **exploding gradient problem** â€” where gradients become extremely large, causing unstable training and NaN losses.\n",
        "\n",
        "In **Keras**, you set gradient clipping when you create the optimizer.\n",
        "There are **two common methods**:\n",
        "## **1. Clip by Value**\n",
        "\n",
        "Limits each gradient component to a fixed range $[-clipvalue, clipvalue]$.\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001, clipvalue=1.0)  # Clip each gradient value\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "This is like saying: *â€œNo single gradient component should be bigger than 1 or smaller than -1.â€*\n",
        "\n",
        "## **2. Clip by Norm**\n",
        "\n",
        "Scales gradients so that the overall **L2 norm** does not exceed a set value.\n",
        "\n",
        "```python\n",
        "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)  # Clip gradient vector norm\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "This is like saying: *â€œThe whole gradient vectorâ€™s size canâ€™t be bigger than 1.â€*\n",
        "### **When to Use Which**\n",
        "\n",
        "* **clipvalue** â†’ Good for very aggressive gradient spikes in individual weights.\n",
        "* **clipnorm** â†’ More common in deep learning; keeps overall gradient scale controlled.\n",
        "\n"
      ],
      "metadata": {
        "id": "-r9wOYW2k2T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10 How can you create a custom loss function in Keras!\n",
        "In **Keras**, you can create a **custom loss function** either as:\n",
        "\n",
        "1. **A Python function**\n",
        "2. **A subclass of `tf.keras.losses.Loss`** (for more control)\n",
        "\n",
        "Letâ€™s go through both.\n",
        "## **1. Custom Loss as a Function**\n",
        "\n",
        "The function must take **`y_true`** and **`y_pred`** as arguments and return a scalar loss value.\n",
        "Example: Mean Squared Error (custom version)\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_mse(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "```\n",
        "\n",
        "**Using it:**\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='adam',\n",
        "              loss=custom_mse,\n",
        "              metrics=['mae'])\n",
        "```\n",
        "## **2. Custom Loss with Extra Parameters**\n",
        "\n",
        "If you want a parameterized loss (e.g., weighted loss), you can use a closure:\n",
        "\n",
        "```python\n",
        "def weighted_mse(weight):\n",
        "    def loss(y_true, y_pred):\n",
        "        return tf.reduce_mean(weight * tf.square(y_true - y_pred))\n",
        "    return loss\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=weighted_mse(0.5),\n",
        "              metrics=['mae'])\n",
        "```\n",
        "## **3. Custom Loss as a Class**\n",
        "\n",
        "Subclass `tf.keras.losses.Loss` for advanced cases:\n",
        "\n",
        "```python\n",
        "class CustomHuberLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, delta=1.0, name=\"custom_huber_loss\"):\n",
        "        super().__init__(name=name)\n",
        "        self.delta = delta\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) <= self.delta\n",
        "        small_error_loss = 0.5 * tf.square(error)\n",
        "        big_error_loss = self.delta * (tf.abs(error) - 0.5 * self.delta)\n",
        "        return tf.reduce_mean(tf.where(is_small_error, small_error_loss, big_error_loss))\n",
        "```\n",
        "\n",
        "**Using it:**\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='adam',\n",
        "              loss=CustomHuberLoss(delta=1.5),\n",
        "              metrics=['mae'])\n",
        "```\n",
        "âœ… **Key Points:**\n",
        "\n",
        "* Always return a **scalar tensor** from your loss.\n",
        "* Loss should be **differentiable** so TensorFlow can compute gradients.\n",
        "* You can mix TensorFlow ops and NumPy, but NumPy wonâ€™t be differentiable\n"
      ],
      "metadata": {
        "id": "rnN8p3C1lYRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11 How can you visualize the structure of a neural network model in Keras?\n",
        "In **Keras**, you can visualize a neural networkâ€™s structure in a couple of ways â€” either as a **summary in text form** or as a **graph diagram**.\n",
        "## **1. View Model Summary (Text)**\n",
        "\n",
        "```python\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "This shows:\n",
        "\n",
        "* Layer names\n",
        "* Output shapes\n",
        "* Number of parameters\n",
        "\n",
        "Example output:\n",
        "\n",
        "```\n",
        "Layer (type)          Output Shape       Param #\n",
        "================================================\n",
        "dense (Dense)         (None, 64)         6464\n",
        "dropout (Dropout)     (None, 64)         0\n",
        "dense_1 (Dense)       (None, 32)         2080\n",
        "dense_2 (Dense)       (None, 1)          33\n",
        "================================================\n",
        "Total params: 8,577\n",
        "Trainable params: 8,577\n",
        "Non-trainable params: 0\n",
        "```\n",
        "## **2. Visualize as a Diagram**\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
        "```\n",
        "\n",
        "* `show_shapes=True` â†’ displays output shapes per layer\n",
        "* `show_layer_names=True` â†’ shows the names you gave layers\n",
        "\n",
        "**Note:** You need `pydot` and `graphviz` installed:\n",
        "\n",
        "```bash\n",
        "pip install pydot graphviz\n",
        "```\n",
        "## **3. In Jupyter Notebook (Inline Display)**\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import Image\n",
        "\n",
        "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
        "Image(filename='model.png')\n",
        "```"
      ],
      "metadata": {
        "id": "V-Ee1XHJlu1k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}