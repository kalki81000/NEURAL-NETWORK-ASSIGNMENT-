{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN48D1Hz+nzSiTlS/7ZpjxZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalki81000/NEURAL-NETWORK-ASSIGNMENT-/blob/main/Untitled115.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ##Neural Network A Simple Perception\n",
        ""
      ],
      "metadata": {
        "id": "jyh9s-cATuPb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0Y2Zl-PykiE"
      },
      "outputs": [],
      "source": [
        "#)1 What is deep learning, and how is it connected to artificial intelligence\"\n",
        "Deep learning is a **subset of machine learning**, which itself is a branch of **artificial intelligence (AI)**.\n",
        "\n",
        "Here’s how they connect:\n",
        "\n",
        "1. **Artificial Intelligence (AI)** – This is the broad field concerned with creating systems that can perform tasks normally requiring human intelligence, such as understanding language, recognizing images, or making decisions.\n",
        "\n",
        "2. **Machine Learning (ML)** – A subset of AI that focuses on algorithms that allow machines to learn from data rather than being explicitly programmed for every rule.\n",
        "\n",
        "3. **Deep Learning (DL)** – A further subset of ML that uses **artificial neural networks** with many layers (“deep” networks) to automatically learn complex patterns from large amounts of data.\n",
        "\n",
        "**Key points about deep learning:**\n",
        "\n",
        "* **Neural networks** are inspired by the human brain’s structure, with interconnected “neurons” that process information in layers.\n",
        "* It’s particularly powerful for **image recognition**, **speech recognition**, **natural language processing**, and **autonomous systems**.\n",
        "* Unlike traditional ML, deep learning can **automatically extract features** from raw data, reducing the need for manual feature engineering.\n",
        "* It thrives on **large datasets** and **high computing power** (e.g., GPUs).\n",
        "\n",
        "**Hierarchy analogy:**\n",
        "\n",
        "> AI → Machine Learning → Deep Learning\n",
        "> (like “Mathematics → Algebra → Linear Algebra” — deep learning is a specialized area inside a specialized area)\n",
        "\n",
        "If you want, I can make you a **simple diagram showing the relationship between AI, ML, and DL** so it’s instantly clear. Would you like me to do that?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 What is the mathematical structure of a neural network!\n",
        "The **mathematical structure** of a neural network can be described using **linear algebra and functions**.\n",
        "At its core, a neural network is a **composition of functions** where each layer applies a **linear transformation** followed by a **non-linear activation**.\n",
        "\n",
        "## **1. Neuron (Single Unit)**\n",
        "\n",
        "A single neuron takes an input vector **x** and produces an output **y**:\n",
        "\n",
        "$$\n",
        "z = \\sum_{i=1}^{n} w_i x_i + b\n",
        "$$\n",
        "\n",
        "* $x_i$ → input feature\n",
        "* $w_i$ → weight for each input\n",
        "* $b$ → bias term\n",
        "* $z$ → weighted sum (pre-activation)\n",
        "\n",
        "Then apply an **activation function** $f(\\cdot)$:\n",
        "\n",
        "$$\n",
        "y = f(z) = f\\left( \\sum_{i=1}^{n} w_i x_i + b \\right)\n",
        "$$\n",
        "\n",
        "\n",
        "## **2. Layer**\n",
        "\n",
        "For a layer with **m** neurons receiving **n** inputs:\n",
        "\n",
        "1. Inputs as a column vector:\n",
        "\n",
        "$$\n",
        "\\mathbf{x} =\n",
        "\\begin{bmatrix}\n",
        "x_1 \\\\\n",
        "x_2 \\\\\n",
        "\\vdots \\\\\n",
        "x_n\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. Weight matrix:\n",
        "\n",
        "$$\n",
        "\\mathbf{W} =\n",
        "\\begin{bmatrix}\n",
        "w_{11} & w_{12} & \\dots & w_{1n} \\\\\n",
        "w_{21} & w_{22} & \\dots & w_{2n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "w_{m1} & w_{m2} & \\dots & w_{mn}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "(size: $m \\times n$)\n",
        "\n",
        "3. Bias vector:\n",
        "\n",
        "$$\n",
        "\\mathbf{b} =\n",
        "\\begin{bmatrix}\n",
        "b_1 \\\\\n",
        "b_2 \\\\\n",
        "\\vdots \\\\\n",
        "b_m\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "4. Layer operation:\n",
        "\n",
        "$$\n",
        "\\mathbf{z} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = f(\\mathbf{z})\n",
        "$$\n",
        "\n",
        "Where $f$ is applied element-wise.\n",
        "\n",
        "## **3. Multi-Layer Neural Network**\n",
        "\n",
        "If we have $L$ layers:\n",
        "\n",
        "* **Layer 1**:\n",
        "\n",
        "$$\n",
        "\\mathbf{a}^{(1)} = f^{(1)}(\\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)})\n",
        "$$\n",
        "\n",
        "* **Layer 2**:\n",
        "\n",
        "$$\n",
        "\\mathbf{a}^{(2)} = f^{(2)}(\\mathbf{W}^{(2)} \\mathbf{a}^{(1)} + \\mathbf{b}^{(2)})\n",
        "$$\n",
        "\n",
        "* And so on, until the output layer $\\mathbf{a}^{(L)}$.\n",
        "\n",
        "**General formula** for layer $l$:\n",
        "\n",
        "$$\n",
        "\\mathbf{a}^{(l)} = f^{(l)} \\left( \\mathbf{W}^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)} \\right)\n",
        "$$\n",
        "\n",
        "with $\\mathbf{a}^{(0)} = \\mathbf{x}$ (the input vector).\n",
        "\n",
        "\n",
        "## **4. Overall Function**\n",
        "\n",
        "A neural network essentially computes:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{y}} = F(\\mathbf{x}; \\Theta)\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $\\mathbf{x}$ = input vector\n",
        "* $\\Theta = \\{\\mathbf{W}^{(l)}, \\mathbf{b}^{(l)}\\}_{l=1}^{L}$ = all learnable parameters\n",
        "* $F$ = composition of linear transformations + nonlinear activations\n",
        "\n",
        "## **5. Training (Mathematical Side)**\n",
        "\n",
        "* **Loss Function** $\\mathcal{L}(\\hat{\\mathbf{y}}, \\mathbf{y})$ measures error.\n",
        "* Parameters $\\Theta$ are updated using **gradient descent**:\n",
        "\n",
        "$$\n",
        "\\Theta \\leftarrow \\Theta - \\eta \\, \\nabla_{\\Theta} \\mathcal{L}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "3DkOcDJzz_mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 What is an activation function, and why is it essential in neural\"\n",
        "An **activation function** is a **mathematical function** applied to the output of a neuron in a neural network to decide **whether the neuron should be “activated”** and how it should transform its input.\n",
        "\n",
        "In simple terms, it introduces **non-linearity** into the network so it can learn **complex patterns** instead of just linear relationships.\n",
        "\n",
        "\n",
        "## **1. Mathematical Definition**\n",
        "\n",
        "If a neuron computes:\n",
        "\n",
        "$$\n",
        "z = \\sum_{i=1}^n w_i x_i + b\n",
        "$$\n",
        "\n",
        "then the activation function $f(\\cdot)$ produces:\n",
        "\n",
        "$$\n",
        "a = f(z)\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $z$ = weighted sum (linear)\n",
        "* $a$ = neuron’s output after activation (possibly non-linear)\n",
        "\n",
        "## **2. Why It’s Essential**\n",
        "\n",
        "1. **Introduces Non-Linearity**\n",
        "\n",
        "   * Without activation functions, a neural network is just a **stack of linear equations**, which collapses into a single linear transformation—limiting its ability to model real-world problems.\n",
        "\n",
        "2. **Allows Complex Decision Boundaries**\n",
        "\n",
        "   * Non-linear activations enable networks to classify data that’s not linearly separable.\n",
        "\n",
        "3. **Enables Deep Learning**\n",
        "\n",
        "   * Multi-layer networks with non-linear activations can approximate **any continuous function** (Universal Approximation Theorem).\n",
        "\n",
        "4. **Controls Signal Flow**\n",
        "\n",
        "   * Some activations help avoid problems like exploding or vanishing gradients.\n",
        "\n",
        "## **3. Common Activation Functions**\n",
        "\n",
        "| Function       | Formula                                    | Range        | Key Features                                           | Use Case                        |\n",
        "| -------------- | ------------------------------------------ | ------------ | ------------------------------------------------------ | ------------------------------- |\n",
        "| **Sigmoid**    | $f(z) = \\frac{1}{1+e^{-z}}$                | (0,1)        | Smooth, squashes values; can cause vanishing gradients | Binary classification           |\n",
        "| **Tanh**       | $f(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$ | (-1,1)       | Centered at 0; still suffers vanishing gradient        | Hidden layers                   |\n",
        "| **ReLU**       | $f(z) = \\max(0, z)$                        | \\[0,∞)       | Fast, prevents vanishing gradient for positive values  | Most hidden layers              |\n",
        "| **Leaky ReLU** | $f(z) = \\max(\\alpha z, z)$                 | (-∞,∞)       | Allows small negative slope                            | Solves ReLU “dead neuron” issue |\n",
        "| **Softmax**    | $f(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$  | (0,1), sum=1 | Converts vector to probability distribution            | Output layer for multi-class    |\n",
        "\n",
        "## **4. Without Activation Functions**\n",
        "\n",
        "If you remove activation functions, the network becomes:\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = \\mathbf{W}^{(n)} \\dots \\mathbf{W}^{(2)} \\mathbf{W}^{(1)} \\mathbf{x} + \\text{bias terms}\n",
        "$$\n",
        "\n",
        "This is still **just one linear transformation**, no matter how many layers—so it can only model straight-line relationships.\n"
      ],
      "metadata": {
        "id": "pJa1AA-r0yDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Could you list some common activation functions used in neural networks!\n",
        ". Sigmoid Family\n",
        "Sigmoid / Logistic Function\n",
        "\n",
        "Range: (0, 1)\n",
        "Use: Binary classification outputs, probability mapping.\n",
        "\n",
        "Tanh (Hyperbolic Tangent)\n",
        "Range: (-1, 1)\n",
        "Use: Hidden layers where centered output is beneficial.\n",
        "\n",
        "2. ReLU Variants\n",
        "ReLU (Rectified Linear Unit)\n",
        "\n",
        "f(z)=max(0,z)\n",
        "Range: [0, ∞)\n",
        "Use: Most hidden layers in deep networks.\n",
        "\n",
        "Leaky ReLU\n",
        "≈\n",
        "0.01\n",
        "f(z)=max(αz,z),α≈0.01\n",
        "Range: (-∞, ∞)\n",
        "Use: Prevents “dead neurons” in ReLU.\n",
        "\n",
        "Parametric ReLU (PReLU)\n",
        "Like Leaky ReLU, but\n",
        "𝛼\n",
        "α is learned during training.\n",
        "\n",
        "ELU (Exponential Linear Unit)\n",
        "\n",
        "Smooths negative side for better gradient flow.\n",
        "\n",
        "3. Softmax and Probability Functions\n",
        "Softmax\n",
        "Use: Multi-class classification output layer.\n",
        "\n",
        "LogSoftmax\n",
        "Applies log to softmax output — better numerical stability.\n",
        "\n",
        "4. Advanced / Modern\n",
        "Swish\n",
        "f(z)=z⋅σ(z)\n",
        "Smooth, self-gated — used in Google’s EfficientNet.\n",
        "\n",
        "GELU (Gaussian Error Linear Unit)\n",
        "Combines ReLU + sigmoid-like smoothness — used in Transformer models like BERT.\n",
        "\n",
        "Maxout\n",
        "Outputs the maximum of several linear functions — adapts to different activation shapes."
      ],
      "metadata": {
        "id": "h6idlhV81x4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5 What is a multilayer neural network!\n",
        "A **Multilayer Neural Network** (also called a **Multilayer Perceptron – MLP**) is a type of neural network that has **two or more layers of neurons** between the input and output.\n",
        "\n",
        "It’s the simplest form of a **deep neural network**, and it learns by combining **linear transformations** with **non-linear activation functions** in multiple stages.\n",
        "\n",
        "## **Structure**\n",
        "\n",
        "1. **Input Layer** – Receives raw data (features).\n",
        "2. **Hidden Layers** – One or more layers that process data through weighted connections and activation functions.\n",
        "3. **Output Layer** – Produces the final prediction or classification.\n",
        "\n",
        "### **Mathematical Flow**\n",
        "\n",
        "If we have:\n",
        "\n",
        "* $\\mathbf{x}$ = input vector\n",
        "* $\\mathbf{W}^{(l)}$, $\\mathbf{b}^{(l)}$ = weights and biases for layer $l$\n",
        "* $f^{(l)}$ = activation function for layer $l$\n",
        "\n",
        "Then for each layer:\n",
        "\n",
        "$$\n",
        "\\mathbf{a}^{(l)} = f^{(l)}\\left(\\mathbf{W}^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)}\\right)\n",
        "$$\n",
        "\n",
        "with $\\mathbf{a}^{(0)} = \\mathbf{x}$.\n",
        "\n",
        "## **Key Features**\n",
        "\n",
        "* **Multiple hidden layers** → More representation power.\n",
        "* **Non-linear activations** → Can model complex patterns.\n",
        "* **Fully connected** → Every neuron in one layer connects to every neuron in the next (in standard MLPs).\n",
        "\n",
        "## **Advantages**\n",
        "\n",
        "✅ Can approximate any continuous function (**Universal Approximation Theorem**).\n",
        "✅ Handles non-linear and complex relationships.\n",
        "✅ Versatile — works for regression, classification, and more.\n",
        "\n",
        "## **Limitations**\n",
        "\n",
        "❌ Prone to **overfitting** if too large and not regularized.\n",
        "❌ Can be computationally expensive.\n",
        "❌ Requires careful tuning of learning rate, activation functions, and number of layers.\n",
        "\n",
        "## **Example**\n",
        "\n",
        "A 3-layer neural network (1 input layer, 1 hidden layer, 1 output layer) for predicting whether an email is spam:\n",
        "\n",
        "* **Input Layer:** Features like word frequency, sender domain, presence of links.\n",
        "* **Hidden Layer:** Processes feature interactions.\n",
        "* **Output Layer:** Probability of “spam” vs “not spam\n"
      ],
      "metadata": {
        "id": "Ytv28F0X2pwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6What is a loss function, and why is it crucial for neural network training!\n",
        "A **loss function** (also called a **cost function** or **objective function**) is a mathematical formula that measures **how far a neural network’s predictions are from the actual target values**.\n",
        "\n",
        "It’s the **guide** that tells the network how wrong it is, so it knows how to adjust its weights during training.\n",
        "\n",
        "## **1. Mathematical Definition**\n",
        "\n",
        "If:\n",
        "\n",
        "* $\\hat{y}$ = predicted output of the network\n",
        "* $y$ = true (target) value\n",
        "* $\\mathcal{L}$ = loss function\n",
        "\n",
        "Then the loss is:\n",
        "\n",
        "$$\n",
        "\\text{Loss} = \\mathcal{L}(y, \\hat{y})\n",
        "$$\n",
        "\n",
        "## **2. Why It’s Crucial**\n",
        "\n",
        "1. **Training Signal** – The loss function is the **feedback** that drives learning.\n",
        "\n",
        "   * High loss → large errors → big weight updates.\n",
        "   * Low loss → smaller errors → small weight updates.\n",
        "2. **Optimization Goal** – Training a neural network is about **minimizing** this loss over the training data:\n",
        "\n",
        "$$\n",
        "\\min_{\\Theta} \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}(y_i, \\hat{y}_i)\n",
        "$$\n",
        "\n",
        "where $\\Theta$ = all weights & biases.\n",
        "\n",
        "3. **Direction for Gradient Descent** – Backpropagation computes gradients of the loss with respect to parameters, so without a loss function, the network wouldn’t know **how to improve**.\n",
        "## **3. Common Loss Functions**\n",
        "\n",
        "### **For Regression**\n",
        "\n",
        "* **Mean Squared Error (MSE)**:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "* **Mean Absolute Error (MAE)**:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|\n",
        "$$\n",
        "\n",
        "### **For Classification**\n",
        "\n",
        "* **Binary Cross-Entropy**:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i) \\right]\n",
        "$$\n",
        "\n",
        "* **Categorical Cross-Entropy** (for multi-class):\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = - \\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\n",
        "$$\n",
        "\n",
        "### **For Special Tasks**\n",
        "\n",
        "* **Hinge Loss** → Support vector–style classification.\n",
        "* **Huber Loss** → Robust regression with fewer outlier effects.\n",
        "## **4. Without a Loss Function**\n",
        "\n",
        "If a network didn’t have a loss function, it would have **no measurable target** to improve toward.\n",
        "It would be like a student taking an exam and never getting the results — they wouldn’t know what to study or how to get better.\n"
      ],
      "metadata": {
        "id": "LWQ_V89Z3M2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7 What are some common types of loss functions!\n",
        "Here’s a **quick categorized list** of common loss functions used in neural networks, with their main purposes:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Regression Loss Functions**\n",
        "\n",
        "Used when predicting **continuous values**.\n",
        "\n",
        "| Loss Function                 | Formula                                        | Key Feature                            | Use Case           |                    |                   |\n",
        "| ----------------------------- | ---------------------------------------------- | -------------------------------------- | ------------------ | ------------------ | ----------------- |\n",
        "| **Mean Squared Error (MSE)**  | $\\frac{1}{N} \\sum (y - \\hat{y})^2$             | Penalizes large errors more strongly   | General regression |                    |                   |\n",
        "| **Mean Absolute Error (MAE)** | ( \\frac{1}{N} \\sum                             | y - \\hat{y}                            | )                  | Robust to outliers | Robust regression |\n",
        "| **Huber Loss**                | Piecewise: MSE for small errors, MAE for large | Combines robustness & smooth gradients | Noisy regression   |                    |                   |\n",
        "| **Log-Cosh Loss**             | $\\sum \\log(\\cosh(y - \\hat{y}))$                | Smooth and less sensitive to outliers  | Stable regression  |                    |                   |\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Classification Loss Functions**\n",
        "\n",
        "Used when predicting **discrete classes**.\n",
        "\n",
        "| Loss Function                        | Formula                                    | Key Feature                        | Use Case                   |\n",
        "| ------------------------------------ | ------------------------------------------ | ---------------------------------- | -------------------------- |\n",
        "| **Binary Cross-Entropy (Log Loss)**  | $-[y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})]$ | Works with probabilities in (0,1)  | Binary classification      |\n",
        "| **Categorical Cross-Entropy**        | $-\\sum y_i \\log(\\hat{y}_i)$                | Multi-class probability prediction | Multi-class classification |\n",
        "| **Sparse Categorical Cross-Entropy** | Like categorical but with integer labels   | Memory-efficient                   | Large-class classification |\n",
        "| **Hinge Loss**                       | $\\max(0, 1 - y\\hat{y})$                    | Margin-based                       | SVM-style classification   |\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Ranking & Probability Losses**\n",
        "\n",
        "Used for ranking problems or probabilistic outputs.\n",
        "\n",
        "| Loss Function                                   | Key Feature                                                     | Use Case                                  |\n",
        "| ----------------------------------------------- | --------------------------------------------------------------- | ----------------------------------------- |\n",
        "| **Kullback–Leibler Divergence (KL Divergence)** | Measures how one probability distribution diverges from another | Variational autoencoders, language models |\n",
        "| **Contrastive Loss**                            | Pushes similar pairs together, dissimilar apart                 | Siamese networks                          |\n",
        "| **Triplet Loss**                                | Uses anchor-positive-negative triplets for embedding learning   | Face recognition                          |\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Specialized Losses**\n",
        "\n",
        "For tasks beyond standard classification/regression.\n",
        "\n",
        "| Loss Function                                        | Key Feature                                       | Use Case                                    |\n",
        "| ---------------------------------------------------- | ------------------------------------------------- | ------------------------------------------- |\n",
        "| **Dice Loss**                                        | Measures overlap between predicted & actual masks | Medical image segmentation                  |\n",
        "| **IoU Loss (Jaccard Loss)**                          | Intersection-over-union for shapes                | Object detection & segmentation             |\n",
        "| **Perceptual Loss**                                  | Compares high-level features instead of pixels    | Image style transfer                        |\n",
        "| **CTC Loss (Connectionist Temporal Classification)** | Allows training without exact alignment           | Speech recognition, handwriting recognition |\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can prepare a **cheat sheet table** with **formulas, graphs, pros/cons, and best-use cases** for all these loss functions so you can revise them quickly before exams.\n",
        "Do you want me to make that next?\n"
      ],
      "metadata": {
        "id": "0h9kkdop367y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8 How does a neural network learn!\n",
        "A **neural network learns** by **adjusting its weights and biases** so that its predictions become closer to the correct answers.\n",
        "This happens through a cycle of **forward pass → loss calculation → backward pass → parameter update**.\n",
        "## **1. The Learning Process (Step-by-Step)**\n",
        "\n",
        "### **Step 1: Forward Propagation**\n",
        "\n",
        "* Input data ($\\mathbf{x}$) passes through the network layer by layer.\n",
        "* Each neuron computes:\n",
        "\n",
        "$$\n",
        "z = \\mathbf{W} \\mathbf{x} + \\mathbf{b}\n",
        "$$\n",
        "\n",
        "* An **activation function** $f(z)$ adds non-linearity.\n",
        "* The final output $\\hat{y}$ is the network’s prediction.\n",
        "### **Step 2: Loss Calculation**\n",
        "\n",
        "* The network’s output $\\hat{y}$ is compared to the actual target $y$ using a **loss function**:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(y, \\hat{y})\n",
        "$$\n",
        "\n",
        "* This gives a number that measures **how wrong** the network is.\n",
        "### **Step 3: Backpropagation**\n",
        "\n",
        "* The loss is propagated **backward** through the network to compute the **gradient** of the loss with respect to each weight and bias.\n",
        "* Uses the **chain rule of calculus**:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial W_{ij}} = \\frac{\\partial \\mathcal{L}}{\\partial a_j} \\cdot \\frac{\\partial a_j}{\\partial z_j} \\cdot \\frac{\\partial z_j}{\\partial W_{ij}}\n",
        "$$\n",
        "### **Step 4: Weight Update (Gradient Descent)**\n",
        "\n",
        "* The network updates parameters using:\n",
        "\n",
        "$$\n",
        "W \\leftarrow W - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b \\leftarrow b - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $\\eta$ = learning rate (step size)\n",
        "* Gradients come from backpropagation\n",
        "### **Step 5: Repeat**\n",
        "\n",
        "* This process is repeated for many **epochs** (full passes through the training data) until:\n",
        "\n",
        "  * The loss becomes small enough\n",
        "  * Or performance stops improving\n",
        "## **2. Summary Flow**\n",
        "\n",
        "1. **Forward pass** → Get predictions.\n",
        "2. **Loss function** → Measure error.\n",
        "3. **Backpropagation** → Calculate gradients.\n",
        "4. **Gradient descent** → Update weights.\n",
        "5. **Repeat** until the model learns patterns.\n",
        "## **3. Analogy**\n",
        "\n",
        "Think of it like **throwing darts blindfolded**:\n",
        "\n",
        "* You throw a dart (make a prediction).\n",
        "* Someone tells you how far you missed (loss function).\n",
        "* You adjust your aim based on feedback (backpropagation).\n",
        "* Over time, you hit closer to the bullseye (better prediction.\n"
      ],
      "metadata": {
        "id": "UhdhONsY4boH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9 What is an optimizer in neural networks, and why is it necessary!\n",
        "An **optimizer** in a neural network is an **algorithm** that updates the model’s weights and biases during training so that the **loss function is minimized**.\n",
        "\n",
        "In short:\n",
        "\n",
        "* **Loss function** → tells us *how wrong* the model is.\n",
        "* **Optimizer** → decides *how to change* the weights to get better.\n",
        "## **1. Why It’s Necessary**\n",
        "\n",
        "* Without an optimizer, the weights in the network wouldn’t change, and the model would **never learn**.\n",
        "* Optimizers decide **direction** (which way to move in the loss landscape) and **magnitude** (how big a step to take).\n",
        "* They help find the set of parameters $\\Theta = \\{W, b\\}$ that make predictions most accurate.\n",
        "## **2. How It Works**\n",
        "\n",
        "During training:\n",
        "\n",
        "1. **Forward pass** → Predictions are made.\n",
        "2. **Loss function** → Error is calculated.\n",
        "3. **Backpropagation** → Gradients ($\\frac{\\partial \\mathcal{L}}{\\partial W}$) are computed.\n",
        "4. **Optimizer** → Uses these gradients to update weights:\n",
        "\n",
        "$$\n",
        "W \\leftarrow W - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W}\n",
        "$$\n",
        "\n",
        "where $\\eta$ = learning rate.\n",
        "## **3. Common Optimizers**\n",
        "\n",
        "| Optimizer                             | Key Idea                                                         | Pros                                        | Cons                                   |\n",
        "| ------------------------------------- | ---------------------------------------------------------------- | ------------------------------------------- | -------------------------------------- |\n",
        "| **SGD (Stochastic Gradient Descent)** | Updates weights using gradient of one (or few) samples at a time | Simple, memory-efficient                    | May be slow to converge                |\n",
        "| **Momentum**                          | Adds a fraction of previous updates to current update            | Speeds up convergence, reduces oscillations | Needs tuning of momentum term          |\n",
        "| **Adagrad**                           | Adapts learning rate for each parameter based on past gradients  | Works well for sparse data                  | Learning rate may decay too much       |\n",
        "| **RMSProp**                           | Keeps moving average of squared gradients                        | Works well for RNNs                         | Needs learning rate tuning             |\n",
        "| **Adam (Adaptive Moment Estimation)** | Combines Momentum + RMSProp                                      | Fast, widely used, minimal tuning           | Can overfit if learning rate not tuned |\n",
        "| **AdamW**                             | Adam with weight decay for regularization                        | Better generalization                       | Slightly more complex                  |\n",
        "\n",
        "## **4. Analogy**\n",
        "\n",
        "Training a neural network is like **hiking down a mountain blindfolded**:\n",
        "\n",
        "* **Loss function** = your altitude (you want to minimize it).\n",
        "* **Gradients** = tell you which way is downhill.\n",
        "* **Optimizer** = decides how big a step you should take and adjusts your path for efficiency\n"
      ],
      "metadata": {
        "id": "BbUV-COi5yC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Could you briefly describe some common optimizers!\n",
        "Sure — here’s a **quick overview** of the most common neural network optimizers:\n",
        "### **1. Stochastic Gradient Descent (SGD)**\n",
        "\n",
        "* **How it works**: Updates weights using the gradient from a single (or small batch of) training sample(s).\n",
        "* **Update rule**:\n",
        "\n",
        "  $$\n",
        "  W \\leftarrow W - \\eta \\cdot \\nabla_W \\mathcal{L}\n",
        "  $$\n",
        "* **Pros**: Simple, memory-efficient.\n",
        "* **Cons**: Can be slow, oscillates in narrow valleys.\n",
        "### **2. SGD with Momentum**\n",
        "\n",
        "* **How it works**: Adds a fraction of the previous update to the current update to speed up learning and smooth oscillations.\n",
        "* **Update rule**:\n",
        "\n",
        "  $$\n",
        "  v_t = \\beta v_{t-1} + \\eta \\nabla_W \\mathcal{L}\n",
        "  $$\n",
        "\n",
        "  $$\n",
        "  W \\leftarrow W - v_t\n",
        "  $$\n",
        "* **Pros**: Faster convergence, especially in deep networks.\n",
        "* **Cons**: Needs momentum term $\\beta$ tuning.\n",
        "### **3. Adagrad**\n",
        "\n",
        "* **How it works**: Adapts the learning rate for each parameter based on historical gradient magnitude.\n",
        "* **Update rule**:\n",
        "\n",
        "  $$\n",
        "  W \\leftarrow W - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\cdot \\nabla_W \\mathcal{L}\n",
        "  $$\n",
        "* **Pros**: Good for sparse features (e.g., NLP).\n",
        "* **Cons**: Learning rate keeps shrinking, may stop learning.\n",
        "### **4. RMSProp**\n",
        "\n",
        "* **How it works**: Keeps an exponentially decaying average of squared gradients for normalization.\n",
        "* **Update rule**:\n",
        "\n",
        "  $$\n",
        "  W \\leftarrow W - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\cdot \\nabla_W \\mathcal{L}\n",
        "  $$\n",
        "* **Pros**: Works well for RNNs and non-stationary problems.\n",
        "* **Cons**: Needs tuning of decay rate.\n",
        "### **5. Adam (Adaptive Moment Estimation)**\n",
        "\n",
        "* **How it works**: Combines **Momentum** (moving average of gradients) and **RMSProp** (adaptive learning rates).\n",
        "* **Update rule**: Uses first moment $m_t$ and second moment $v_t$ estimates:\n",
        "\n",
        "  $$\n",
        "  W \\leftarrow W - \\frac{\\eta \\cdot \\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
        "  $$\n",
        "* **Pros**: Fast, widely used, works well out of the box.\n",
        "* **Cons**: Can generalize poorly if not tuned.\n",
        "\n",
        "### **6. AdamW**\n",
        "\n",
        "* **How it works**: Adam + weight decay for better regularization.\n",
        "* **Pros**: Better generalization than Adam.\n",
        "* **Cons**: Slightly more complex.\n"
      ],
      "metadata": {
        "id": "hRTT0ozZ6g9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11 Can you explain forward and backward propagation in a neural network!\n",
        "Sure — let’s break it down clearly.\n",
        "## **1. Forward Propagation**\n",
        "\n",
        "**Goal:** Pass input data through the network to get predictions.\n",
        "\n",
        "### **Step-by-step**\n",
        "\n",
        "1. **Input Layer** — The data $\\mathbf{x}$ is fed into the network.\n",
        "2. **Weighted Sum** — Each neuron calculates:\n",
        "\n",
        "   $$\n",
        "   z^{(l)} = \\mathbf{W}^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)}\n",
        "   $$\n",
        "\n",
        "   where:\n",
        "\n",
        "   * $l$ = layer number\n",
        "   * $\\mathbf{a}^{(0)} = \\mathbf{x}$ (input features)\n",
        "3. **Activation Function** — Apply a non-linear function:\n",
        "\n",
        "   $$\n",
        "   a^{(l)} = f^{(l)}(z^{(l)})\n",
        "   $$\n",
        "4. **Output Layer** — Produces the prediction $\\hat{\\mathbf{y}}$.\n",
        "\n",
        "💡 **Analogy:** Like water flowing forward through pipes — each layer processes and passes data along.\n",
        "\n",
        "## **2. Backward Propagation (Backprop)**\n",
        "\n",
        "**Goal:** Calculate how each weight contributed to the error, so we can update them.\n",
        "\n",
        "### **Step-by-step**\n",
        "\n",
        "1. **Loss Calculation** — Compare prediction $\\hat{\\mathbf{y}}$ with actual target $\\mathbf{y}$ using a **loss function**:\n",
        "\n",
        "   $$\n",
        "   \\mathcal{L}(y, \\hat{y})\n",
        "   $$\n",
        "2. **Error at Output** — Compute gradient of loss with respect to output:\n",
        "\n",
        "   $$\n",
        "   \\delta^{(L)} = \\frac{\\partial \\mathcal{L}}{\\partial z^{(L)}}\n",
        "   $$\n",
        "3. **Propagate Error Backwards** — For each hidden layer:\n",
        "\n",
        "   $$\n",
        "   \\delta^{(l)} = (\\mathbf{W}^{(l+1)})^T \\delta^{(l+1)} \\odot f'^{(l)}(z^{(l)})\n",
        "   $$\n",
        "\n",
        "   where $\\odot$ is element-wise multiplication.\n",
        "4. **Calculate Gradients** — For weights and biases:\n",
        "\n",
        "   $$\n",
        "   \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(l)}} = \\delta^{(l)} (\\mathbf{a}^{(l-1)})^T\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(l)}} = \\delta^{(l)}\n",
        "   $$\n",
        "5. **Update Weights** — Using an optimizer (like SGD, Adam):\n",
        "\n",
        "   $$\n",
        "   \\mathbf{W}^{(l)} \\leftarrow \\mathbf{W}^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(l)}}\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   \\mathbf{b}^{(l)} \\leftarrow \\mathbf{b}^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(l)}}\n",
        "   $$\n",
        "\n",
        "💡 **Analogy:** Imagine fixing a factory line — you start from the last stage (output) and trace back to see which machine (layer) introduced the error.\n",
        "## **3. Process Overview**\n",
        "\n",
        "1. **Forward Pass** → Make predictions.\n",
        "2. **Loss Calculation** → Measure error.\n",
        "3. **Backward Pass** → Find which weights caused the error.\n",
        "4. **Optimization Step** → Adjust weights to reduce future error.\n",
        "\n"
      ],
      "metadata": {
        "id": "T3234hC97IDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12 What is weight initialization, and how does it impact training!\n",
        "**Weight Initialization** is the process of assigning the starting values (usually small random numbers) to the weights of a neural network **before** training begins.\n",
        "\n",
        "It’s more important than it might sound — a bad initialization can make training very slow or even stop the network from learning altogether.\n",
        "\n",
        "## **Why Weight Initialization Matters**\n",
        "\n",
        "When training, the weights are updated step-by-step using gradients.\n",
        "If the starting weights are poorly chosen:\n",
        "\n",
        "1. **Too small (near zero)** → Signals shrink layer by layer → *vanishing gradients* → slow or no learning.\n",
        "2. **Too large** → Signals explode layer by layer → *exploding gradients* → unstable training.\n",
        "3. **All equal** → Every neuron learns the same thing → no diversity in features.\n",
        "\n",
        "A good initialization keeps the scale of activations and gradients **balanced** as they flow forward and backward through the network.\n",
        "\n",
        "## **Common Weight Initialization Methods**\n",
        "\n",
        "| Method                           | Idea                                                                                               | When to Use                                         |\n",
        "| -------------------------------- | -------------------------------------------------------------------------------------------------- | --------------------------------------------------- |\n",
        "| **Zero Initialization**          | All weights = 0                                                                                    | ❌ Never for hidden layers (causes symmetry problem) |\n",
        "| **Random Initialization**        | Small random numbers from uniform or normal distribution                                           | Very basic, often replaced by better methods        |\n",
        "| **Xavier/Glorot Initialization** | Variance depends on number of input & output neurons: $\\text{Var}(W) = \\frac{2}{n_{in} + n_{out}}$ | Works well with sigmoid/tanh                        |\n",
        "| **He Initialization**            | Variance: $\\text{Var}(W) = \\frac{2}{n_{in}}$                                                       | Works well with ReLU/variants                       |\n",
        "| **LeCun Initialization**         | Variance: $\\text{Var}(W) = \\frac{1}{n_{in}}$                                                       | Works well with SELU                                |\n",
        "| **Orthogonal Initialization**    | Weight matrix is orthogonal                                                                        | Used in RNNs for stable long-term dependencies      |\n",
        "## **Impact on Training**\n",
        "\n",
        "* **Faster convergence**: Good initialization can reduce the number of epochs needed.\n",
        "* **Stable gradients**: Prevents vanishing or exploding gradients.\n",
        "* **Better final accuracy**: Allows network to learn richer features early on\n",
        "💡 **Analogy:**\n",
        "Think of weight initialization like choosing a starting point for climbing a hill in fog (gradient descent).\n",
        "\n",
        "* If you start at a terrible spot (bad init), you might get stuck in a ditch (local minimum) or slide down endlessly (exploding gradients).\n",
        "* If you start at a reasonable height (good init), you reach the top faster and more reliably."
      ],
      "metadata": {
        "id": "UJPDRibt7wuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13 What is the vanishing gradient problem in deep learning!\n",
        "The **vanishing gradient problem** is a common issue in training deep neural networks, where the gradients (partial derivatives of the loss with respect to weights) become **very small** as they are backpropagated through many layers.\n",
        "\n",
        "Here’s what happens step-by-step:\n",
        "\n",
        "1. **Backpropagation uses the chain rule**\n",
        "\n",
        "   * The gradient at each layer is computed by multiplying derivatives from the next layer.\n",
        "   * In deep networks, this means multiplying many small numbers together.\n",
        "\n",
        "2. **Small derivatives shrink exponentially**\n",
        "\n",
        "   * If the activation function’s derivative is less than 1 (e.g., sigmoid or tanh), repeated multiplication across layers makes the gradient approach **zero** for early layers.\n",
        "\n",
        "3. **Impact**\n",
        "\n",
        "   * **Early layers** (closer to the input) learn **extremely slowly** because their weights receive almost no update.\n",
        "   * The network might fail to capture important low-level features.\n",
        "\n",
        "4. **Example with sigmoid activation**\n",
        "\n",
        "   * The sigmoid derivative is at most 0.25.\n",
        "   * If your network has 10 layers, multiplying numbers ≤ 0.25 repeatedly can make gradients vanish to near zero.\n",
        "\n",
        "5. **Why it’s a problem**\n",
        "\n",
        "   * Training becomes **very slow** or **stalls entirely**.\n",
        "   * The network might get stuck with poor performance.\n",
        "**Common solutions**:\n",
        "\n",
        "* Use activation functions with better gradient flow (e.g., **ReLU**, Leaky ReLU).\n",
        "* Use **batch normalization** to stabilize activations.\n",
        "* Use **residual connections** (ResNets) to shorten gradient paths.\n",
        "* Careful **weight initialization** (e.g., Xavier/He initialization)."
      ],
      "metadata": {
        "id": "r7qibt9I8bgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14 What is the exploding gradient problem?\n",
        "The **exploding gradient problem** is the opposite of the vanishing gradient problem — instead of gradients becoming tiny during backpropagation, they become **very large** as they pass through many layers.\n",
        "### How it happens\n",
        "\n",
        "1. **Backpropagation multiplies derivatives**\n",
        "\n",
        "   * In deep networks, if the derivatives or weight values are **greater than 1**, multiplying them repeatedly across layers can make gradients grow **exponentially**.\n",
        "\n",
        "2. **Causes**\n",
        "\n",
        "   * Poor **weight initialization** (too large values).\n",
        "   * Activation functions with large derivatives.\n",
        "   * **Recurrent Neural Networks (RNNs)** with long sequences are especially prone because of repeated weight multiplication over time.\n",
        "### Impact\n",
        "\n",
        "* **Unstable training**: The loss oscillates wildly or becomes `NaN` due to numerical overflow.\n",
        "* **Diverging weights**: The model fails to converge.\n",
        "* Large updates cause the network to overshoot the optimal weights.\n",
        "### Example intuition\n",
        "\n",
        "If the derivative in each layer is \\~2, and you have 10 layers:\n",
        "\n",
        "$$\n",
        "2^{10} = 1024\n",
        "$$\n",
        "\n",
        "That’s a **thousandfold increase** in the gradient magnitude, causing massive updates.\n",
        "### Common solutions\n",
        "\n",
        "* **Gradient clipping**: Limit the gradient’s maximum norm or value (common in RNNs).\n",
        "* **Careful weight initialization** (e.g., Xavier, He initialization).\n",
        "* **Lower learning rates**.\n",
        "* **Batch normalization** to keep activations stable.\n",
        "* **Residual networks** to shorten gradient paths.\n"
      ],
      "metadata": {
        "id": "cqAgUyp59TiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VLt4Suks9zfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practicle\n"
      ],
      "metadata": {
        "id": "xmKQw-lHUCH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 How do you create a simple perceptron for basic binary classification!\n",
        "Alright, let’s go step-by-step and make a **simple perceptron** for **binary classification** in Python.\n",
        "\n",
        "We’ll use only **NumPy** to show the core logic—no high-level ML libraries—so you can see exactly how it works--\n",
        "\n",
        "## **1. What is a Perceptron?**\n",
        "\n",
        "A perceptron is a single-layer neural network unit that:\n",
        "\n",
        "1. Takes inputs\n",
        "2. Multiplies them by weights\n",
        "3. Sums them up (plus a bias)\n",
        "4. Passes the result through a step function to output either 0 or 1.\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "$$\n",
        "y = f(w \\cdot x + b)\n",
        "$$\n",
        "\n",
        "where $f$ is usually the step function:\n",
        "\n",
        "$$\n",
        "f(z) = \\begin{cases}\n",
        "1 & \\text{if } z > 0 \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "## **2. Code for a Simple Perceptron**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Step activation function\n",
        "def step_function(x):\n",
        "    return np.where(x >= 0, 1, 0)\n",
        "\n",
        "# Perceptron training\n",
        "def perceptron_train(X, y, learning_rate=0.1, epochs=10):\n",
        "    weights = np.zeros(X.shape[1])\n",
        "    bias = 0\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for xi, target in zip(X, y):\n",
        "            linear_output = np.dot(xi, weights) + bias\n",
        "            prediction = step_function(linear_output)\n",
        "            update = learning_rate * (target - prediction)\n",
        "            weights += update * xi\n",
        "            bias += update\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "# Perceptron prediction\n",
        "def perceptron_predict(X, weights, bias):\n",
        "    return step_function(np.dot(X, weights) + bias)\n",
        "\n",
        "# Example dataset: AND logic gate\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Train perceptron\n",
        "weights, bias = perceptron_train(X, y, learning_rate=0.1, epochs=10)\n",
        "\n",
        "# Test perceptron\n",
        "predictions = perceptron_predict(X, weights, bias)\n",
        "\n",
        "print(\"Weights:\", weights)\n",
        "print(\"Bias:\", bias)\n",
        "print(\"Predictions:\", predictions)\n",
        "```\n",
        "## **3. Output Example (for AND gate)**\n",
        "\n",
        "```\n",
        "Weights: [0.1 0.1]\n",
        "Bias: -0.1\n",
        "Predictions: [0 0 0 1]\n",
        "``\n",
        "✅ **Key Notes**:\n",
        "\n",
        "* This example uses a **step function** so it can only do **linear binary classification** (e.g., AND, OR).\n",
        "* It won’t work for problems that aren’t linearly separable (e.g., XOR).\n",
        "* For more complex tasks, you’d use multiple perceptrons (multi-layer) and activation functions like **sigmoid**, **ReLU**, etc."
      ],
      "metadata": {
        "id": "-sLV1emoUH5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2 How can you build a neural network with one hidden layer using Keras!\n",
        "Alright — let’s go step-by-step and build a **neural network with one hidden layer** using **Keras** (part of TensorFlow).\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Basic Idea**\n",
        "\n",
        "We’ll make a neural network:\n",
        "\n",
        "* **Input layer** → takes features (X)\n",
        "* **Hidden layer** → applies weights, bias, and activation function\n",
        "* **Output layer** → gives final prediction\n",
        "\n",
        "For **binary classification**, the output layer will have **1 neuron** with a **sigmoid** activation.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Code Example**\n",
        "\n",
        "```python\n",
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Example dataset (XOR problem just for demo)\n",
        "import numpy as np\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([0,1,1,0])\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "\n",
        "# Hidden layer: 4 neurons, relu activation\n",
        "model.add(Dense(4, input_dim=2, activation='relu'))\n",
        "\n",
        "# Output layer: 1 neuron, sigmoid activation (binary classification)\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model.fit(X, y, epochs=200, verbose=0)\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(X, y)\n",
        "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(X)\n",
        "print(\"Predictions:\", predictions)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Explanation of Parameters**\n",
        "\n",
        "* `Dense(4, activation='relu')` → hidden layer with 4 neurons\n",
        "* `input_dim=2` → number of input features (in our example: two bits)\n",
        "* `Dense(1, activation='sigmoid')` → output layer for binary classification\n",
        "* `loss='binary_crossentropy'` → suitable loss for binary classification\n",
        "* `optimizer='adam'` → adaptive learning rate optimizer\n",
        "* `epochs=200` → number of passes through the training data\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Output Example**\n",
        "\n",
        "```\n",
        "Accuracy: 100.00%\n",
        "Predictions:\n",
        "[[0.01]\n",
        " [0.98]\n",
        " [0.98]\n",
        " [0.02]]\n",
        "```\n",
        "\n",
        "Here, outputs close to `0` mean class **0**, and close to `1` mean class **1**."
      ],
      "metadata": {
        "id": "0E_2HZeTVNun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 How do you initialize weights using the Xavier (Glorot) initialization method in Keras!\n",
        "In Keras, **Xavier initialization** is already available as **Glorot initialization** (named after Xavier Glorot, who proposed it).\n",
        "\n",
        "Here’s how you can use it:\n",
        "\n",
        "## **1. Using Glorot Initialization in Keras**\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.initializers import GlorotUniform, GlorotNormal\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Hidden layer with Xavier (Glorot) uniform initialization\n",
        "model.add(Dense(\n",
        "    units=4,\n",
        "    activation='relu',\n",
        "    kernel_initializer=GlorotUniform(),  # Xavier uniform\n",
        "    input_dim=2\n",
        "))\n",
        "\n",
        "# Output layer with Xavier (Glorot) normal initialization\n",
        "model.add(Dense(\n",
        "    units=1,\n",
        "    activation='sigmoid',\n",
        "    kernel_initializer=GlorotNormal()  # Xavier normal\n",
        "))\n",
        "```\n",
        "## **2. Explanation**\n",
        "\n",
        "* **`GlorotUniform()`** → draws weights from a uniform distribution\n",
        "\n",
        "  $$\n",
        "  \\text{U}\\left(-\\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}, \\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}\\right)\n",
        "  $$\n",
        "* **`GlorotNormal()`** → draws weights from a normal distribution\n",
        "\n",
        "  $$\n",
        "  \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{\\text{in}} + n_{\\text{out}}}}\\right)\n",
        "  $$\n",
        "* $n_{\\text{in}}$ = number of input neurons\n",
        "* $n_{\\text{out}}$ = number of output neurons in the layer\n",
        "## **3. Why Use Xavier Initialization?**\n",
        "\n",
        "It keeps the variance of the activations **consistent** across layers, preventing exploding or vanishing gradients during training, especially for activations like **sigmoid** and **tanh**."
      ],
      "metadata": {
        "id": "Wd4yBP_BVjsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4  How can you apply different activation functions in a neural network in Keras!\n",
        "In **Keras**, you can apply **different activation functions** simply by specifying the `activation` parameter in each `Dense` (or other) layer.\n",
        "You can mix and match per layer depending on your network’s needs.\n",
        "## **1. Example: Different Activations in Different Layers**\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "\n",
        "# Input → Hidden layer 1 with ReLU\n",
        "model.add(Dense(8, input_dim=4, activation='relu'))\n",
        "\n",
        "# Hidden layer 2 with Tanh\n",
        "model.add(Dense(6, activation='tanh'))\n",
        "\n",
        "# Hidden layer 3 with LeakyReLU (as a separate layer)\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "model.add(Dense(4))  # No activation here\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "# Output layer with Sigmoid (for binary classification)\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "```\n",
        "## **2. Notes on Activation Choices**\n",
        "\n",
        "* **ReLU** (`relu`) → default for hidden layers, fast & avoids vanishing gradients.\n",
        "* **Tanh** (`tanh`) → outputs in range \\[-1, 1], good for normalized data.\n",
        "* **Sigmoid** (`sigmoid`) → outputs in \\[0, 1], often used for binary classification output layers.\n",
        "* **Softmax** (`softmax`) → for multi-class classification output layers.\n",
        "* **LeakyReLU**, **ELU**, etc. → can be added as separate layers for more control.\n",
        "## **3. Using Activation Layers Explicitly**\n",
        "\n",
        "Instead of passing `activation='relu'` inside `Dense`, you can also use:\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.layers import Activation\n",
        "\n",
        "model.add(Dense(8, input_dim=4))\n",
        "model.add(Activation('relu'))\n",
        "```\n",
        "\n",
        "This gives more flexibility if you want to apply custom logic before/after activation."
      ],
      "metadata": {
        "id": "woTZGDj2V7ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 How do you add dropout to a neural network model to prevent overfitting!\n",
        "In a neural network, **dropout** is a regularization technique that helps prevent overfitting by randomly “dropping” (ignoring) a fraction of neurons during training. This forces the model to not rely too heavily on specific neurons and improves generalization.\n",
        "\n",
        "Here’s how to add dropout in **Keras** step-by-step:\n",
        "\n",
        "### **1. Import the required library**\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "```\n",
        "### **2. Define the model with dropout layers**\n",
        "\n",
        "```python\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer\n",
        "model.add(Dense(64, activation='relu', input_shape=(100,)))\n",
        "\n",
        "# Dropout layer (drop 30% of neurons)\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Hidden layer\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "# Dropout again\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Output layer (binary classification example)\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "```\n",
        "### **3. Compile and train**\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "```\n",
        "### **Key Points**\n",
        "\n",
        "* `Dropout(rate)` → **rate** is the fraction of neurons to drop, e.g., `0.3` means drop **30%** of neurons during training.\n",
        "* Dropout is **only applied during training**. During evaluation or prediction, all neurons are active.\n",
        "* It’s usually added **after dense or convolutional layers**, not before the input layer.\n"
      ],
      "metadata": {
        "id": "SLeCOBChg-zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 How do you manually implement forward propagation in a simple neural network!\n",
        "Alright — let’s go through **manual forward propagation** in a simple neural network step-by-step, without using Keras, TensorFlow, or PyTorch.\n",
        "\n",
        "We’ll make a **small example**:\n",
        "\n",
        "* Input layer: 2 neurons\n",
        "* One hidden layer: 2 neurons (ReLU activation)\n",
        "* Output layer: 1 neuron (Sigmoid activation)\n",
        "\n",
        "## **1. Import dependencies**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "```\n",
        "## **2. Define input, weights, and biases**\n",
        "\n",
        "```python\n",
        "# Example input (1 sample, 2 features)\n",
        "X = np.array([[0.5, 0.2]])\n",
        "\n",
        "# Weights and biases (initialized manually)\n",
        "W1 = np.array([[0.1, 0.4],    # weights for input → hidden\n",
        "               [0.8, 0.5]])   # shape: (2, 2)\n",
        "b1 = np.array([[0.1, 0.2]])   # bias for hidden layer\n",
        "\n",
        "W2 = np.array([[0.3],         # weights for hidden → output\n",
        "               [0.9]])        # shape: (2, 1)\n",
        "b2 = np.array([[0.05]])       # bias for output layer\n",
        "```\n",
        "## **3. Define activation functions**\n",
        "\n",
        "```python\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "```\n",
        "## **4. Forward propagation steps**\n",
        "\n",
        "```python\n",
        "# Step 1: Input → Hidden layer\n",
        "z1 = np.dot(X, W1) + b1\n",
        "a1 = relu(z1)\n",
        "\n",
        "# Step 2: Hidden → Output layer\n",
        "z2 = np.dot(a1, W2) + b2\n",
        "a2 = sigmoid(z2)  # final output\n",
        "```\n",
        "## **5. Print results**\n",
        "\n",
        "```python\n",
        "print(\"Hidden layer linear output (z1):\", z1)\n",
        "print(\"Hidden layer activation (a1):\", a1)\n",
        "print(\"Output layer linear output (z2):\", z2)\n",
        "print(\"Final prediction (a2):\", a2)\n",
        "```\n",
        "### **Flow Recap**\n",
        "\n",
        "1. **Linear transformation**: $z = XW + b$\n",
        "2. **Apply activation**: $a = f(z)$\n",
        "3. Repeat for each layer until the output layer is reached."
      ],
      "metadata": {
        "id": "RQ2DA2RYiXT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7  How do you add batch normalization to a neural network model in Keras!\n",
        "Batch Normalization (**BN**) is a technique that normalizes the output of a layer to have a mean of 0 and a variance of 1 during training, which helps:\n",
        "\n",
        "* **Speed up convergence**\n",
        "* **Reduce internal covariate shift**\n",
        "* **Act as a form of regularization** (sometimes reducing the need for dropout)\n",
        "## **1. Import the necessary classes**\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
        "```\n",
        "## **2. Add Batch Normalization to the model**\n",
        "\n",
        "Here’s a simple example:\n",
        "\n",
        "```python\n",
        "model = Sequential()\n",
        "\n",
        "# Input + Dense\n",
        "model.add(Dense(64, activation='relu', input_shape=(100,)))\n",
        "\n",
        "# Batch Normalization\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Optional Dropout (for extra regularization)\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Hidden layer\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "```\n",
        "## **3. Compile and train**\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "```\n",
        "### **Best Practices**\n",
        "\n",
        "* Place **`BatchNormalization()`** *after* the Dense/Conv layer, but **before** the activation in some designs:\n",
        "\n",
        "  ```python\n",
        "  model.add(Dense(64, use_bias=False))  # Bias not needed, BN handles it\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  ```\n",
        "* BN can sometimes reduce the need for dropout — but using both is possible.\n",
        "* It works well for deep networks by stabilizing training."
      ],
      "metadata": {
        "id": "U0qaMgT5jE93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8 How can you visualize the training process with accuracy and loss curves!\n",
        "To visualize the **training process** of a neural network, you can plot **accuracy** and **loss curves** from the `History` object returned by `model.fit()` in Keras.\n",
        "\n",
        "## **1. Train the model and store history**\n",
        "\n",
        "```python\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)\n",
        "```\n",
        "\n",
        "The `history` variable contains `history.history`, a dictionary with keys like:\n",
        "\n",
        "* `\"accuracy\"` → training accuracy\n",
        "* `\"val_accuracy\"` → validation accuracy\n",
        "* `\"loss\"` → training loss\n",
        "* `\"val_loss\"` → validation loss\n",
        "## **2. Plot accuracy and loss curves**\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot Loss\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "## **3. What to look for**\n",
        "\n",
        "* **If training loss keeps going down but validation loss goes up** → overfitting.\n",
        "* **If both losses stagnate at high values** → underfitting.\n",
        "* **Smooth curves** are good; highly noisy curves might mean a learning rate that’s too high.\n"
      ],
      "metadata": {
        "id": "B342iUISj4LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9 How can you use gradient clipping in Keras to control the gradient size and prevent exploding gradients!\n",
        "**Gradient clipping** is a technique that limits (clips) the size of gradients during backpropagation to prevent the **exploding gradient problem** — where gradients become extremely large, causing unstable training and NaN losses.\n",
        "\n",
        "In **Keras**, you set gradient clipping when you create the optimizer.\n",
        "There are **two common methods**:\n",
        "## **1. Clip by Value**\n",
        "\n",
        "Limits each gradient component to a fixed range $[-clipvalue, clipvalue]$.\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001, clipvalue=1.0)  # Clip each gradient value\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "This is like saying: *“No single gradient component should be bigger than 1 or smaller than -1.”*\n",
        "\n",
        "## **2. Clip by Norm**\n",
        "\n",
        "Scales gradients so that the overall **L2 norm** does not exceed a set value.\n",
        "\n",
        "```python\n",
        "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)  # Clip gradient vector norm\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "This is like saying: *“The whole gradient vector’s size can’t be bigger than 1.”*\n",
        "### **When to Use Which**\n",
        "\n",
        "* **clipvalue** → Good for very aggressive gradient spikes in individual weights.\n",
        "* **clipnorm** → More common in deep learning; keeps overall gradient scale controlled.\n",
        "\n"
      ],
      "metadata": {
        "id": "-r9wOYW2k2T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10 How can you create a custom loss function in Keras!\n",
        "In **Keras**, you can create a **custom loss function** either as:\n",
        "\n",
        "1. **A Python function**\n",
        "2. **A subclass of `tf.keras.losses.Loss`** (for more control)\n",
        "\n",
        "Let’s go through both.\n",
        "## **1. Custom Loss as a Function**\n",
        "\n",
        "The function must take **`y_true`** and **`y_pred`** as arguments and return a scalar loss value.\n",
        "Example: Mean Squared Error (custom version)\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_mse(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "```\n",
        "\n",
        "**Using it:**\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='adam',\n",
        "              loss=custom_mse,\n",
        "              metrics=['mae'])\n",
        "```\n",
        "## **2. Custom Loss with Extra Parameters**\n",
        "\n",
        "If you want a parameterized loss (e.g., weighted loss), you can use a closure:\n",
        "\n",
        "```python\n",
        "def weighted_mse(weight):\n",
        "    def loss(y_true, y_pred):\n",
        "        return tf.reduce_mean(weight * tf.square(y_true - y_pred))\n",
        "    return loss\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=weighted_mse(0.5),\n",
        "              metrics=['mae'])\n",
        "```\n",
        "## **3. Custom Loss as a Class**\n",
        "\n",
        "Subclass `tf.keras.losses.Loss` for advanced cases:\n",
        "\n",
        "```python\n",
        "class CustomHuberLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, delta=1.0, name=\"custom_huber_loss\"):\n",
        "        super().__init__(name=name)\n",
        "        self.delta = delta\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) <= self.delta\n",
        "        small_error_loss = 0.5 * tf.square(error)\n",
        "        big_error_loss = self.delta * (tf.abs(error) - 0.5 * self.delta)\n",
        "        return tf.reduce_mean(tf.where(is_small_error, small_error_loss, big_error_loss))\n",
        "```\n",
        "\n",
        "**Using it:**\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='adam',\n",
        "              loss=CustomHuberLoss(delta=1.5),\n",
        "              metrics=['mae'])\n",
        "```\n",
        "✅ **Key Points:**\n",
        "\n",
        "* Always return a **scalar tensor** from your loss.\n",
        "* Loss should be **differentiable** so TensorFlow can compute gradients.\n",
        "* You can mix TensorFlow ops and NumPy, but NumPy won’t be differentiable\n"
      ],
      "metadata": {
        "id": "rnN8p3C1lYRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11 How can you visualize the structure of a neural network model in Keras?\n",
        "In **Keras**, you can visualize a neural network’s structure in a couple of ways — either as a **summary in text form** or as a **graph diagram**.\n",
        "## **1. View Model Summary (Text)**\n",
        "\n",
        "```python\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "This shows:\n",
        "\n",
        "* Layer names\n",
        "* Output shapes\n",
        "* Number of parameters\n",
        "\n",
        "Example output:\n",
        "\n",
        "```\n",
        "Layer (type)          Output Shape       Param #\n",
        "================================================\n",
        "dense (Dense)         (None, 64)         6464\n",
        "dropout (Dropout)     (None, 64)         0\n",
        "dense_1 (Dense)       (None, 32)         2080\n",
        "dense_2 (Dense)       (None, 1)          33\n",
        "================================================\n",
        "Total params: 8,577\n",
        "Trainable params: 8,577\n",
        "Non-trainable params: 0\n",
        "```\n",
        "## **2. Visualize as a Diagram**\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
        "```\n",
        "\n",
        "* `show_shapes=True` → displays output shapes per layer\n",
        "* `show_layer_names=True` → shows the names you gave layers\n",
        "\n",
        "**Note:** You need `pydot` and `graphviz` installed:\n",
        "\n",
        "```bash\n",
        "pip install pydot graphviz\n",
        "```\n",
        "## **3. In Jupyter Notebook (Inline Display)**\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import Image\n",
        "\n",
        "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
        "Image(filename='model.png')\n",
        "```"
      ],
      "metadata": {
        "id": "V-Ee1XHJlu1k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}