{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPO8ata4DJNWYG7HNYsY1cd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalki81000/NEURAL-NETWORK-ASSIGNMENT-/blob/main/Untitled132.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLP Introduction and Text Preprocessing"
      ],
      "metadata": {
        "id": "PrIcoFdNzAfA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlHobGiMy2oe"
      },
      "outputs": [],
      "source": [
        "#  What is the primary goal of Natural Language Processing (NLP) ?\n",
        "The **primary goal of Natural Language Processing (NLP)** is to enable computers to **understand, interpret, generate, and interact with human language** in a way that is meaningful and useful.\n",
        "\n",
        "In simple terms:\n",
        "üëâ NLP bridges the gap between **human communication (natural language)** and **computer understanding (machine language)**.\n",
        "\n",
        "Key objectives include:\n",
        "\n",
        "1. **Understanding** ‚Äì Making machines comprehend spoken or written text (e.g., sentiment analysis, question answering).\n",
        "2. **Representation** ‚Äì Converting human language into structured formats that machines can process (e.g., embeddings, parsing).\n",
        "3. **Generation** ‚Äì Enabling machines to produce human-like language (e.g., chatbots, text summarization, translation).\n",
        "4. **Interaction** ‚Äì Facilitating smooth communication between humans and computers (e.g., voice assistants, search engines).\n",
        "\n",
        "‚úÖ In short: The goal of NLP is to make machines **process and use natural language as effectively as humans do.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 What does \"tokenization\" refer to in text processing ?\n",
        "Great question üëç\n",
        "\n",
        "In **text processing**, **tokenization** refers to the process of **breaking down text into smaller units called *tokens***.\n",
        "\n",
        "* These tokens can be **words, subwords, characters, or even sentences**, depending on the application.\n",
        "* Example:\n",
        "  ```\n",
        "  Text: \"Natural Language Processing is fun!\"\n",
        "  Tokens (word-level): [\"Natural\", \"Language\", \"Processing\", \"is\", \"fun\", \"!\"]\n",
        "  ```\n",
        "üîë **Why it‚Äôs important:**\n",
        "\n",
        "* It‚Äôs usually the **first step in NLP**, because models and algorithms can‚Äôt directly work with raw text.\n",
        "* Tokenization helps in **structuring text** so that it can be analyzed, processed, or converted into numbers (vectors/embeddings).\n",
        "\n",
        "üëâ In short: **Tokenization = splitting text into manageable pieces (tokens) so machines can understand it.**\n"
      ],
      "metadata": {
        "id": "cA6NM_Csz0uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 What is the difference between lemmatization and stemming ?\n",
        "Good one üëç This is a very common exam and interview question.\n",
        "\n",
        "Here‚Äôs the **difference between Lemmatization and Stemming** in NLP:\n",
        "### üîπ **Stemming**\n",
        "\n",
        "* **Definition:** A rule-based process that chops off prefixes or suffixes to get the \"root\" form of a word.\n",
        "* **Output:** May not be a real word.\n",
        "* **Speed:** Fast but less accurate.\n",
        "* **Example:**\n",
        "\n",
        "  * *\"studies\" ‚Üí \"studi\"*\n",
        "  * *\"playing\" ‚Üí \"play\"*\n",
        "  * *\"better\" ‚Üí \"bett\"*\n",
        "### üîπ **Lemmatization**\n",
        "\n",
        "* **Definition:** A vocabulary and dictionary-based process that reduces a word to its **base form (lemma)**, considering the **word‚Äôs meaning and context**.\n",
        "* **Output:** Always a valid word.\n",
        "* **Speed:** Slower but more accurate.\n",
        "* **Example:**\n",
        "\n",
        "  * *\"studies\" ‚Üí \"study\"*\n",
        "  * *\"playing\" ‚Üí \"play\"*\n",
        "  * *\"better\" ‚Üí \"good\"* (context-aware)\n",
        "### ‚úÖ **Key Difference (in one line):**\n",
        "\n",
        "* **Stemming = crude cutting of word endings (fast, may produce non-words).**\n",
        "* **Lemmatization = context-aware reduction to dictionary base form (slower, meaningful words).**\n"
      ],
      "metadata": {
        "id": "jqsxvqpV0ajH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 What is the role of regular expressions (regex) in text processing ?\n",
        "Great question üëç\n",
        "\n",
        "### üîπ **Role of Regular Expressions (Regex) in Text Processing**\n",
        "\n",
        "Regular expressions (regex) are **patterns used to search, match, and manipulate text** efficiently.\n",
        "### ‚úÖ **Key Roles:**\n",
        "\n",
        "1. **Text Matching & Searching**\n",
        "\n",
        "   * Find specific words, phrases, or patterns in text.\n",
        "   * Example: `\\d+` ‚Üí matches numbers like *\"123\"*\n",
        "\n",
        "2. **Text Cleaning & Preprocessing**\n",
        "\n",
        "   * Remove unwanted characters, symbols, or extra spaces.\n",
        "   * Example: `[^a-zA-Z0-9]` ‚Üí removes special characters.\n",
        "\n",
        "3. **Tokenization / Splitting**\n",
        "\n",
        "   * Split text into tokens based on spaces, punctuation, or patterns.\n",
        "   * Example: `re.split(\"\\s+\", \"NLP is fun\")` ‚Üí `[\"NLP\", \"is\", \"fun\"]`\n",
        "\n",
        "4. **Validation**\n",
        "\n",
        "   * Check if text follows a certain format.\n",
        "   * Example: `^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}$` ‚Üí validates email.\n",
        "\n",
        "5. **Substitution / Replacement**\n",
        "\n",
        "   * Replace patterns in text.\n",
        "   * Example: `re.sub(\"\\d\", \"#\", \"Room 123\")` ‚Üí `\"Room ###\"`\n",
        "### üîë **In short:**\n",
        "\n",
        "**Regex is a powerful tool in NLP/text processing to find, clean, split, and transform text based on patterns.**\n"
      ],
      "metadata": {
        "id": "kolkD7Ia0age"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 What is Word2Vec and how does it represent words in a vector space ?\n",
        "Word2Vec is a **neural network-based model** introduced by Google (Mikolov et al., 2013) that learns to represent words as **dense vectors (embeddings)** in a continuous vector space, where words with similar meanings are located close to each other.\n",
        "### üîë Key Idea:\n",
        "\n",
        "Instead of representing words as **one-hot vectors** (sparse and high-dimensional, with no notion of similarity), Word2Vec maps each word into a **low-dimensional dense vector** (e.g., 100‚Äì300 dimensions) such that **semantic and syntactic relationships are captured**.\n",
        "### ‚öôÔ∏è How Word2Vec Works:\n",
        "\n",
        "It‚Äôs based on the **distributional hypothesis**:\n",
        "*\"Words that occur in similar contexts tend to have similar meanings.\"*\n",
        "\n",
        "There are two main architectures:\n",
        "\n",
        "1. **Continuous Bag of Words (CBOW)**\n",
        "\n",
        "   * Predicts a target word from its surrounding context words.\n",
        "   * Example: context = \\[\"the\", \"cat\", \"on\", \"the\", \"‚Ä¶\"], predict \"mat\".\n",
        "   * Learns embeddings such that words appearing in similar contexts are closer.\n",
        "\n",
        "2. **Skip-Gram**\n",
        "\n",
        "   * Predicts surrounding context words given a target word.\n",
        "   * Example: input = \"cat\", predict context = \\[\"the\", \"on\", \"mat\"].\n",
        "   * Works better for small datasets and rare words.\n",
        "### üìå Representation in Vector Space:\n",
        "\n",
        "* Each word is represented by a **dense vector of real numbers**.\n",
        "* Words with **similar meanings** (e.g., *king, queen, prince*) have embeddings close together.\n",
        "* Word2Vec embeddings capture **linear relationships** too, e.g.:\n",
        "\n",
        "$$\n",
        "\\text{vector(\"King\")} - \\text{vector(\"Man\")} + \\text{vector(\"Woman\")} \\approx \\text{vector(\"Queen\")}\n",
        "$$\n",
        "### ‚úÖ Summary:\n",
        "\n",
        "* Word2Vec = model that learns **word embeddings**.\n",
        "* Uses **CBOW or Skip-Gram**.\n",
        "* Represents words in a **low-dimensional vector space** where distances/angles reflect semantic similarity.\n",
        "* Makes NLP tasks like sentiment analysis, translation, and question answering more effective."
      ],
      "metadata": {
        "id": "a_gjERfz0adN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 How does frequency distribution help in text analysis ?\n",
        "Great question! Let‚Äôs break it down.\n",
        "## üìä What is Frequency Distribution?\n",
        "\n",
        "A **frequency distribution** in text analysis is simply a count of how often each word (or token) appears in a text or corpus.\n",
        "For example, in the sentence:\n",
        "\n",
        "*\"The cat sat on the mat.\"*\n",
        "\n",
        "Word frequency distribution would be:\n",
        "\n",
        "* the ‚Üí 2\n",
        "* cat ‚Üí 1\n",
        "* sat ‚Üí 1\n",
        "* on ‚Üí 1\n",
        "* mat ‚Üí 1\n",
        "## üîë How Frequency Distribution Helps in Text Analysis:\n",
        "\n",
        "1. **Identify Important/Relevant Words**\n",
        "\n",
        "   * Frequently used words often indicate the **main topics** or themes.\n",
        "   * Example: In a news dataset, words like *election, candidate, vote* may dominate.\n",
        "\n",
        "2. **Feature Extraction for NLP Models**\n",
        "\n",
        "   * Frequencies can be used as features in machine learning (e.g., **Bag of Words** model).\n",
        "   * Helps convert text into numerical form for algorithms.\n",
        "\n",
        "3. **Removing Stopwords**\n",
        "\n",
        "   * High-frequency words like *the, is, and* usually add little meaning.\n",
        "   * Frequency analysis helps identify and remove these stopwords.\n",
        "\n",
        "4. **Keyword Extraction & Topic Modeling**\n",
        "\n",
        "   * Medium-frequency words often point to **keywords** and **themes**.\n",
        "   * Useful for summarization and search engines.\n",
        "\n",
        "5. **Zipf‚Äôs Law Verification**\n",
        "\n",
        "   * In natural language, a few words occur very frequently, while most words are rare.\n",
        "   * Frequency distribution helps confirm this property in a corpus.\n",
        "\n",
        "6. **Detecting Noise or Anomalies**\n",
        "\n",
        "   * Unusually frequent rare words may indicate **spam**, **errors**, or **domain-specific jargon**.\n",
        "\n",
        "7. **Visualization & Insights**\n",
        "\n",
        "   * Frequency distribution can be plotted as **word clouds** or **histograms** to quickly grasp the text‚Äôs content.\n",
        "‚úÖ **Summary:**\n",
        "Frequency distribution helps in **understanding, cleaning, and transforming text data** into useful insights. It reveals patterns, important words, and topics, making it a **foundational step in NLP and text mining**.\n"
      ],
      "metadata": {
        "id": "7r3AdfJz2fSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 Why is text normalization important in NLP ?\n",
        "Text normalization is the process of **transforming text into a standard, consistent format** before feeding it into an NLP pipeline.\n",
        "It deals with the many variations in natural language (spelling, capitalization, contractions, symbols, etc.) so that the model can interpret the text more effectively.\n",
        "## ‚úÖ Why Text Normalization is Important in NLP?\n",
        "\n",
        "1. **Reduces Vocabulary Size**\n",
        "\n",
        "   * Without normalization, words like *\"run\", \"runs\", \"running\"* are treated as separate tokens.\n",
        "   * Normalization (stemming/lemmatization) maps them to one base form (*\"run\"*), reducing dimensionality.\n",
        "\n",
        "2. **Improves Model Accuracy**\n",
        "\n",
        "   * Models learn better when they don‚Äôt waste parameters distinguishing between unnecessary variations (*USA, U.S.A, US*).\n",
        "   * Ensures similar words are treated as the same concept.\n",
        "\n",
        "3. **Handles Noise in Data**\n",
        "\n",
        "   * Real-world text often has typos, abbreviations, emojis, mixed casing, etc.\n",
        "   * Normalization cleans this noise (e.g., *‚ÄúThx‚Äù ‚Üí ‚Äúthanks‚Äù*).\n",
        "\n",
        "4. **Makes Comparisons Possible**\n",
        "\n",
        "   * If you want to search for ‚ÄúApple‚Äù in a text, but the document contains *apple, APPLE, apples*, normalization ensures consistency.\n",
        "\n",
        "5. **Enables Better Frequency Analysis**\n",
        "\n",
        "   * Without normalization, frequency counts are scattered across variations of the same word.\n",
        "   * Normalization groups them together, leading to more meaningful analysis.\n",
        "\n",
        "6. **Essential for Downstream NLP Tasks**\n",
        "\n",
        "   * Tasks like **sentiment analysis, machine translation, question answering, and text classification** all rely on clean, consistent input.\n",
        "## ‚öôÔ∏è Common Text Normalization Steps:\n",
        "\n",
        "* **Lowercasing**: \"ChatGPT\" ‚Üí \"chatgpt\"\n",
        "* **Removing punctuation/special characters**: \"hello!!!\" ‚Üí \"hello\"\n",
        "* **Expanding contractions**: \"don't\" ‚Üí \"do not\"\n",
        "* **Stemming / Lemmatization**: \"running\" ‚Üí \"run\"\n",
        "* **Removing stopwords**: \"the, is, and\" ‚Üí (removed)\n",
        "* **Handling spelling variations**: \"colour\" ‚Üí \"color\"\n",
        "‚úÖ **In short:**\n",
        "Text normalization is important in NLP because it **reduces noise, ensures consistency, and improves both efficiency and accuracy** of language models and downstream tasks.\n"
      ],
      "metadata": {
        "id": "YGFlKPum2fOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8 What is the difference between sentence tokenization and word tokenization?\n",
        "## üîë **Sentence Tokenization vs Word Tokenization**\n",
        "\n",
        "### 1. **Sentence Tokenization (Sentence Segmentation)**\n",
        "\n",
        "* **Definition**: Breaking a text into individual **sentences**.\n",
        "* **Purpose**: Helps models understand boundaries of thoughts or complete units of meaning.\n",
        "* **Example**:\n",
        "  Input:\n",
        "  `\"Hello world. How are you today?\"`\n",
        "  Output:\n",
        "  `[\"Hello world.\", \"How are you today?\"]`\n",
        "\n",
        "‚úÖ Useful for: summarization, translation, sentiment analysis at the sentence level.\n",
        "### 2. **Word Tokenization**\n",
        "\n",
        "* **Definition**: Splitting a sentence into **words or tokens** (basic meaningful units).\n",
        "* **Purpose**: Prepares text for further NLP tasks like frequency analysis, embeddings, parsing.\n",
        "* **Example**:\n",
        "  Input:\n",
        "  `\"How are you today?\"`\n",
        "  Output:\n",
        "  `[\"How\", \"are\", \"you\", \"today\", \"?\"]`\n",
        "\n",
        "‚úÖ Useful for: building vocabulary, word embeddings (Word2Vec, GloVe), language modeling.\n",
        "## ‚öñÔ∏è **Key Differences**\n",
        "\n",
        "| Aspect             | Sentence Tokenization             | Word Tokenization                    |\n",
        "| ------------------ | --------------------------------- | ------------------------------------ |\n",
        "| **Unit of split**  | Sentences                         | Words (or sub-words)                 |\n",
        "| **Granularity**    | Coarse (larger chunks)            | Fine-grained (smaller tokens)        |\n",
        "| **Output Example** | \\[\"Hello world.\", \"How are you?\"] | \\[\"Hello\", \"world\", \".\"]             |\n",
        "| **Use cases**      | Summarization, dialogue systems   | Embeddings, text classification, NER |\n",
        "‚úÖ **In short:**\n",
        "\n",
        "* **Sentence tokenization** splits text into sentences.\n",
        "* **Word tokenization** splits sentences into words/tokens.\n",
        "  Both are often used together: first break text into **sentences**, then tokenize each sentence into **words**.\n"
      ],
      "metadata": {
        "id": "nPKqH3M82fLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9 What are co-occurrence vectors in NLP ?\n",
        "## üîë What are Co-occurrence Vectors in NLP?\n",
        "\n",
        "A **co-occurrence vector** represents a word based on the **frequency of other words appearing near it (in its context window)** in a large corpus.\n",
        "\n",
        "It comes from the **distributional hypothesis**:\n",
        "\n",
        "> *‚ÄúWords that occur in similar contexts tend to have similar meanings.‚Äù*\n",
        "## ‚öôÔ∏è How Co-occurrence Vectors are Created\n",
        "\n",
        "1. **Build a Vocabulary**\n",
        "   Suppose your corpus is:\n",
        "   `\"The cat sits on the mat.\"`\n",
        "\n",
        "   Vocabulary = \\[the, cat, sits, on, mat]\n",
        "\n",
        "2. **Choose a Context Window**\n",
        "   Let‚Äôs say window size = 2 (words to the left and right).\n",
        "\n",
        "3. **Count Co-occurrences**\n",
        "\n",
        "   * For word **‚Äúcat‚Äù**, nearby words are {the, sits}.\n",
        "   * For word **‚Äúmat‚Äù**, nearby words are {the, on}.\n",
        "\n",
        "4. **Form Vectors**\n",
        "   Each word is represented by a vector counting how often each vocabulary word appears near it.\n",
        "\n",
        "   Example (simplified):\n",
        "\n",
        "   | Word | the | cat | sits | on | mat |\n",
        "   | ---- | --- | --- | ---- | -- | --- |\n",
        "   | cat  | 1   | 0   | 1    | 0  | 0   |\n",
        "   | mat  | 1   | 0   | 0    | 1  | 0   |\n",
        "\n",
        "So **‚Äúcat‚Äù** is represented as `[1,0,1,0,0]` and **‚Äúmat‚Äù** as `[1,0,0,1,0]`.\n",
        "## üìå Why It‚Äôs Useful\n",
        "\n",
        "* Captures **semantic similarity**: words appearing in similar contexts have similar vectors.\n",
        "* Forms the basis of early word representation methods (before Word2Vec, GloVe).\n",
        "* Still used in **statistical NLP**, **information retrieval**, and **topic modeling**.\n",
        "## ‚öñÔ∏è Difference from Word Embeddings\n",
        "\n",
        "* **Co-occurrence vectors** ‚Üí sparse, high-dimensional (size = vocabulary).\n",
        "* **Word embeddings (Word2Vec, GloVe)** ‚Üí dense, low-dimensional, learned using co-occurrence but compressed.\n",
        "‚úÖ **In short:**\n",
        "A co-occurrence vector represents a word by counting how often other words appear around it. It‚Äôs an early way to capture word meaning from context, forming the foundation of modern embeddings.\n"
      ],
      "metadata": {
        "id": "hzcD7Va_2fH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 What is the significance of lemmatization in improving NLP tasks ?\n",
        "\n",
        "Lemmatization is the process of reducing a word to its **base or dictionary form (lemma)**, while considering its **morphological analysis and part of speech**.\n",
        "\n",
        "Example:\n",
        "\n",
        "* *‚Äúrunning‚Äù ‚Üí ‚Äúrun‚Äù*\n",
        "* *‚Äúbetter‚Äù ‚Üí ‚Äúgood‚Äù* (uses linguistic rules, unlike stemming)\n",
        "## ‚úÖ Significance of Lemmatization in Improving NLP Tasks\n",
        "\n",
        "1. **Reduces Vocabulary Size**\n",
        "\n",
        "   * Without lemmatization, ‚Äúrun‚Äù, ‚Äúruns‚Äù, ‚Äúrunning‚Äù are treated as different words.\n",
        "   * Lemmatization maps them all to ‚Äúrun‚Äù, making models more efficient and reducing sparsity.\n",
        "\n",
        "2. **Improves Text Consistency**\n",
        "\n",
        "   * Different inflections of a word are standardized.\n",
        "   * Example: *‚Äúmice‚Äù ‚Üí ‚Äúmouse‚Äù*, *‚Äúfeet‚Äù ‚Üí ‚Äúfoot‚Äù*.\n",
        "\n",
        "3. **Enhances Information Retrieval & Search**\n",
        "\n",
        "   * Searching for ‚Äúrunning shoes‚Äù should also return results with ‚Äúrun shoes‚Äù.\n",
        "   * Lemmatization ensures better **recall** in search engines.\n",
        "\n",
        "4. **Boosts Model Accuracy**\n",
        "\n",
        "   * Sentiment analysis, classification, and topic modeling improve when words are normalized to their base forms.\n",
        "   * Prevents models from being confused by word variations.\n",
        "\n",
        "5. **Better Semantic Understanding**\n",
        "\n",
        "   * Unlike stemming (which may chop words incorrectly), lemmatization ensures linguistically correct forms.\n",
        "   * This preserves **true meaning**, important in tasks like **machine translation** or **question answering**.\n",
        "\n",
        "6. **Useful for Frequency & Co-occurrence Analysis**\n",
        "\n",
        "   * Frequency counts and co-occurrence matrices become more meaningful when similar words are grouped.\n",
        "## ‚öñÔ∏è Lemmatization vs Stemming (Quick Note)\n",
        "\n",
        "* **Stemming**: crude, rule-based chopping (e.g., *‚Äúrunning‚Äù ‚Üí ‚Äúrun‚Äù*, *‚Äústudies‚Äù ‚Üí ‚Äústudi‚Äù*).\n",
        "* **Lemmatization**: linguistically accurate (e.g., *‚Äústudies‚Äù ‚Üí ‚Äústudy‚Äù*).\n",
        "\n",
        "üëâ Lemmatization is slower but gives **higher accuracy** for downstream NLP tasks.\n",
        "‚úÖ **In short:**\n",
        "Lemmatization improves NLP tasks by **standardizing words, reducing vocabulary size, preserving meaning, and boosting accuracy** in tasks like classification, search, machine translation, and sentiment analysis.\n"
      ],
      "metadata": {
        "id": "36LQrvdU7vUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. What is the primary use of word embeddings in NLP ?\n",
        "\n",
        "Word embeddings are **dense vector representations** of words, where words with similar meanings are mapped to vectors that are close in the vector space.\n",
        "(Examples: **Word2Vec, GloVe, FastText, BERT embeddings**).\n",
        "## ‚úÖ Primary Use of Word Embeddings in NLP\n",
        "\n",
        "The **main use** of word embeddings is to **convert words into numerical representations that capture semantic meaning and relationships**, so machine learning models can process and understand text effectively\n",
        "## üìå Why They‚Äôre Important:\n",
        "\n",
        "1. **Capturing Semantic Similarity**\n",
        "\n",
        "   * Similar words (e.g., *king, queen, prince*) have embeddings that are close in vector space.\n",
        "   * Enables models to understand meaning beyond exact word matching.\n",
        "\n",
        "2. **Input Features for ML/DL Models**\n",
        "\n",
        "   * Word embeddings are used as **input features** for tasks like text classification, sentiment analysis, named entity recognition, and translation.\n",
        "\n",
        "3. **Dimensionality Reduction**\n",
        "\n",
        "   * Instead of huge sparse one-hot vectors, embeddings provide compact (e.g., 100‚Äì300D) dense vectors.\n",
        "\n",
        "4. **Improves Generalization**\n",
        "\n",
        "   * Models can recognize that *‚Äúdog‚Äù* and *‚Äúpuppy‚Äù* are related, improving performance on unseen data.\n",
        "\n",
        "5. **Facilitates Analogical Reasoning**\n",
        "\n",
        "   * Embeddings capture relationships:\n",
        "\n",
        "     $$\n",
        "     \\text{vector(\"king\")} - \\text{vector(\"man\")} + \\text{vector(\"woman\")} \\approx \\text{vector(\"queen\")}\n",
        "     $$\n",
        "## ‚úÖ Summary:\n",
        "\n",
        "The **primary use of word embeddings in NLP** is to provide **meaningful, dense numerical representations of words** that preserve semantic and syntactic relationships, making text understandable for machine learning and deep learning models.\n"
      ],
      "metadata": {
        "id": "AP-ey3lK8ejS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12 What is an annotator in NLP ?\n",
        "An **annotator** in NLP is a tool, algorithm, or sometimes a human process that **labels or enriches raw text with additional information** to make it usable for NLP tasks.\n",
        "\n",
        "Think of it as a **processor** that takes plain text and adds structured data (annotations).\n",
        "## ‚öôÔ∏è Examples of Annotation in NLP\n",
        "\n",
        "1. **Tokenization Annotator** ‚Äì splits text into words or sentences.\n",
        "\n",
        "   * Input: `\"I love NLP.\"`\n",
        "   * Output: `[\"I\", \"love\", \"NLP\", \".\"]`\n",
        "\n",
        "2. **POS (Part-of-Speech) Tagging Annotator** ‚Äì assigns grammatical categories.\n",
        "\n",
        "   * Input: `\"Dogs bark.\"`\n",
        "   * Output: `[(\"Dogs\", NOUN), (\"bark\", VERB)]`\n",
        "\n",
        "3. **Named Entity Recognition (NER) Annotator** ‚Äì detects entities like people, places, organizations.\n",
        "\n",
        "   * Input: `\"Barack Obama was born in Hawaii.\"`\n",
        "   * Output: `[(\"Barack Obama\", PERSON), (\"Hawaii\", LOCATION)]`\n",
        "\n",
        "4. **Sentiment Annotator** ‚Äì labels text with sentiment.\n",
        "\n",
        "   * Input: `\"I love this movie!\"`\n",
        "   * Output: `\"Positive\"`\n",
        "\n",
        "5. **Coreference Annotator** ‚Äì links pronouns to the nouns they refer to.\n",
        "\n",
        "   * Input: `\"Alice said she is happy.\"`\n",
        "   * Output: `[Alice ‚Üî she]`\n",
        "## üìå Why Annotators are Important\n",
        "\n",
        "* Convert **unstructured text ‚Üí structured data**.\n",
        "* Enable **training supervised models** (need labeled data).\n",
        "* Used in **NLP pipelines** (e.g., Stanford CoreNLP, spaCy, NLTK).\n",
        "‚úÖ **In short:**\n",
        "An **annotator** in NLP is a component (human or algorithmic) that **adds labels, tags, or structure** to text, such as part-of-speech tags, entities, or sentiment, making it ready for deeper analysis.\n"
      ],
      "metadata": {
        "id": "k7CneOoO8ef8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13 What are the key steps in text processing before applying machine learning models ?\n",
        "## üîë Key Steps in Text Processing Before Applying ML Models\n",
        "### 1. **Text Cleaning (Noise Removal)**\n",
        "\n",
        "* Remove unwanted characters, HTML tags, URLs, numbers (if not needed).\n",
        "* Example: `\"Visit https://abc.com!!!\"` ‚Üí `\"Visit\"\n",
        "### 2. **Text Normalization**\n",
        "\n",
        "* **Lowercasing** ‚Üí `\"Apple\"` ‚Üí `\"apple\"`\n",
        "* **Expanding contractions** ‚Üí `\"don't\"` ‚Üí `\"do not\"`\n",
        "* **Handling spelling variations** ‚Üí `\"colour\"` ‚Üí `\"color\"\n",
        "### 3. **Tokenization**\n",
        "\n",
        "* Breaking text into **sentences** or **words**.\n",
        "* Example: `\"I love NLP\"` ‚Üí `[\"I\", \"love\", \"NLP\"]`\n",
        "### 4. **Stopword Removal**\n",
        "\n",
        "* Removing frequent but uninformative words like *‚Äúthe, is, and‚Äù*.\n",
        "* Keeps only meaningful tokens.\n",
        "### 5. **Stemming or Lemmatization**\n",
        "\n",
        "* **Stemming** ‚Üí crude chopping (e.g., `\"studies\"` ‚Üí `\"studi\"`)\n",
        "* **Lemmatization** ‚Üí dictionary form (e.g., `\"studies\"` ‚Üí `\"study\"`)\n",
        "* Helps reduce vocabulary size.\n",
        "### 6. **Feature Extraction (Vectorization)**\n",
        "\n",
        "Since ML models work on numbers, we must convert words into vectors:\n",
        "\n",
        "* **Bag of Words (BoW)** ‚Üí counts word occurrences.\n",
        "* **TF-IDF** ‚Üí gives importance to less frequent but more informative words.\n",
        "* **Word Embeddings (Word2Vec, GloVe, FastText, BERT embeddings)** ‚Üí captures meaning and context.\n",
        "### 7. **Handling Class Imbalance / Rare Words**\n",
        "\n",
        "* Remove very rare words (reduce noise).\n",
        "* Handle class imbalance if working on classification.\n",
        "### 8. **Final Preprocessing for Models**\n",
        "\n",
        "* Padding/truncating sequences (for deep learning models like LSTMs, Transformers).\n",
        "* Train-test split and vectorization.\n",
        "## ‚úÖ Summary\n",
        "\n",
        "Before applying ML models on text, the **essential text processing pipeline** usually includes:\n",
        "**Cleaning ‚Üí Normalization ‚Üí Tokenization ‚Üí Stopword Removal ‚Üí Lemmatization/Stemming ‚Üí Vectorization (BoW, TF-IDF, embeddings).**\n",
        "\n",
        "This ensures the model gets **clean, consistent, and informative numerical input**."
      ],
      "metadata": {
        "id": "Q4pUsarO8edT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14 What is the history of NLP and how has it evolved ?\n",
        "Awesome question üôå ‚Äî the history of **Natural Language Processing (NLP)** is fascinating because it shows how we went from simple rule-based systems to today‚Äôs **transformer-based AI models like GPT**.\n",
        "## üìú **History & Evolution of NLP**\n",
        "\n",
        "### **1. 1950s‚Äì1960s: Early Days (Rule-Based & Symbolic NLP)**\n",
        "\n",
        "* **Alan Turing (1950)** proposed the **Turing Test** as a measure of machine intelligence.\n",
        "* Early NLP systems were **rule-based** and relied on grammar rules and symbolic logic.\n",
        "* **Machine Translation (MT)** research started ‚Äî first attempts to translate Russian ‚Üî English.\n",
        "* Famous system: **ELIZA (1966)** ‚Äì an early chatbot simulating a psychotherapist using pattern matching.\n",
        "### **2. 1970s‚Äì1980s: Linguistics + Statistical Beginnings**\n",
        "\n",
        "* Systems still mostly rule-based, but linguistics theories (like **Chomsky‚Äôs grammar**) influenced NLP.\n",
        "* **SHRDLU (1970s)** could understand simple English commands in a blocks world.\n",
        "* Start of **knowledge-based systems** (expert systems).\n",
        "* **Late 1980s**: Shift toward **statistical NLP** using probability and data-driven methods, as more digital text became available.\n",
        "### **3. 1990s: Statistical NLP & Machine Learning**\n",
        "\n",
        "* Explosion of **large corpora (e.g., Penn Treebank)** enabled data-driven NLP.\n",
        "* Use of **Hidden Markov Models (HMMs)** for speech recognition and part-of-speech tagging.\n",
        "* **N-gram language models** became popular for text prediction.\n",
        "* Transition from rules ‚Üí **probabilistic and machine learning methods**.\n",
        "### **4. 2000s: Feature-Based Machine Learning**\n",
        "\n",
        "* NLP moved to **supervised ML approaches** (SVMs, logistic regression, CRFs).\n",
        "* Tasks like **POS tagging, named entity recognition (NER), sentiment analysis** improved.\n",
        "* **Bag of Words & TF-IDF** were common text representations.\n",
        "* Still limited in capturing deep meaning (no context in word vectors).\n",
        "### **5. 2010s: Deep Learning Revolution**\n",
        "\n",
        "* Introduction of **word embeddings** like **Word2Vec (2013)**, **GloVe (2014)** ‚Äî captured semantic similarity in dense vectors.\n",
        "* **RNNs, LSTMs, GRUs** used for sequence modeling (e.g., translation, sentiment).\n",
        "* **Attention mechanism (2014)** improved handling of long sequences.\n",
        "* **Seq2Seq models** (Encoder-Decoder) enabled better machine translation.\n",
        "### **6. 2017‚ÄìPresent: Transformer Era & Large Language Models**\n",
        "\n",
        "* **Transformer architecture (2017, Vaswani et al. ‚ÄúAttention is All You Need‚Äù)** revolutionized NLP.\n",
        "* Self-attention allowed parallel processing and long-range context understanding.\n",
        "* Pretrained models on massive corpora:\n",
        "\n",
        "  * **BERT (2018)** ‚Äì bidirectional contextual embeddings.\n",
        "  * **GPT (2018‚Äì2023, GPT-1 ‚Üí GPT-4)** ‚Äì autoregressive large language models.\n",
        "  * **T5, XLNet, RoBERTa, LLaMA, etc.**\n",
        "* Emergence of **LLMs (Large Language Models)** like **ChatGPT** that perform multi-task NLP without task-specific training.\n",
        "## ‚úÖ **Summary of Evolution**\n",
        "\n",
        "1. **1950s‚Äì70s** ‚Üí Rule-based, symbolic systems.\n",
        "2. **1980s‚Äì90s** ‚Üí Statistical NLP (probability, corpora).\n",
        "3. **2000s** ‚Üí Machine learning with handcrafted features.\n",
        "4. **2010s** ‚Üí Deep learning (RNNs, embeddings).\n",
        "5. **2017‚ÄìNow** ‚Üí Transformers & LLMs (contextual embeddings, generative AI).\n"
      ],
      "metadata": {
        "id": "H8-3a4m0KnRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15 Why is sentence processing important in NLP ?\n",
        "Sentence processing means analyzing sentences to understand their **structure, meaning, and relationships**.\n",
        "It goes beyond just words ‚Üí it looks at how words combine to form **coherent meaning**.\n",
        "## ‚úÖ Why Sentence Processing is Important in NLP\n",
        "\n",
        "1. **Captures Complete Meaning (Contextual Understanding)**\n",
        "\n",
        "   * A single word often has multiple meanings.\n",
        "   * Example: `\"bank\"` could mean *river bank* or *financial bank*.\n",
        "   * Sentence-level processing uses surrounding words to resolve ambiguity.\n",
        "2. **Essential for Syntax and Grammar Analysis**\n",
        "\n",
        "   * Understanding how words are ordered (syntax) helps identify roles like subject, verb, object.\n",
        "   * Example:\n",
        "\n",
        "     * `\"The dog chased the cat.\"` (dog = subject, cat = object)\n",
        "     * `\"The cat chased the dog.\"` (cat = subject, dog = object)\n",
        "3. **Improves Higher-Level NLP Tasks**\n",
        "\n",
        "   * Many tasks require sentence-level meaning, not just word-level:\n",
        "\n",
        "     * **Machine Translation** ‚Üí needs to preserve sentence meaning.\n",
        "     * **Summarization** ‚Üí extracts or generates full sentences.\n",
        "     * **Question Answering** ‚Üí requires understanding sentence context.\n",
        "     * **Chatbots** ‚Üí respond at the sentence level.\n",
        "4. **Handles Coreference & Dependencies**\n",
        "\n",
        "   * Sentence processing helps link pronouns to nouns.\n",
        "   * Example: `\"Alice said she is happy.\"` ‚Üí (she = Alice).\n",
        "   * Also captures **dependencies** (who did what to whom).\n",
        "5. **Improves Semantic Representation**\n",
        "\n",
        "   * Word embeddings (like Word2Vec) capture word meaning, but sentence embeddings (like Sentence-BERT) capture **meaning of whole sentences**, which is more useful in retrieval, similarity search, and clustering.\n",
        "## ‚úÖ Summary\n",
        "\n",
        "Sentence processing is important in NLP because it:\n",
        "\n",
        "* Ensures **contextual meaning**,\n",
        "* Resolves **ambiguity**,\n",
        "* Supports **syntactic and semantic analysis**,\n",
        "* Enables **downstream tasks** (translation, summarization, QA, dialogue).\n",
        "\n",
        "üëâ Basically, **without sentence-level understanding, NLP would only ‚Äúsee words,‚Äù not true meaning.**"
      ],
      "metadata": {
        "id": "rIKM2VzcLV1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 16 How do word embeddings improve the understanding of language semantics in NLP ?\n",
        "Excellent question üôå ‚Äî this gets to the **heart of why modern NLP works so well**.\n",
        "## üîë Quick Recap: What Are Word Embeddings?\n",
        "\n",
        "Word embeddings are **dense vector representations of words** in a continuous vector space, where words with similar meanings are placed closer together.\n",
        "(Examples: **Word2Vec, GloVe, FastText, BERT embeddings**).\n",
        "## ‚úÖ How Word Embeddings Improve Semantic Understanding\n",
        "\n",
        "### 1. **Capture Semantic Similarity**\n",
        "\n",
        "* In one-hot encoding, `\"cat\"` and `\"dog\"` are completely unrelated.\n",
        "* In embeddings, `\"cat\"` and `\"dog\"` are close because they often occur in similar contexts.\n",
        "* This reflects the **distributional hypothesis**: *‚ÄúWords used in similar contexts have similar meanings.‚Äù*\n",
        "### 2. **Represent Contextual Relationships**\n",
        "\n",
        "* Traditional embeddings (Word2Vec, GloVe) capture **global semantic relationships**.\n",
        "* Contextual embeddings (BERT, GPT) adjust a word‚Äôs meaning depending on the **sentence**.\n",
        "\n",
        "  * Example:\n",
        "\n",
        "    * `\"He went to the **bank** to deposit money.\"` ‚Üí (finance)\n",
        "    * `\"He sat on the **bank** of the river.\"` ‚Üí (geography)\n",
        "### 3. **Enable Analogical Reasoning**\n",
        "\n",
        "* Embeddings preserve **vector arithmetic properties**:\n",
        "\n",
        "  $$\n",
        "  \\text{vector(\"King\")} - \\text{vector(\"Man\")} + \\text{vector(\"Woman\")} \\approx \\text{vector(\"Queen\")}\n",
        "  $$\n",
        "* This shows embeddings capture deeper **semantic and syntactic patterns**.\n",
        "### 4. **Reduce Sparsity & Dimensionality**\n",
        "\n",
        "* One-hot vectors are high-dimensional and sparse (size = vocabulary).\n",
        "* Embeddings compress meaning into 100‚Äì300 dimensions, making models more efficient **and semantically richer**.\n",
        "### 5. **Transfer Learning & Generalization**\n",
        "\n",
        "* Pretrained embeddings (Word2Vec, GloVe, BERT) bring **prior knowledge** of language semantics into downstream tasks.\n",
        "* Helps models generalize better, even with limited labeled data.\n",
        "## ‚úÖ Summary\n",
        "\n",
        "Word embeddings improve semantic understanding in NLP by:\n",
        "\n",
        "* Placing semantically similar words close in vector space,\n",
        "* Capturing **context-dependent meanings**,\n",
        "* Preserving **relationships** between words,\n",
        "* Making models more efficient and generalizable.\n",
        "\n",
        "üëâ Without embeddings, models would only treat words as arbitrary symbols; **with embeddings, they capture meaning, similarity, and relationships.*"
      ],
      "metadata": {
        "id": "wi-bbIf2MPf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17  How does the frequency distribution of words help in text classification ?\n",
        "Great question üëç ‚Äî let‚Äôs connect **frequency distribution** directly to **text classification**.\n",
        "## üîë What is Frequency Distribution of Words?\n",
        "It‚Äôs the count of how often each word appears in a text or across a dataset.\n",
        "Example (sentence: *‚ÄúThe cat sat on the mat‚Äù*):\n",
        "* the ‚Üí 2\n",
        "* cat ‚Üí 1\n",
        "* sat ‚Üí 1\n",
        "* on ‚Üí 1\n",
        "* mat ‚Üí 1\n",
        "## ‚úÖ How It Helps in Text Classification\n",
        "\n",
        "### 1. **Feature Representation (Bag of Words / TF-IDF)**\n",
        "\n",
        "* Word frequencies form the basis of numerical features for ML models.\n",
        "* Example: For **spam detection**, words like *‚Äúfree, win, offer‚Äù* occur more frequently in spam than in normal text.\n",
        "* Bag of Words (BoW) or TF-IDF vectors use frequency counts to build feature matrices.\n",
        "### 2. **Identify Discriminative Words**\n",
        "\n",
        "* Frequent words that differ across categories help classifiers distinguish classes.\n",
        "* Example: In **movie reviews**:\n",
        "\n",
        "  * Positive: frequent words ‚Üí *‚Äúamazing, love, great‚Äù*\n",
        "  * Negative: frequent words ‚Üí *‚Äúboring, bad, waste‚Äù*\n",
        "### 3. **Reduce Noise with Stopwords**\n",
        "\n",
        "* Very frequent but non-informative words (*‚Äúthe, is, and‚Äù*) can be identified and removed.\n",
        "* Improves classification accuracy by focusing on **content words**.\n",
        "### 4. **Feature Selection / Dimensionality Reduction**\n",
        "\n",
        "* Frequency distribution helps drop **rare words** that add little value.\n",
        "* Keeps the vocabulary meaningful while reducing computational cost.\n",
        "### 5. **Class-Specific Profiling**\n",
        "\n",
        "* By comparing frequency distributions across labels (e.g., spam vs ham, positive vs negative), we can profile which words are strong indicators for each class.\n",
        "## üìå Example (Spam vs Ham Email Classification)\n",
        "\n",
        "* Spam emails: high frequency of *‚Äúoffer, free, win, prize, click‚Äù*\n",
        "* Ham emails: high frequency of *‚Äúmeeting, project, report, schedule‚Äù*\n",
        "\n",
        "üëâ By looking at frequency distributions, a classifier learns which words are predictive of spam vs ham.\n",
        "## ‚úÖ Summary\n",
        "\n",
        "The **frequency distribution of words** helps in text classification by:\n",
        "\n",
        "* Converting text into numerical features (BoW/TF-IDF),\n",
        "* Highlighting discriminative words,\n",
        "* Removing uninformative stopwords,\n",
        "* Reducing dimensionality,\n",
        "* Enabling better class separation.\n",
        "\n",
        "üëâ In short: **frequency counts are the foundation of feature engineering for traditional text classifiers.**"
      ],
      "metadata": {
        "id": "8m830xI7MPbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 18 What are the advantages of using regex in text cleaning ?\n",
        "Great question üôå ‚Äî **regular expressions (regex)** are one of the most powerful tools for text preprocessing in NLP.\n",
        "## üîë Advantages of Using Regex in Text Cleaning\n",
        "\n",
        "### 1. **Powerful Pattern Matching**\n",
        "\n",
        "* Regex can detect **complex text patterns** (emails, phone numbers, dates, hashtags, URLs, etc.) in a single line of code.\n",
        "* Example: `\\d{4}-\\d{2}-\\d{2}` ‚Üí matches dates like `2025-08-31`\n",
        "### 2. **Efficiency & Speed**\n",
        "\n",
        "* Regex is highly optimized for **fast string matching and substitution**, even on large corpora.\n",
        "* Instead of writing multiple if-else conditions, a compact regex can handle it in one go.\n",
        "### 3. **Flexibility**\n",
        "\n",
        "* Can clean **varied noise**: extra spaces, special characters, punctuation, HTML tags, non-alphanumeric symbols.\n",
        "* Example: `re.sub(r'[^a-zA-Z0-9 ]', '', text)` ‚Üí removes everything except letters, digits, spaces.\n",
        "### 4. **Conciseness**\n",
        "\n",
        "* One regex can replace **dozens of lines** of manual text cleaning code.\n",
        "* Example:\n",
        "\n",
        "  * Remove multiple spaces: `re.sub(r'\\s+', ' ', text)`\n",
        "### 5. **Custom Cleaning Rules**\n",
        "\n",
        "* Regex allows **fine-grained control** depending on domain (e.g., cleaning tweets, medical texts, logs).\n",
        "* Example: extract hashtags from tweets ‚Üí `r'#\\w+'`.\n",
        "### 6. **Reusability**\n",
        "\n",
        "* Once you write a regex pattern (say for emails or URLs), it can be reused across projects and datasets.\n",
        "## ‚úÖ Example in Practice\n",
        "\n",
        "```python\n",
        "import re\n",
        "\n",
        "text = \"Contact me at test123@example.com!!   Visit: https://abc.com  \"\n",
        "\n",
        "# Remove email\n",
        "text = re.sub(r'\\S+@\\S+', '', text)\n",
        "# Remove URL\n",
        "text = re.sub(r'http\\S+', '', text)\n",
        "# Remove extra spaces\n",
        "text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "print(text)\n",
        "# Output: \"Contact me at !! Visit:\"\n",
        "```\n",
        "## üìå Summary\n",
        "\n",
        "Using regex in text cleaning is advantageous because it is:\n",
        "\n",
        "* **Powerful** (captures complex patterns)\n",
        "* **Fast & efficient** (optimized search/replace)\n",
        "* **Flexible** (handles varied cleaning tasks)\n",
        "* **Concise** (saves lines of code)\n",
        "* **Reusable** across datasets\n",
        "\n",
        "üëâ In short: Regex is a **must-have tool** in NLP preprocessing pipelines for cleaning and normalizing text.\n"
      ],
      "metadata": {
        "id": "GHc2LAuAMPWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 19 What is the difference between word2vec and doc2vec ?\n",
        "Great question üëç ‚Äî **Word2Vec** and **Doc2Vec** are closely related, but they serve different purposes in NLP. Let‚Äôs break it down.\n",
        "## üîë **Word2Vec**\n",
        "\n",
        "* **Goal**: Represent **words** as dense vectors.\n",
        "* **How**: Learns embeddings by predicting context words (Skip-gram) or target words (CBOW).\n",
        "* **Output**: A vector for **each word** in the vocabulary.\n",
        "* **Example**:\n",
        "\n",
        "  * `\"king\" ‚Äì \"man\" + \"woman\" ‚âà \"queen\"`\n",
        "  * Useful for semantic similarity between words.\n",
        "## üîë **Doc2Vec**\n",
        "\n",
        "* **Goal**: Represent **larger text units** (sentences, paragraphs, or documents) as dense vectors.\n",
        "* **How**: Extends Word2Vec by adding a **document ID (paragraph vector)** during training, so the model learns a unique vector for each document in addition to word embeddings.\n",
        "* **Output**: A vector for **each document**, not just words.\n",
        "* **Example**:\n",
        "\n",
        "  * Represent a full **movie review** as a vector for sentiment classification.\n",
        "  * Useful for document clustering, retrieval, and classification.\n",
        "## ‚öñÔ∏è **Key Differences**\n",
        "\n",
        "| Feature               | Word2Vec üìù (Word-level)                                            | Doc2Vec üìÑ (Document-level)                                 |\n",
        "| --------------------- | ------------------------------------------------------------------- | ----------------------------------------------------------- |\n",
        "| **Unit of embedding** | Word                                                                | Sentence / Paragraph / Document                             |\n",
        "| **Training Input**    | Word + context window                                               | Word + context window + document ID                         |\n",
        "| **Output**            | Word vectors                                                        | Document vectors (plus word vectors)                        |\n",
        "| **Use cases**         | Synonym detection, analogy tasks, semantic similarity between words | Document classification, clustering, recommendation systems |\n",
        "## ‚úÖ Summary\n",
        "\n",
        "* **Word2Vec** ‚Üí captures **relationships between words**.\n",
        "* **Doc2Vec** ‚Üí captures **semantic meaning of entire documents**.\n",
        "\n",
        "üëâ Think of **Word2Vec as \"meaning of a word\"** and **Doc2Vec as \"meaning of a whole text\"**.\n"
      ],
      "metadata": {
        "id": "OvU_T0wrPILB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 20 Why is understanding text normalization important in NLP ?\n",
        "Excellent question üôå ‚Äî this one really cuts to the **foundation of NLP preprocessing**.\n",
        "## üîë What is Text Normalization?\n",
        "\n",
        "Text normalization is the process of converting raw text into a **standardized and consistent format** so that NLP models can interpret it correctly.\n",
        "It includes tasks like:\n",
        "\n",
        "* Lowercasing (`\"Apple\"` ‚Üí `\"apple\"`)\n",
        "* Removing punctuation (`\"hello!!!\"` ‚Üí `\"hello\"`)\n",
        "* Expanding contractions (`\"don‚Äôt\"` ‚Üí `\"do not\"`)\n",
        "* Lemmatization/Stemming (`\"running\"` ‚Üí `\"run\"`)\n",
        "* Handling spelling variations (`\"colour\"` ‚Üí `\"color\"`)\n",
        "## ‚úÖ Why Understanding Text Normalization is Important in NLP\n",
        "\n",
        "### 1. **Consistency in Text Representation**\n",
        "\n",
        "* Natural language has many variations. Without normalization, `\"Run\"`, `\"RUN\"`, `\"running\"` are treated as different tokens.\n",
        "* Normalization groups them ‚Üí reduces confusion for models\n",
        "### 2. **Reduces Vocabulary Size (Dimensionality)**\n",
        "\n",
        "* Without normalization: vocabulary = {run, runs, running, ran}.\n",
        "* With normalization: vocabulary = {run}.\n",
        "* A smaller, cleaner vocabulary makes ML/DL models **faster and more accurate**.\n",
        "### 3. **Improves Model Accuracy**\n",
        "\n",
        "* Raw text is noisy (typos, casing, slang).\n",
        "* Normalization ensures similar words are treated as the same, improving downstream tasks like **sentiment analysis, classification, machine translation**\n",
        "### 4. **Better Frequency & Feature Extraction**\n",
        "\n",
        "* Word frequency and co-occurrence analysis are more meaningful when similar forms are merged.\n",
        "* Example: `\"study\", \"studies\", \"studying\"` all contribute to the same concept after normalization\n",
        "### 5. **Critical for Search & Retrieval**\n",
        "\n",
        "* Search engines must match `\"USA\"`, `\"U.S.A.\"`, `\"us\"`.\n",
        "* Normalization improves **recall and precision** in information retrieval systems.\n",
        "### 6. **Domain-Specific Adaptation**\n",
        "\n",
        "* In medical, legal, or social media text, normalization rules may differ.\n",
        "* Example: `\"BP\"` ‚Üí `\"blood pressure\"` in medical context.\n",
        "* Understanding normalization ensures correct domain-specific interpretation.\n",
        "## ‚úÖ Summary\n",
        "\n",
        "Understanding text normalization is important because it:\n",
        "\n",
        "* Ensures **consistency** in text,\n",
        "* Reduces **noise and dimensionality**,\n",
        "* Improves **accuracy** of NLP models,\n",
        "* Makes features more **meaningful**,\n",
        "* Enables better **retrieval, search, and analysis**.\n",
        "\n",
        "üëâ In short: **Without normalization, NLP models waste effort learning noise; with normalization, they focus on meaning.**\n"
      ],
      "metadata": {
        "id": "kEhJcP-xPIG-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}