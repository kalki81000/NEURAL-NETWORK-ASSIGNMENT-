{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+oVjV+3hrPfllXaXHA19u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalki81000/NEURAL-NETWORK-ASSIGNMENT-/blob/main/Untitled118.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Learning Frameworks\n",
        "\n"
      ],
      "metadata": {
        "id": "b_iEMLxMAhyZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bGUAAhgAGgy"
      },
      "outputs": [],
      "source": [
        "1.#  What is TensorFlow 2.0, and how is it different from TensorFlow 1.x ?\n",
        "**TensorFlow 2.0** is a major upgrade of Google’s open-source deep learning framework, released in September 2019, designed to make model building easier, more intuitive, and more flexible than **TensorFlow 1.x**.\n",
        "It focuses on simplicity, ease of use, and integration with modern Python programming practices.\n",
        "## **Key Differences between TensorFlow 2.0 and TensorFlow 1.x**\n",
        "\n",
        "| Feature / Aspect         | TensorFlow 1.x                                                                                                           | TensorFlow 2.0                                                                                                                              |\n",
        "| ------------------------ | ------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Execution Mode**       | *Default*: **Graph Execution** (define-and-run) — you first define a static computation graph, then run it in a session. | *Default*: **Eager Execution** (define-by-run) — operations execute immediately as you write them, making debugging and prototyping easier. |\n",
        "| **Ease of Use**          | More verbose; requires sessions, placeholders, and manual graph management.                                              | Simplified, Pythonic API — no need for `tf.Session()` or `tf.placeholder()`.                                                                |\n",
        "| **Keras Integration**    | Keras was separate; TF had its own lower-level API.                                                                      | **`tf.keras`** is the default high-level API for building and training models.                                                              |\n",
        "| **API Simplification**   | Many redundant APIs; confusing for beginners.                                                                            | Cleaned-up API — deprecated unused functions, consolidated many methods.                                                                    |\n",
        "| **Control Flow**         | Uses `tf.while_loop`, `tf.cond` — graph-based and less intuitive.                                                        | Native Python control flow works directly with eager execution.                                                                             |\n",
        "| **Model Building**       | Sequential / functional APIs not tightly integrated; Estimators heavily used.                                            | Encourages **Keras Sequential, Functional, and subclassing APIs** for model creation.                                                       |\n",
        "| **Distributed Training** | Complex setup using `tf.distribute` in 1.x.                                                                              | Unified, simpler distributed training API (`tf.distribute.Strategy`).                                                                       |\n",
        "| **Compatibility**        | Older syntax; upgrading requires significant code changes.                                                               | Has a **`tf.compat.v1`** module to help run old 1.x code.                                                                                   |\n",
        "| **Default Behavior**     | Static computation graph by default.                                                                                     | Dynamic execution by default.                                                                                                               |\n",
        "\n",
        "### **Example: Code Comparison**\n",
        "\n",
        "**TensorFlow 1.x**\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "x = tf.placeholder(tf.float32)\n",
        "y = tf.placeholder(tf.float32)\n",
        "z = x + y\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    result = sess.run(z, feed_dict={x: 3, y: 4})\n",
        "    print(result)\n",
        "```\n",
        "\n",
        "**TensorFlow 2.0**\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "x = tf.constant(3.0)\n",
        "y = tf.constant(4.0)\n",
        "z = x + y\n",
        "\n",
        "print(z.numpy())\n",
        "```\n",
        "\n",
        "No sessions, no placeholders — just Python code that runs immediately."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.How do you install TensorFlow 2.02\n",
        "Installing **TensorFlow 2.0** is straightforward — it’s done through `pip` (Python’s package manager).\n",
        "## **1. Basic Installation (CPU Version)**\n",
        "\n",
        "If you just want TensorFlow 2 (CPU-only, lighter installation):\n",
        "\n",
        "```bash\n",
        "pip install tensorflow==2.0.0\n",
        "```\n",
        "## **2. GPU Version (with CUDA support)**\n",
        "\n",
        "If your system has a compatible NVIDIA GPU and you’ve installed the required CUDA and cuDNN versions:\n",
        "\n",
        "```bash\n",
        "pip install tensorflow-gpu==2.0.0\n",
        "```\n",
        "\n",
        "> In TensorFlow versions after 2.1, `tensorflow` automatically includes GPU support if CUDA is present — no separate `tensorflow-gpu` package is needed. But for **exactly 2.0**, you must use `tensorflow-gpu` for GPU.\n",
        "\n",
        "## **3. Verify Installation**\n",
        "\n",
        "After installation, check the version:\n",
        "\n",
        "```bash\n",
        "python -c \"import tensorflow as tf; print(tf.__version__)\"\n",
        "```\n",
        "\n",
        "It should output:\n",
        "\n",
        "```\n",
        "2.0.0\n",
        "```\n",
        "## **4. Recommended: Virtual Environment**\n",
        "\n",
        "To avoid conflicts with other Python packages:\n",
        "\n",
        "```bash\n",
        "# Create a new environment\n",
        "python -m venv tf_env\n",
        "\n",
        "# Activate the environment\n",
        "# On Windows:\n",
        "tf_env\\Scripts\\activate\n",
        "# On Mac/Linux:\n",
        "source tf_env/bin/activate\n",
        "\n",
        "# Install TensorFlow 2.0\n",
        "pip install tensorflow==2.0.0\n",
        "```"
      ],
      "metadata": {
        "id": "tDSd4Gh7BtLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. What is the primary function of the tf.function in TensorFlow 2.02\n",
        "In **TensorFlow 2.0.2**, the **primary function** of `tf.function` is to **convert a regular Python function into a TensorFlow computation graph** for **faster execution and optimizations**.\n",
        "### **Why it matters**\n",
        "\n",
        "* TensorFlow 2 runs in **eager execution mode** by default (operations run immediately).\n",
        "* While eager execution is great for debugging, it’s slower than graph execution.\n",
        "* `tf.function` lets you keep the easy-to-read Python style **but** execute as an optimized graph under the hood.\n",
        "### **How it works**\n",
        "\n",
        "When you decorate a function with `@tf.function`:\n",
        "\n",
        "1. TensorFlow **traces** the Python code the first time it’s called.\n",
        "2. It **builds a computation graph** from that code.\n",
        "3. On subsequent calls, it runs the **compiled graph** directly instead of reinterpreting Python code each time.\n",
        "### **Example**\n",
        "\n",
        "**Without `tf.function` (Eager Execution)**\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "print(add(tf.constant(3), tf.constant(4)))  # Runs eagerly\n",
        "```\n",
        "\n",
        "**With `tf.function` (Graph Execution)**\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "@tf.function\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "print(add(tf.constant(3), tf.constant(4)))  # Runs as a compiled graph\n",
        "```\n",
        "### **Key Benefits**\n",
        "\n",
        "* **Performance**: Graph execution can be much faster for repeated operations.\n",
        "* **Optimizations**: TensorFlow can apply advanced optimizations like operation fusion.\n",
        "* **Deployment**: Graphs can be saved and run on different devices/environments.\n",
        "💡 **In short:**\n",
        "`tf.function` is TensorFlow 2.0.2’s **bridge between eager execution and graph mode**, letting you write code naturally while still getting the **speed of static graphs**"
      ],
      "metadata": {
        "id": "CmlFB5sBEoCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4 What is the purpose of the Model class in TensorFlow 2.40\n",
        "In **TensorFlow 2.0.2**, the **`Model` class** (from `tf.keras.Model`) is the **core class for building, training, evaluating, and saving neural networks**.\n",
        "\n",
        "It’s the backbone of the **Keras API** inside TensorFlow, and all models — whether `Sequential`, functional, or subclassed — are instances of this class (directly or indirectly).\n",
        "## **Purpose of the `Model` class**\n",
        "\n",
        "1. **Encapsulate a neural network’s architecture**\n",
        "\n",
        "   * Stores the network’s layers, inputs, and outputs.\n",
        "   * Handles forward propagation (`call` method).\n",
        "\n",
        "2. **Provide training and evaluation utilities**\n",
        "\n",
        "   * `model.compile(...)` — set loss, optimizer, and metrics.\n",
        "   * `model.fit(...)` — train the model.\n",
        "   * `model.evaluate(...)` — test the model.\n",
        "   * `model.predict(...)` — run inference.\n",
        "\n",
        "3. **Manage model state**\n",
        "\n",
        "   * Tracks weights and parameters.\n",
        "   * Saves/loads the model (`model.save`, `tf.keras.models.load_model`).\n",
        "\n",
        "4. **Support subclassing for custom behavior**\n",
        "\n",
        "   * You can create your own model by subclassing `tf.keras.Model` and writing a custom `call()` method.\n",
        "## **Example: Using `Model` via the Functional API**\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "# Define layers\n",
        "inputs = tf.keras.Input(shape=(28, 28))\n",
        "x = layers.Flatten()(inputs)\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "outputs = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "# Create Model\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile and train\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model.fit(x_train, y_train, epochs=5)  # Example training call\n",
        "```\n",
        "## **Example: Subclassing the `Model` class**\n",
        "\n",
        "```python\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.flatten = layers.Flatten()\n",
        "        self.dense1 = layers.Dense(128, activation='relu')\n",
        "        self.dense2 = layers.Dense(10, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.flatten(inputs)\n",
        "        x = self.dense1(x)\n",
        "        return self.dense2(x)\n",
        "\n",
        "model = MyModel()\n",
        "```\n",
        "✅ **In short:**\n",
        "The `Model` class in TensorFlow 2.0.2 is the **central object that represents your neural network** — it stores layers, runs forward passes, manages training, and handles saving/loading."
      ],
      "metadata": {
        "id": "Q6znOC0rGWv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5 How do you create a neural network using TensorFlow 2.0 ?\n",
        "In **TensorFlow 2.0.2**, you can create a neural network in **three main ways**:\n",
        "\n",
        "1. **Sequential API** — Best for simple, layer-by-layer models.\n",
        "2. **Functional API** — For complex architectures like multi-input/output or skip connections.\n",
        "3. **Subclassing `tf.keras.Model`** — For full customization.\n",
        "## **1️⃣ Sequential API (Simple, Linear Stack)**\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(128, activation='relu', input_shape=(784,)),  # Input layer\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train (example with dummy data)\n",
        "import numpy as np\n",
        "x_train = np.random.random((1000, 784))\n",
        "y_train = np.random.randint(10, size=(1000,))\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
        "```\n",
        "## **2️⃣ Functional API (More Flexible)**\n",
        "\n",
        "```python\n",
        "from tensorflow.keras import Input, Model\n",
        "\n",
        "# Input layer\n",
        "inputs = Input(shape=(784,))\n",
        "x = layers.Dense(128, activation='relu')(inputs)\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "outputs = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "# Create model\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "## **3️⃣ Subclassing `tf.keras.Model` (Custom Behavior)**\n",
        "\n",
        "```python\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.d1 = layers.Dense(128, activation='relu')\n",
        "        self.d2 = layers.Dense(64, activation='relu')\n",
        "        self.d3 = layers.Dense(10, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.d1(x)\n",
        "        x = self.d2(x)\n",
        "        return self.d3(x)\n",
        "\n",
        "# Create and compile\n",
        "model = MyModel()\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "### **Steps for all approaches**\n",
        "\n",
        "1. **Define the model** (Sequential, Functional, or Subclassing).\n",
        "2. **Compile** — choose optimizer, loss, and metrics.\n",
        "3. **Fit** — train with `model.fit()`.\n",
        "4. **Evaluate** — test with `model.evaluate()`.\n",
        "5. **Predict** — use `model.predict()` for inference.\n"
      ],
      "metadata": {
        "id": "ZEba8eSgHsWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6 What is the importance of Tensor Space in TensorFlow2\n",
        "In **TensorFlow 2**, the concept of a **Tensor space** is important because it defines the “mathematical universe” in which all data and computations in TensorFlow happen.\n",
        "\n",
        "Think of it as the **coordinate system + dimensions + rules** that govern how TensorFlow stores, manipulates, and transforms data.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. What is a Tensor Space?**\n",
        "\n",
        "A **tensor** in TensorFlow is a **multi-dimensional array** (like a generalization of scalars, vectors, and matrices).\n",
        "The **tensor space** refers to:\n",
        "\n",
        "* All possible tensors of a given **shape** (dimensions)\n",
        "* With a specific **data type** (dtype)\n",
        "* Living on a specific **device** (CPU, GPU, TPU)\n",
        "\n",
        "Example:\n",
        "\n",
        "* **Scalar space** → shape `()` (0D tensor)\n",
        "* **Vector space** → shape `(n,)` (1D tensor)\n",
        "* **Matrix space** → shape `(m, n)` (2D tensor)\n",
        "* **Higher-order tensor space** → shape `(d1, d2, ..., dn)`\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Why It’s Important in TensorFlow 2**\n",
        "\n",
        "1. **Foundation of Computation**\n",
        "\n",
        "   * All inputs, outputs, and parameters in a TensorFlow model live in a tensor space.\n",
        "   * Even model weights are tensors inside a defined space.\n",
        "\n",
        "2. **Shape Consistency**\n",
        "\n",
        "   * TensorFlow enforces shape compatibility during operations.\n",
        "   * Example: Adding two tensors requires them to be in the same **space** (shape + dtype).\n",
        "\n",
        "3. **Performance Optimization**\n",
        "\n",
        "   * The tensor space determines how data is laid out in memory, which affects speed.\n",
        "   * TensorFlow can optimize operations when tensor spaces are known.\n",
        "\n",
        "4. **Device Placement**\n",
        "\n",
        "   * The tensor space also includes the **device** location (e.g., `CPU:0`, `GPU:0`).\n",
        "   * This helps TensorFlow schedule operations efficiently.\n",
        "\n",
        "5. **Mathematical Abstraction**\n",
        "\n",
        "   * Neural networks are built on linear algebra, and tensor spaces are the “vector spaces” in which these transformations happen.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Example**\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Create tensors in a specific space\n",
        "t1 = tf.constant([1, 2, 3], dtype=tf.float32)  # 1D space, float32\n",
        "t2 = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)  # 2D space\n",
        "\n",
        "# Operations happen within compatible tensor spaces\n",
        "result = tf.matmul(t2, tf.reshape(t1[:2], (2, 1)))  # Matmul in 2D space\n",
        "print(result)\n",
        "```\n",
        "\n",
        "Here:\n",
        "\n",
        "* `t1` lives in a **vector space** (shape `(3,)`, dtype `float32`)\n",
        "* `t2` lives in a **matrix space** (shape `(2,2)`, dtype `float32`)\n",
        "* We reshape `t1` to match the **space** needed for matrix multiplication.\n",
        "\n",
        "---\n",
        "\n",
        "✅ **In short:**\n",
        "In TensorFlow 2, **tensor space** is important because it defines the **structure, type, and device context** of all data. Correctly managing tensor spaces ensures mathematical correctness, efficient computation, and compatibility between operations."
      ],
      "metadata": {
        "id": "9PWXGb-TIMNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7 How can TensorBoard be integrated with TensorFlow 2.02\n",
        "In **TensorFlow 2.0.2**, **TensorBoard** is directly integrated into the training workflow through `tf.keras.callbacks.TensorBoard`.\n",
        "It allows you to **visualize training progress, metrics, model graphs, and more** in your browser.\n",
        "## **1️⃣ Install TensorBoard**\n",
        "\n",
        "If it’s not already installed:\n",
        "\n",
        "```bash\n",
        "pip install tensorboard\n",
        "```\n",
        "## **2️⃣ Basic Integration with `model.fit()`**\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "# Dummy data\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Simple model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Create log directory with timestamp\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# Train with TensorBoard callback\n",
        "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test),\n",
        "          callbacks=[tensorboard_callback])\n",
        "```\n",
        "## **3️⃣ Launch TensorBoard**\n",
        "\n",
        "In a terminal:\n",
        "\n",
        "```bash\n",
        "tensorboard --logdir=logs/fit\n",
        "```\n",
        "\n",
        "Then open in your browser:\n",
        "\n",
        "```\n",
        "http://localhost:6006\n",
        "```\n",
        "\n",
        "## **4️⃣ What You Can See in TensorBoard**\n",
        "\n",
        "* **Scalars** → Loss, accuracy, learning rate over epochs.\n",
        "* **Graphs** → Model computation graph.\n",
        "* **Histograms** → Weight and bias distributions.\n",
        "* **Images** → Sample data or filters.\n",
        "* **Projector** → Embedding visualizations.\n",
        "## **5️⃣ Example for Custom Training Loops**\n",
        "\n",
        "If you use `tf.GradientTape` instead of `model.fit()`, you can still log to TensorBoard:\n",
        "\n",
        "```python\n",
        "writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "with writer.as_default():\n",
        "    for step in range(100):\n",
        "        tf.summary.scalar('my_metric', step ** 0.5, step=step)\n",
        "``\n",
        "✅ **In short:**\n",
        "You integrate TensorBoard in TensorFlow 2.0.2 by creating a `TensorBoard` callback and passing it to `model.fit()`, then launch TensorBoard to visualize the logs.\n",
        "\n",
        "If you want, I can give you a **TensorFlow 2.0.2 + TensorBoard training workflow diagram** so the whole process from training to visualization is crystal clear. Would you like me to prepare that?\n"
      ],
      "metadata": {
        "id": "DkxaHHbWI7a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8 What is the purpose of TensorFlow Playground2\n",
        "**TensorFlow Playground** isn’t actually part of the TensorFlow 2.0.2 library — it’s an **interactive, browser-based tool** made by the TensorFlow team that lets you **experiment with and visualize how neural networks learn**.\n",
        "\n",
        "It’s available here: [https://playground.tensorflow.org](https://playground.tensorflow.org)\n",
        "## **Purpose of TensorFlow Playground**\n",
        "\n",
        "1. **Educational Demonstration**\n",
        "\n",
        "   * Helps beginners understand core neural network concepts without coding.\n",
        "   * Lets you *see* how network layers, neurons, activation functions, and learning rates affect results.\n",
        "\n",
        "2. **Visual Intuition**\n",
        "\n",
        "   * Shows data points and decision boundaries in real time as the model trains.\n",
        "   * Displays weight connections with varying strengths (thicker/darker lines = stronger weights).\n",
        "\n",
        "3. **Experimentation Without Setup**\n",
        "\n",
        "   * No Python, no installations — it runs entirely in the browser using JavaScript.\n",
        "   * Instant feedback on changes to architecture or hyperparameters.\n",
        "\n",
        "4. **Concept Testing**\n",
        "\n",
        "   * You can quickly try:\n",
        "\n",
        "     * Number of hidden layers/neurons\n",
        "     * Activation functions\n",
        "     * Learning rate changes\n",
        "     * Regularization strength\n",
        "     * Input feature selection\n",
        "## **Example Features**\n",
        "\n",
        "* **Datasets**: Spiral, circles, clusters, etc.\n",
        "* **Architecture Control**: Add/remove layers and neurons.\n",
        "* **Training Settings**: Batch size, learning rate, activation functions.\n",
        "* **Regularization**: L1/L2 penalties to prevent overfitting.\n",
        "* **Real-time Visualization**: Decision boundaries and loss curves update as training progresses.\n",
        "✅ **In short:**\n",
        "The purpose of **TensorFlow Playground** is to **teach and demonstrate how neural networks work** by letting you experiment visually and interactively — making abstract ML concepts more concrete."
      ],
      "metadata": {
        "id": "7ne3_CZRJfYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9 What is Netron, and how is it useful for deep learning models?\n",
        "Netron is an **open-source model visualizer** designed to help developers, researchers, and engineers **inspect and understand deep learning, machine learning, and neural network models**.\n",
        "\n",
        "### 🔹 What Netron Is\n",
        "* A **graphical viewer** that lets you **open, explore, and visualize the structure** of trained models.\n",
        "* Available as a **desktop app** (Windows, macOS, Linux), **web app**, and even as an **extension for VS Code**.\n",
        "### 🔹 Supported Model Formats\n",
        "Netron supports many popular frameworks and file formats, including:\n",
        "\n",
        "* **TensorFlow / Keras**: `.pb`, `.h5`, `.tflite`, `.json`, `.keras`\n",
        "* **PyTorch / TorchScript**: `.pt`, `.pth`, `.pkl`, `.onnx`\n",
        "* **ONNX**: `.onnx`\n",
        "* **Caffe / Caffe2**: `.caffemodel`, `.pbtxt`\n",
        "* **CoreML**: `.mlmodel`\n",
        "* **MXNet**: `.model`, `.params`, `.json`\n",
        "* Others (CNTK, Darknet, PaddlePaddle, scikit-learn, XGBoost, etc.)\n",
        "### 🔹 Why It’s Useful in Deep Learning\n",
        "\n",
        "1. **Model Structure Visualization**\n",
        "\n",
        "   * See the layers (Conv, Dense, LSTM, BatchNorm, etc.) in a **graph format**.\n",
        "   * Understand how inputs flow through the network to outputs.\n",
        "\n",
        "2. **Debugging & Verification**\n",
        "\n",
        "   * Quickly check if the model architecture matches expectations.\n",
        "   * Helps detect errors in model conversion (e.g., TensorFlow → ONNX).\n",
        "\n",
        "3. **Interoperability**\n",
        "\n",
        "   * Since it supports multiple frameworks, it’s helpful when sharing or deploying models across different platforms.\n",
        "\n",
        "4. **Transparency & Documentation**\n",
        "\n",
        "   * Makes models more understandable for teams, researchers, and even non-experts.\n",
        "   * Useful in presentations and reports.\n",
        "\n",
        "5. **No Training Needed**\n",
        "\n",
        "   * Netron only inspects the **static model file**, not the training process.\n",
        "   * It’s lightweight and doesn’t require GPU/CPU execution.\n"
      ],
      "metadata": {
        "id": "sxsszR7zIiV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10.What is the difference between TensorFlow and PyTorch?\n",
        "Great question! 🚀\n",
        "TensorFlow and PyTorch are the **two most widely used deep learning frameworks**, but they differ in philosophy, usability, and ecosystem.\n",
        "Here’s a structured comparison:\n",
        "## 🔹 1. **Origin & Popularity**\n",
        "\n",
        "* **TensorFlow (TF)**: Developed by Google Brain (2015). Widely used in industry, production, and large-scale deployments.\n",
        "* **PyTorch (PT)**: Developed by Facebook AI Research (2016). Became very popular in research for its flexibility and ease of use.\n",
        "## 🔹 2. **Programming Style**\n",
        "\n",
        "* **TensorFlow**:\n",
        "\n",
        "  * Initially **static computation graph** (define-and-run).\n",
        "  * TensorFlow 2.x introduced **Eager Execution** (define-by-run, like PyTorch).\n",
        "  * More boilerplate code compared to PyTorch (but improved with Keras API).\n",
        "* **PyTorch**:\n",
        "\n",
        "  * **Dynamic computation graph** (define-by-run).\n",
        "  * Feels more \"pythonic\" and intuitive for debugging.\n",
        "  * Easy to write and modify models on the fly.\n",
        "\n",
        "👉 **Key difference**: PyTorch is more *intuitive & flexible* for research; TensorFlow is more *structured* and geared toward production.\n",
        "## 🔹 3. **Model Deployment**\n",
        "\n",
        "* **TensorFlow**:\n",
        "\n",
        "  * TensorFlow Serving, TensorFlow Lite (mobile/IoT), TensorFlow\\.js (browser).\n",
        "  * Strong production ecosystem.\n",
        "* **PyTorch**:\n",
        "\n",
        "  * TorchServe, TorchScript, and PyTorch Mobile (improving fast).\n",
        "  * Deployment used to be weaker than TF, but catching up with ONNX support.\n",
        "## 🔹 4. **Ecosystem & Tools**\n",
        "\n",
        "* **TensorFlow**:\n",
        "\n",
        "  * Rich ecosystem: **TensorBoard** (visualization), **TF Hub** (pre-trained models), **TFX** (ML pipelines), **TPU support**.\n",
        "* **PyTorch**:\n",
        "\n",
        "  * Excellent for research: **TorchVision, TorchText, TorchAudio**.\n",
        "  * Hugely popular in academic papers.\n",
        "  * Visualization via TensorBoard (supported) or Visdom.\n",
        "## 🔹 5. **Community & Learning Curve**\n",
        "\n",
        "* **TensorFlow**:\n",
        "\n",
        "  * Larger industry adoption, many tutorials, but steeper learning curve (especially TF1.x).\n",
        "* **PyTorch**:\n",
        "\n",
        "  * Easier to learn and debug, strong research community.\n",
        "## 🔹 6. **Performance**\n",
        "\n",
        "* Both use **GPU acceleration (CUDA, cuDNN)** and support distributed training.\n",
        "* **TensorFlow** traditionally better optimized for production-scale workloads, but **PyTorch is catching up** with performance improvements.\n",
        "## ✅ Summary Table\n",
        "\n",
        "| Feature         | TensorFlow                          | PyTorch                              |\n",
        "| --------------- | ----------------------------------- | ------------------------------------ |\n",
        "| **Style**       | Static (TF1), Dynamic (TF2 + Keras) | Dynamic (define-by-run)              |\n",
        "| **Ease of Use** | More complex, better with Keras     | Very intuitive, Pythonic             |\n",
        "| **Deployment**  | Strong (TF Serving, Lite, JS, TPU)  | Improving (TorchServe, TorchScript)  |\n",
        "| **Ecosystem**   | Large (TFX, TensorBoard, TF Hub)    | Research-focused (TorchVision, etc.) |\n",
        "| **Community**   | Industry-heavy                      | Research-heavy                       |\n",
        "| **Performance** | Optimized for large-scale prod      | Excellent, catching up               |\n",
        "👉 **In simple terms**:\n",
        "\n",
        "* Use **PyTorch** if you’re doing research, experimenting, or want easy debugging.\n",
        "* Use **TensorFlow** if you need robust deployment, production pipelines, or TPU support."
      ],
      "metadata": {
        "id": "PQ---OgiJKtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. How do you install PyTorch ?\n",
        "Installing **PyTorch** depends on your operating system, Python version, and whether you want **CPU-only** or **GPU (CUDA)** support.\n",
        "\n",
        "Here’s a step-by-step guide:\n",
        "## 🔹 1. Using `pip` (most common)\n",
        "\n",
        "For **CPU-only**:\n",
        "\n",
        "```bash\n",
        "pip install torch torchvision torchaudio\n",
        "```\n",
        "\n",
        "For **GPU with CUDA** (example: CUDA 11.8):\n",
        "\n",
        "```bash\n",
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "```\n",
        "👉 Replace `cu118` with the CUDA version you want (`cu117`, `cu121`, etc.). If you’re not sure about CUDA, you can stick to **CPU-only** first.\n",
        "## 🔹 2. Using `conda`\n",
        "\n",
        "For **CPU-only**:\n",
        "\n",
        "```bash\n",
        "conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
        "``\n",
        "For **GPU with CUDA** (example: CUDA 11.8):\n",
        "\n",
        "```bash\n",
        "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
        "```\n",
        "## 🔹 3. Verify Installation\n",
        "\n",
        "Open Python and run:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "print(torch.__version__)        # shows installed version\n",
        "print(torch.cuda.is_available())  # True if CUDA (GPU) is available.\n",
        "⚡ **Tip:**\n",
        "\n",
        "* If you don’t have an NVIDIA GPU or don’t need GPU acceleration → install the **CPU-only** version (lighter & simpler).\n",
        "* If you have an NVIDIA GPU → match your **CUDA version** with PyTorch’s supported builds.\n",
        "Do you want me to give you the **exact installation command** for your system (Windows/Linux, Python version, GPU/CPU)?\n"
      ],
      "metadata": {
        "id": "Q8Op9iziJ5Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12 What is the basic structure of a PyTorch neural network ?\n",
        "In **PyTorch**, the basic structure of a neural network is built using the `torch.nn.Module` class.\n",
        "It provides a way to define layers and the forward pass.\n",
        "\n",
        "Here’s the general structure:\n",
        "### **1. Import required libraries**\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "```\n",
        "### **2. Define the Neural Network Class**\n",
        "\n",
        "* Inherit from `nn.Module`.\n",
        "* Define layers inside `__init__`.\n",
        "* Define how data flows through layers in `forward`.\n",
        "\n",
        "```python\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "\n",
        "        # Define layers\n",
        "        self.fc1 = nn.Linear(784, 128)   # Input → Hidden (e.g., 784 for MNIST)\n",
        "        self.fc2 = nn.Linear(128, 64)    # Hidden → Hidden\n",
        "        self.fc3 = nn.Linear(64, 10)     # Hidden → Output (10 classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass (data flow)\n",
        "        x = F.relu(self.fc1(x))   # Activation function\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)           # Output layer (logits)\n",
        "        return x\n",
        "```\n",
        "### **3. Create a Model Instance**\n",
        "\n",
        "```python\n",
        "model = SimpleNN()\n",
        "print(model)\n",
        "```\n",
        "### **4. Add Optimizer and Loss Function**\n",
        "\n",
        "```python\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()   # Loss function\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)   # Optimizer\n",
        "```\n",
        "### **5. Training Loop (basic structure)**\n",
        "\n",
        "```python\n",
        "for epoch in range(10):\n",
        "    for data, target in dataloader:\n",
        "        optimizer.zero_grad()      # Reset gradients\n",
        "        output = model(data)       # Forward pass\n",
        "        loss = criterion(output, target)  # Compute loss\n",
        "        loss.backward()            # Backpropagation\n",
        "        optimizer.step()           # Update weights\n",
        "```\n",
        "✅ **Summary:**\n",
        "A PyTorch neural network has this **basic structure**:\n",
        "\n",
        "1. **Define the model** using `nn.Module` (`__init__` for layers, `forward()` for flow).\n",
        "2. **Choose a loss function** (`nn.CrossEntropyLoss`, `nn.MSELoss`, etc.).\n",
        "3. **Choose an optimizer** (`SGD`, `Adam`, etc.).\n",
        "4. **Training loop**: forward pass → loss → backward pass → update."
      ],
      "metadata": {
        "id": "nWL-Z91fKddj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13 What is the significance of tensors in PyTorch ?\n",
        "Great question! 🚀\n",
        "\n",
        "In **PyTorch**, **tensors** are the **core data structure**, just like **NumPy arrays**, but with additional superpowers designed for **deep learning and GPU acceleration**.\n",
        "## 🔑 **Significance of Tensors in PyTorch**\n",
        "\n",
        "### 1. **Primary Data Structure**\n",
        "\n",
        "* Everything in PyTorch (inputs, outputs, weights, gradients) is represented as a **tensor**.\n",
        "* Example:\n",
        "\n",
        "  ```python\n",
        "  import torch\n",
        "  x = torch.tensor([[1, 2], [3, 4]])\n",
        "  print(x)\n",
        "  ```\n",
        "### 2. **Similar to NumPy Arrays, but More Powerful**\n",
        "\n",
        "* Tensors support operations like addition, multiplication, reshaping, slicing (like `numpy`).\n",
        "* BUT they can also run seamlessly on **GPU** for speed.\n",
        "* Example:\n",
        "\n",
        "  ```python\n",
        "  x = torch.tensor([1.0, 2.0, 3.0])\n",
        "  x_gpu = x.to('cuda')   # Move tensor to GPU\n",
        "  ```\n",
        "### 3. **Automatic Differentiation (Autograd)**\n",
        "\n",
        "* Tensors can track operations performed on them when `requires_grad=True`.\n",
        "* This enables **backpropagation** in neural networks automatically.\n",
        "* Example:\n",
        "\n",
        "  ```python\n",
        "  x = torch.tensor(2.0, requires_grad=True)\n",
        "  y = x**2\n",
        "  y.backward()\n",
        "  print(x.grad)   # dy/dx = 2x = 4\n",
        "  ```\n",
        "### 4. **Bridge Between Data and Models**\n",
        "\n",
        "* Training data → **Tensor**\n",
        "* Model parameters (weights & biases) → **Tensor**\n",
        "* Gradients → **Tensor**\n",
        "* Thus, **all computations in PyTorch are tensor operations**.\n",
        "### 5. **Highly Optimized Performance**\n",
        "\n",
        "* PyTorch uses optimized backends (**C/CUDA, cuDNN, MKL**) for tensor operations.\n",
        "* This makes deep learning training very efficient.\n",
        "✅ **In summary:**\n",
        "Tensors in PyTorch are the **fundamental building blocks** for representing data and parameters. They combine the flexibility of NumPy arrays with the power of GPUs and automatic differentiation—making them essential for building and training deep learning models."
      ],
      "metadata": {
        "id": "z6dUoaPPLHBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14 What is the difference between torch.Tensor and torch.cuda.Tensor in PyTorch2 ?\n",
        "Good question 👍 This confuses many beginners in PyTorch.\n",
        "\n",
        "Let’s break it down:\n",
        "## 🔑 **1. `torch.Tensor`**\n",
        "\n",
        "* The **default tensor type** in PyTorch.\n",
        "* Usually created on the **CPU** (unless you explicitly move it to GPU).\n",
        "* Example:\n",
        "\n",
        "  ```python\n",
        "  import torch\n",
        "  a = torch.Tensor([1, 2, 3])   # By default on CPU\n",
        "  print(a.device)   # cpu\n",
        "  ```\n",
        "## 🔑 **2. `torch.cuda.Tensor`**\n",
        "\n",
        "* A tensor stored on a **GPU device**.\n",
        "* Allows **fast parallel computation** using CUDA (NVIDIA GPU backend).\n",
        "* You don’t create `torch.cuda.Tensor` directly most of the time—instead, you move a normal tensor to GPU:\n",
        "\n",
        "  ```python\n",
        "  b = torch.Tensor([1, 2, 3]).to('cuda')  # Move to GPU\n",
        "  print(b.device)  # cuda:0\n",
        "  ```\n",
        "## ⚖️ **Key Differences**\n",
        "\n",
        "| Feature               | `torch.Tensor` (CPU)                               | `torch.cuda.Tensor` (GPU)               |\n",
        "| --------------------- | -------------------------------------------------- | --------------------------------------- |\n",
        "| **Device**            | Stored in CPU memory                               | Stored in GPU (CUDA) memory             |\n",
        "| **Computation speed** | Slower for large data (serial/limited parallelism) | Much faster for deep learning workloads |\n",
        "| **Usage**             | Default, general-purpose                           | Used when training models on GPU        |\n",
        "| **Conversion**        | Default tensor type                                | Created by `.to('cuda')` or `.cuda()`   |\n",
        "| **Example**           | `torch.tensor([1,2])`                              | `torch.tensor([1,2]).cuda()`            |\n",
        "\n",
        "## ✅ Example: Comparing CPU vs GPU\n",
        "\n",
        "```python\n",
        "x_cpu = torch.randn(1000, 1000)        # CPU tensor\n",
        "x_gpu = torch.randn(1000, 1000, device='cuda')  # GPU tensor\n",
        "\n",
        "# Matrix multiplication (will be much faster on GPU)\n",
        "y_cpu = torch.mm(x_cpu, x_cpu)\n",
        "y_gpu = torch.mm(x_gpu, x_gpu)\n",
        "```\n",
        "## ⚠️ Important Notes\n",
        "\n",
        "* You **cannot** directly operate between a CPU tensor and a GPU tensor. They must be on the same device:\n",
        "\n",
        "  ```python\n",
        "  a = torch.tensor([1,2])\n",
        "  b = torch.tensor([3,4], device='cuda')\n",
        "  a + b   # ❌ RuntimeError\n",
        "  ``\n",
        "  Fix:\n",
        "\n",
        "  ```python\n",
        "  a = a.to('cuda')\n",
        "  c = a + b  # ✅ Now both on GPU\n",
        "  ```\n",
        "✅ **In short:**\n",
        "\n",
        "* `torch.Tensor` → Default CPU tensor.\n",
        "* `torch.cuda.Tensor` → Tensor stored on GPU for accelerated computation.\n",
        "* Use `.to('cuda')` or `.cuda()` to move between them"
      ],
      "metadata": {
        "id": "kx795p8HUxop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15 What is the purpose of the torch.optim module in PyTorch ?\n",
        "The **`torch.optim`** module in PyTorch provides **optimization algorithms** that are used to update the parameters (weights and biases) of a neural network during training.\n",
        "\n",
        "When training a model, we minimize a **loss function** that measures the difference between predicted and actual values. To do this, we use optimization algorithms (like **Stochastic Gradient Descent, Adam, RMSprop, etc.**) that adjust model parameters based on gradients computed during **backpropagation**.\n",
        "### 🔑 **Main purposes of `torch.optim`**:\n",
        "\n",
        "1. **Parameter Updates**\n",
        "\n",
        "   * It automates the process of updating weights using gradient descent and its variants.\n",
        "   * Example: `optimizer.step()` updates the parameters after gradients are calculated with `.backward()`.\n",
        "\n",
        "2. **Encapsulation of Algorithms**\n",
        "\n",
        "   * Provides ready-to-use implementations of popular optimizers (SGD, Adam, Adagrad, RMSprop, etc.), so you don’t have to implement them manually.\n",
        "\n",
        "3. **Gradient Management**\n",
        "\n",
        "   * Works with `loss.backward()` to apply gradients to parameters.\n",
        "   * Requires `optimizer.zero_grad()` to reset gradients before the next backward pass (since PyTorch accumulates gradients by default).\n",
        "\n",
        "4. **Flexibility**\n",
        "\n",
        "   * Allows specifying learning rates, momentum, weight decay (regularization), etc.\n",
        "   * Supports parameter groups so different parts of the model can have different hyperparameters.\n",
        "### 🔧 **Example Usage**\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Simple model\n",
        "model = nn.Linear(10, 1)\n",
        "\n",
        "# Define optimizer (Stochastic Gradient Descent)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Training loop\n",
        "for data, target in dataloader:\n",
        "    optimizer.zero_grad()        # Reset gradients\n",
        "    output = model(data)         # Forward pass\n",
        "    loss = loss_fn(output, target)\n",
        "    loss.backward()              # Backpropagation\n",
        "    optimizer.step()             # Update parameters\n",
        "```"
      ],
      "metadata": {
        "id": "S-GCIazNmSYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16 What are some common activation functions used in neural networks ?\n",
        "Great question! 🚀\n",
        "Activation functions are mathematical functions applied to neurons in a neural network. They introduce **non-linearity**, allowing the network to learn complex patterns instead of just linear relationships.\n",
        "\n",
        "Here are some **common activation functions** used in neural networks:\n",
        "### 🔑 **1. Sigmoid (Logistic Function)**\n",
        "\n",
        "$$\n",
        "f(x) = \\frac{1}{1 + e^{-x}}\n",
        "$$\n",
        "\n",
        "* **Range**: (0, 1)\n",
        "* **Pros**: Good for probabilities.\n",
        "* **Cons**: Vanishing gradient problem (for very large or very small inputs, gradients become nearly zero).\n",
        "* **Usage**: Historically popular, now mostly used in the output layer for binary classification.\n",
        "### 🔑 **2. Hyperbolic Tangent (tanh)**\n",
        "\n",
        "$$\n",
        "f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
        "$$\n",
        "\n",
        "* **Range**: (-1, 1)\n",
        "* **Pros**: Zero-centered (better than sigmoid).\n",
        "* **Cons**: Still suffers from vanishing gradients for large inputs.\n",
        "* **Usage**: Sometimes used in hidden layers, but often replaced by ReLU.\n",
        "### 🔑 **3. Rectified Linear Unit (ReLU)**\n",
        "\n",
        "$$\n",
        "f(x) = \\max(0, x)\n",
        "$$\n",
        "\n",
        "* **Range**: \\[0, ∞)\n",
        "* **Pros**: Simple, fast, helps avoid vanishing gradient problem.\n",
        "* **Cons**: Can cause \"dying ReLU\" (neurons stuck at 0 and never update).\n",
        "* **Usage**: Most common activation in hidden layers of deep networks.\n",
        "### 🔑 **4. Leaky ReLU**\n",
        "\n",
        "$$\n",
        "f(x) =\n",
        "\\begin{cases}\n",
        "x & \\text{if } x > 0 \\\\\n",
        "\\alpha x & \\text{if } x \\leq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "* **Range**: (-∞, ∞)\n",
        "* **Pros**: Fixes dying ReLU by allowing a small slope ($\\alpha$, e.g. 0.01) for negative values.\n",
        "* **Usage**: Often used as an alternative to ReLU.\n",
        "### 🔑 **5. Parametric ReLU (PReLU)**\n",
        "\n",
        "* Similar to Leaky ReLU, but the slope $\\alpha$ is learned during training.\n",
        "* Provides flexibility compared to fixed Leaky ReLU.\n",
        "### 🔑 **6. Softmax**\n",
        "\n",
        "$$\n",
        "f(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
        "$$\n",
        "\n",
        "* **Range**: (0, 1), outputs sum to 1.\n",
        "* **Pros**: Converts raw scores (logits) into probabilities.\n",
        "* **Usage**: Output layer for **multi-class classification**.\n",
        "### 🔑 **7. Swish**\n",
        "\n",
        "$$\n",
        "f(x) = x \\cdot \\sigma(x)\n",
        "$$\n",
        "\n",
        "(where $\\sigma(x)$ is the sigmoid function)\n",
        "\n",
        "* **Pros**: Smooth and avoids dead neurons, better performance in some deep networks.\n",
        "* **Usage**: Used in Google’s EfficientNet--\n",
        "\n",
        "### 🔑 **8. GELU (Gaussian Error Linear Unit)**\n",
        "\n",
        "$$\n",
        "f(x) = x \\cdot \\Phi(x)\n",
        "$$\n",
        "\n",
        "(where $\\Phi(x)$ is the Gaussian cumulative distribution function)\n",
        "\n",
        "* **Pros**: Combines benefits of ReLU and sigmoid; smoother than ReLU.\n",
        "* **Usage**: Popular in **Transformers (BERT, GPT, etc.)**.\n",
        "✅ **Summary**:\n",
        "\n",
        "* **Hidden layers**: ReLU (or variants like Leaky ReLU, GELU, Swish)\n",
        "* **Binary classification output**: Sigmoid\n",
        "* **Multi-class classification output**: Softmax"
      ],
      "metadata": {
        "id": "1e3c7e39mzYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 17What is the difference between torch.nn.Module and torch.nn.Sequential in PyTorch ?\n",
        "Good question 👍 — these two are closely related but serve **different purposes** in PyTorch. Let’s break it down clearly:\n",
        "## 🔹 **`torch.nn.Module`**\n",
        "\n",
        "* **Base class** for **all neural network components** in PyTorch.\n",
        "* Every custom model, layer, or operation is usually a subclass of `nn.Module`.\n",
        "* Gives you full **flexibility** to define:\n",
        "\n",
        "  * Layers in the `__init__` method.\n",
        "  * Forward computation logic in the `forward()` method.\n",
        "\n",
        "✅ Use it when you need **complex models** with multiple branches, loops, conditional logic, or anything beyond a straight sequence of layers.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 20)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(20, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = MyModel()\n",
        "print(model)\n",
        "```\n",
        "\n",
        "This allows **custom computations** in the forward pass.\n",
        "## 🔹 **`torch.nn.Sequential`**\n",
        "\n",
        "* A **container module** that lets you stack layers **in order**.\n",
        "* The input flows **sequentially** through each layer, without needing to explicitly define a `forward()` method.\n",
        "* Easier and shorter syntax for **simple feedforward models**.\n",
        "\n",
        "✅ Use it when your model is just a **straight chain of layers**, without any custom logic.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```python\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 20),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 1)\n",
        ")\n",
        "\n",
        "print(model)\n",
        "```\n",
        "This does the same as the `MyModel` above, but with **less code**.\n",
        "## 🔑 **Key Differences**\n",
        "\n",
        "| Feature           | `nn.Module`                                                         | `nn.Sequential`                                           |\n",
        "| ----------------- | ------------------------------------------------------------------- | --------------------------------------------------------- |\n",
        "| **Flexibility**   | Maximum flexibility; you define forward logic manually.             | Limited to simple forward pass (layers applied in order). |\n",
        "| **Custom Logic**  | Can include loops, branches, conditionals, multiple inputs/outputs. | Not possible — strictly sequential.                       |\n",
        "| **Code Length**   | More verbose (need to define class + forward).                      | Short and simple for straightforward models.              |\n",
        "| **Best Use Case** | Complex architectures (CNNs, RNNs, transformers, etc.)              | Quick prototyping or simple feedforward networks.         |\n",
        "✅ **Summary**:\n",
        "\n",
        "* Use **`nn.Sequential`** for **simple, linear stack models**.\n",
        "* Use **`nn.Module`** when you need **customization and control** over the forward pass."
      ],
      "metadata": {
        "id": "bgdbMrognhRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 18 How can you monitor training progress in TensorFlow 2.02 ?\n",
        "Great question! 🚀 Monitoring training progress is essential to understand how well your model is learning, prevent overfitting, and tune hyperparameters.\n",
        "\n",
        "In **TensorFlow 2.x** (including **2.0.2**), you can monitor training progress using several built-in tools and methods:\n",
        "## 🔹 1. **Using the `fit()` Method Output**\n",
        "\n",
        "When you call `model.fit()`, TensorFlow prints the training loss and metrics (like accuracy) for each epoch and batch (if `verbose=1` or `2`).\n",
        "\n",
        "```python\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(x_val, y_val),\n",
        "    verbose=1\n",
        ")\n",
        "```\n",
        "\n",
        "* `history` stores logs of training and validation loss/metrics.\n",
        "* You can later **plot them** for better visualization.\n",
        "## 🔹 2. **Visualizing Training with Matplotlib**\n",
        "\n",
        "The `history` object returned by `fit()` contains a dictionary of metrics.\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='train accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "## 🔹 3. **TensorBoard (Most Powerful Tool)**\n",
        "\n",
        "TensorBoard is TensorFlow’s built-in visualization dashboard.\n",
        "\n",
        "**Step 1: Import and setup callback**\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir=\"logs\",\n",
        "    histogram_freq=1\n",
        ")\n",
        "```\n",
        "\n",
        "**Step 2: Use callback in training**\n",
        "\n",
        "```python\n",
        "model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=10,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[tensorboard_callback]\n",
        ")\n",
        "```\n",
        "\n",
        "**Step 3: Launch TensorBoard**\n",
        "\n",
        "```bash\n",
        "tensorboard --logdir=logs\n",
        "```\n",
        "\n",
        "👉 Then open `http://localhost:6006/` in a browser to see loss, accuracy, histograms, graphs, etc.\n",
        "## 🔹 4. **Custom Callbacks**\n",
        "\n",
        "You can define your own callback to monitor progress, log metrics, or stop training early.\n",
        "\n",
        "**Example: Early stopping**\n",
        "\n",
        "```python\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=50,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "```\n",
        "\n",
        "This will **stop training automatically** if validation loss does not improve for 3 epochs.\n",
        "## 🔹 5. **Progress Bars with `tqdm`**\n",
        "\n",
        "You can wrap the training loop with `tqdm` for nicer progress visualization (when using custom training loops).\n",
        "\n",
        "```python\n",
        "from tqdm import tqdm\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for x_batch, y_batch in tqdm(dataset):\n",
        "        # training step\n",
        "        pass\n",
        "```\n",
        "✅ **Summary**:\n",
        "\n",
        "* **Basic monitoring** → `fit()` output + `history` plots.\n",
        "* **Advanced visualization** → **TensorBoard**.\n",
        "* **Training control** → **Callbacks** (like EarlyStopping, ModelCheckpoint).\n",
        "* **Custom loops** → `tqdm` progress bars.\n"
      ],
      "metadata": {
        "id": "-2yw-zgsoHiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 19 How does the Keras API fit into TensorFlow 2.02 ?\n",
        "Great question! 🙌\n",
        "In **TensorFlow 2.x** (including **2.0.2**), **Keras is the official high-level API** for building and training neural networks.\n",
        "\n",
        "Let’s break it down:\n",
        "## 🔹 1. **Integration of Keras into TensorFlow 2.x**\n",
        "\n",
        "* In earlier versions, **Keras** was a **separate standalone library** (`keras` package).\n",
        "* Starting from **TensorFlow 2.0**, Keras has been **fully integrated** and is available as:\n",
        "\n",
        "  ```python\n",
        "  import tensorflow as tf\n",
        "  from tensorflow import keras\n",
        "  ```\n",
        "* This means you no longer need to install `keras` separately — you use **`tf.keras`**, which is maintained and optimized by the TensorFlow team.\n",
        "## 🔹 2. **Roles of Keras inside TensorFlow**\n",
        "\n",
        "Keras provides:\n",
        "\n",
        "1. **Model definition APIs**\n",
        "\n",
        "   * **Sequential API**: Simple stack of layers.\n",
        "   * **Functional API**: More flexible, allows complex architectures (multiple inputs/outputs).\n",
        "   * **Model subclassing**: Full control, by subclassing `tf.keras.Model`.\n",
        "\n",
        "2. **Predefined Layers and Models**\n",
        "\n",
        "   * Layers: `Dense`, `Conv2D`, `LSTM`, `Dropout`, etc.\n",
        "   * Ready-to-use models: `MobileNet`, `ResNet`, `BERT` (from `tf.keras.applications`).\n",
        "\n",
        "3. **Training Utilities**\n",
        "\n",
        "   * `compile()` → Define optimizer, loss, and metrics.\n",
        "   * `fit()` → Training loop.\n",
        "   * `evaluate()` → Evaluate model.\n",
        "   * `predict()` → Make predictions.\n",
        "\n",
        "4. **Callbacks for Training Control**\n",
        "\n",
        "   * Early stopping, learning rate scheduling, TensorBoard logging, checkpoint saving, etc.\n",
        "## 🔹 3. **Example in TensorFlow 2.0.2 with Keras**\n",
        "\n",
        "Here’s how you typically use Keras inside TensorFlow:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define model\n",
        "model = models.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(100,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate\n",
        "model.evaluate(x_test, y_test)\n",
        "```\n",
        "\n",
        "👉 Here, everything (`Sequential`, `Dense`, `fit`, etc.) comes from **`tf.keras`**.\n",
        "\n",
        "## 🔹 4. **Why This Integration Matters**\n",
        "\n",
        "* **Unified workflow**: No need to choose between “Keras” and “TensorFlow API.”\n",
        "* **Eager execution support**: TensorFlow 2.x runs eagerly by default, making debugging easier.\n",
        "* **Scalability**: Works seamlessly with GPUs/TPUs.\n",
        "* **Consistency**: Models written in `tf.keras` are portable across different TensorFlow platforms.\n",
        "✅ **Summary**:\n",
        "In TensorFlow 2.0.2, **Keras (`tf.keras`) is the main high-level API** for building, training, and deploying deep learning models. It combines **ease of use** (Keras style) with **TensorFlow’s scalability and performance**."
      ],
      "metadata": {
        "id": "6m4RHK9Ao6xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 20 What is an example of a deep learning project that can be implemented using TensorFlow 2.02\n",
        "Nice one 👍 — let’s walk through a concrete example of a **deep learning project** that you can implement in **TensorFlow 2.0.2**.\n",
        "# 🔹 Example Project: **Image Classification with CIFAR-10 Dataset**\n",
        "\n",
        "The **CIFAR-10 dataset** contains **60,000 images (32×32 pixels)** across **10 categories** like airplanes, cars, birds, cats, etc.\n",
        "This project demonstrates how to build, train, and evaluate a Convolutional Neural Network (CNN) using **TensorFlow 2.0.2 + Keras API**.\n",
        "## 🔧 **Step 1: Import Libraries**\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "```\n",
        "## 🔧 **Step 2: Load and Preprocess Data**\n",
        "\n",
        "```python\n",
        "# Load dataset\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values (0–255 → 0–1)\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "```\n",
        "## 🔧 **Step 3: Build CNN Model**\n",
        "\n",
        "```python\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "## 🔧 **Step 4: Compile the Model**\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "## 🔧 **Step 5: Train the Model**\n",
        "\n",
        "```python\n",
        "history = model.fit(x_train, y_train, epochs=10,\n",
        "                    validation_data=(x_test, y_test))\n",
        "```\n",
        "## 🔧 **Step 6: Evaluate the Model**\n",
        "\n",
        "```python\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f\"Test accuracy: {test_acc:.2f}\")\n",
        "```\n",
        "## 🔧 **Step 7: Visualize Training Progress**\n",
        "\n",
        "```python\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "## ✅ What You’ll Learn from This Project\n",
        "\n",
        "* How to use **TensorFlow 2.0.2 + Keras API** for building CNNs.\n",
        "* How to load and preprocess datasets.\n",
        "* How to train, evaluate, and visualize results.\n",
        "* How deep learning models can **recognize objects in images**.\n",
        "👉 This project is a **beginner-to-intermediate** level task and can be extended by:\n",
        "\n",
        "* Adding **data augmentation** to improve generalization.\n",
        "* Using **transfer learning** with pre-trained models like ResNet or MobileNet.\n",
        "* Deploying the trained model with **TensorFlow Serving** or **TensorFlow Lite** (for mobile)."
      ],
      "metadata": {
        "id": "EpnHH9hrpXSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 21 What is the main advantage of using pre-trained models in TensorFlow and PyTorch?\n",
        "Great question 🙌\n",
        "\n",
        "Using **pre-trained models** is one of the biggest time-savers in deep learning, both in **TensorFlow (`tf.keras.applications`)** and **PyTorch (`torchvision.models`)**.\n",
        "## 🔹 **Main Advantage**\n",
        "\n",
        "The **main advantage** of using pre-trained models is that they let you **leverage knowledge already learned from large datasets** (like ImageNet with 1.2M images), so you don’t have to train a model from scratch.\n",
        "\n",
        "This gives you:\n",
        "\n",
        "1. **Faster training** 🚀\n",
        "\n",
        "   * Since most weights are already optimized, you only fine-tune or adapt the model to your dataset.\n",
        "\n",
        "2. **Better performance with less data** 📈\n",
        "\n",
        "   * Training from scratch often needs massive datasets.\n",
        "   * Pre-trained models generalize well, so even with a smaller dataset, you can achieve high accuracy.\n",
        "\n",
        "3. **Reduced compute cost** 💻\n",
        "\n",
        "   * Training big models like ResNet, BERT, or GPT from scratch needs multiple GPUs/TPUs and weeks of training.\n",
        "   * Using a pre-trained model lets you skip this heavy lifting.\n",
        "\n",
        "4. **Proven architectures** 🏗️\n",
        "\n",
        "   * Pre-trained models are usually well-researched (ResNet, VGG, EfficientNet, BERT, etc.), so you start with a strong baseline.\n",
        "\n",
        "## 🔹 Example in TensorFlow\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Load pre-trained ResNet50\n",
        "base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224,224,3))\n",
        "\n",
        "# Freeze layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add custom classifier on top\n",
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "```\n",
        "## 🔹 Example in PyTorch\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load pre-trained ResNet18\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Freeze layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace final layer for 10-class classification\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "```\n",
        "✅ **Summary**:\n",
        "The **main advantage** of using pre-trained models in TensorFlow and PyTorch is that they provide a **strong starting point** (already trained on massive datasets), enabling **faster, cheaper, and more accurate training** — especially useful when your dataset is small or resources are limited."
      ],
      "metadata": {
        "id": "MQ2tDjh7pXOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical\n"
      ],
      "metadata": {
        "id": "TgqCiaMQqXdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 How do you install and verify that TensorFlow 2.0 was installed successfully2\n",
        "Perfect question 👍 — let’s go step by step to **install TensorFlow 2.0** and then **verify** the installation.\n",
        "# 🔹 1. **Install TensorFlow 2.0**\n",
        "\n",
        "TensorFlow can be installed using **pip**. Run this in your terminal or Anaconda prompt:\n",
        "\n",
        "```bash\n",
        "pip install tensorflow==2.0.0\n",
        "```\n",
        "\n",
        "👉 If you want GPU support (requires CUDA + cuDNN installed), use:\n",
        "\n",
        "```bash\n",
        "pip install tensorflow-gpu==2.0.0\n",
        "```\n",
        "# 🔹 2. **Verify the Installation**\n",
        "\n",
        "After installation, open **Python** (or Jupyter Notebook) and run:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Simple test\n",
        "hello = tf.constant(\"Hello, TensorFlow 2.0!\")\n",
        "print(hello.numpy())\n",
        "```\n",
        "\n",
        "✅ If installed correctly, you should see:\n",
        "\n",
        "```\n",
        "TensorFlow version: 2.0.0\n",
        "b'Hello, TensorFlow 2.0!'\n",
        "``\n",
        "# 🔹 3. **Verify GPU Availability (Optional)**\n",
        "\n",
        "If you installed the GPU version:\n",
        "\n",
        "```python\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
        "```\n",
        "\n",
        "If a GPU is detected, you’ll see a number ≥ 1.\n",
        "# 🔹 4. **Troubleshooting**\n",
        "\n",
        "* If installation fails, first upgrade pip:\n",
        "\n",
        "  ```bash\n",
        "  pip install --upgrade pip\n",
        "  ```\n",
        "* If you’re in Jupyter, restart the kernel after installation.\n",
        "* Make sure your Python version is compatible (**Python 3.5–3.7** for TensorFlow 2.0.0).\n",
        "✅ **Summary**:\n",
        "\n",
        "1. Install with `pip install tensorflow==2.0.0`\n",
        "2. Verify with `print(tf.__version__)` → should show `2.0.0`\n",
        "3. Run a small test constant to confirm execution\n",
        "4. (Optional) Check GPU availability with `tf.config.list_physical_devices('GPU')`"
      ],
      "metadata": {
        "id": "kVvqRKWipXL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 How can you define a simple function in TensorFlow 2.0 to perform addition ?\n",
        "In **TensorFlow 2.0**, functions can be defined just like in standard Python, but you can also use **`@tf.function`** to turn them into **TensorFlow graphs** for better performance.\n",
        "\n",
        "Here’s how you can define a **simple addition function** 👇\n",
        "\n",
        "## 🔹 1. Basic Python + TensorFlow (Eager Execution)\n",
        "\n",
        "By default, TensorFlow 2.0 runs in **eager mode**, so operations run immediately:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Simple addition function\n",
        "def add_numbers(x, y):\n",
        "    return tf.add(x, y)\n",
        "\n",
        "# Test\n",
        "a = tf.constant(5)\n",
        "b = tf.constant(7)\n",
        "result = add_numbers(a, b)\n",
        "print(\"Result:\", result.numpy())  # Output: 12\n",
        "```\n",
        "## 🔹 2. Using `@tf.function` (Graph Mode)\n",
        "\n",
        "If you want TensorFlow to optimize it into a **graph** (faster execution, useful in training loops), you can do:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "@tf.function\n",
        "def add_numbers(x, y):\n",
        "    return tf.add(x, y)\n",
        "\n",
        "# Test\n",
        "a = tf.constant(10)\n",
        "b = tf.constant(20)\n",
        "result = add_numbers(a, b)\n",
        "print(\"Result:\", result.numpy())  # Output: 30\n",
        "```\n",
        "## ✅ Summary\n",
        "\n",
        "* **Eager mode** (default) → behaves like normal Python, easy to debug.\n",
        "* **Graph mode (`@tf.function`)** → compiles into a TensorFlow graph, runs faster."
      ],
      "metadata": {
        "id": "9iILQtlHpXJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 How can you create a simple neural network in TensorFlow 2.0 with one hidden layer ?\n",
        "Great question 🙌 — let’s build a **simple feedforward neural network** in **TensorFlow 2.0** with **one hidden layer** using the **Keras API**.\n",
        "# 🔹 Steps to Build a Simple Neural Network\n",
        "\n",
        "### **1. Import Libraries**\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "```\n",
        "### **2. Define the Model**\n",
        "\n",
        "We’ll make a network with:\n",
        "\n",
        "* **Input layer** (shape depends on dataset, here 20 features)\n",
        "* **One hidden layer** (e.g., 16 neurons, ReLU activation)\n",
        "* **Output layer** (e.g., 1 neuron with sigmoid for binary classification)\n",
        "\n",
        "```python\n",
        "# Sequential model with one hidden layer\n",
        "model = models.Sequential([\n",
        "    layers.Dense(16, activation='relu', input_shape=(20,)),  # hidden layer\n",
        "    layers.Dense(1, activation='sigmoid')  # output layer\n",
        "])\n",
        "``\n",
        "### **3. Compile the Model**\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "### **4. Train the Model**\n",
        "\n",
        "Assume `x_train, y_train` are training data with 20 features:\n",
        "\n",
        "```python\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "```\n",
        "### **5. Evaluate the Model**\n",
        "\n",
        "```python\n",
        "loss, acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(\"Test Accuracy:\", acc)\n",
        "```\n",
        "# ✅ Summary\n",
        "\n",
        "This simple neural network has:\n",
        "\n",
        "* **One hidden layer** with 16 neurons (ReLU activation).\n",
        "* **Output layer** with sigmoid activation (for binary classification).\n",
        "* Trained using **Adam optimizer** and **binary crossentropy loss**.\n"
      ],
      "metadata": {
        "id": "8FaXqQTNr9QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 How can you visualize the training progress using TensorFlow and Matplotlib\n",
        "Great question 👍 — visualizing training progress helps you understand whether your model is **learning well** or **overfitting**.\n",
        "\n",
        "In **TensorFlow 2.x (including 2.0)**, you can easily track progress using the **History object** returned by `model.fit()`, and then plot it with **Matplotlib**.\n",
        "## 🔹 Example: Visualizing Training Progress\n",
        "\n",
        "### **1. Train the Model**\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example: simple model\n",
        "model = models.Sequential([\n",
        "    layers.Dense(16, activation='relu', input_shape=(20,)),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train (history object stores metrics)\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "```\n",
        "### **2. Plot Training vs Validation Accuracy**\n",
        "\n",
        "```python\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "### **3. Plot Training vs Validation Loss**\n",
        "\n",
        "```python\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "## 🔹 What This Shows\n",
        "\n",
        "* If **training accuracy keeps going up but validation accuracy goes down** → model is **overfitting**.\n",
        "* If both curves go up steadily → model is **learning well**.\n",
        "* If both flatten too early → maybe **increase model complexity** or **tune learning rate**.\n",
        "✅ **Summary**:\n",
        "\n",
        "* Use `history = model.fit(...)` → it stores loss & metrics.\n",
        "* Plot `history.history['loss']` and `history.history['accuracy']` with Matplotlib.\n",
        "* Compare training vs validation to detect **underfitting** or **overfitting**.\n"
      ],
      "metadata": {
        "id": "sSDqXZWhsSvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 How do you install PyTorch and verify the PyTorch installation\n",
        "Great question 👍 — let’s go step by step to **install PyTorch** and then **verify** that it works.\n",
        "# 🔹 1. Install PyTorch\n",
        "\n",
        "The recommended way is via **pip** or **conda**, depending on your environment.\n",
        "\n",
        "### ✅ Using pip (CPU-only version)\n",
        "\n",
        "```bash\n",
        "pip install torch torchvision torchaudio\n",
        "```\n",
        "\n",
        "### ✅ Using pip (with GPU support, CUDA 11.8 for example)\n",
        "\n",
        "```bash\n",
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "```\n",
        "\n",
        "> ⚠️ You need to have the matching **NVIDIA drivers + CUDA + cuDNN** installed for GPU versions.\n",
        "\n",
        "### ✅ Using conda (GPU version)\n",
        "\n",
        "```bash\n",
        "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
        "```\n",
        "\n",
        "👉 You can always get the latest installation command tailored to your system at the official site: [PyTorch Get Started](https://pytorch.org/get-started/locally/).\n",
        "\n",
        "# 🔹 2. Verify the Installation\n",
        "\n",
        "Open Python or Jupyter Notebook and run:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Check version\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "\n",
        "# Simple tensor test\n",
        "x = torch.rand(3, 3)\n",
        "print(\"Random tensor:\\n\", x)\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
        "```\n",
        "# 🔹 3. Expected Output\n",
        "\n",
        "✅ If installed correctly, you’ll see something like:\n",
        "\n",
        "```\n",
        "PyTorch version: 2.x.x\n",
        "Random tensor:\n",
        " tensor([[0.2345, 0.8765, 0.1234],\n",
        "         [0.5432, 0.1111, 0.9999],\n",
        "         [0.7654, 0.2222, 0.3333]])\n",
        "CUDA available: True\n",
        "GPU name: NVIDIA GeForce RTX 3060\n",
        "```\n",
        "\n",
        "(or `False` for CUDA if you installed the CPU-only version).\n",
        "✅ **Summary**\n",
        "\n",
        "1. Install via `pip install torch torchvision torchaudio` (CPU) or GPU version with CUDA.\n",
        "2. Import `torch` in Python and check version.\n",
        "3. Run a test tensor operation.\n",
        "4. (Optional) Verify GPU support with `torch.cuda.is_available()`.\n"
      ],
      "metadata": {
        "id": "2sLG-SrBtOIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 How do you create a simple neural network in PyTorch\n",
        "Great question 🙌 — let’s walk through building a **simple feedforward neural network** in **PyTorch** step by step.\n",
        "\n",
        "We’ll make a model with:\n",
        "\n",
        "* Input layer (e.g., 20 features)\n",
        "* One hidden layer (16 neurons, ReLU activation)\n",
        "* Output layer (1 neuron, sigmoid for binary classification)\n",
        "# 🔹 1. Import Libraries\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "```\n",
        "# 🔹 2. Define the Neural Network\n",
        "\n",
        "In PyTorch, you create a network by subclassing **`nn.Module`**:\n",
        "\n",
        "```python\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(20, 16)   # input → hidden\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(16, 1)    # hidden → output\n",
        "        self.sigmoid = nn.Sigmoid()    # for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Create model\n",
        "model = SimpleNN()\n",
        "print(model)\n",
        "```\n",
        "# 🔹 3. Define Loss and Optimizer\n",
        "\n",
        "```python\n",
        "criterion = nn.BCELoss()                 # Binary Cross-Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "```\n",
        "# 🔹 4. Dummy Training Loop (Example)\n",
        "\n",
        "```python\n",
        "# Example data (100 samples, 20 features each)\n",
        "x_train = torch.rand(100, 20)\n",
        "y_train = torch.randint(0, 2, (100, 1)).float()  # binary labels (0 or 1)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(x_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "```\n",
        "# 🔹 5. Make Predictions\n",
        "\n",
        "```python\n",
        "with torch.no_grad():\n",
        "    sample = torch.rand(1, 20)\n",
        "    prediction = model(sample)\n",
        "    print(\"Prediction:\", prediction.item())\n",
        "```\n",
        "## ✅ Summary\n",
        "\n",
        "* Define model by subclassing **`nn.Module`**.\n",
        "* Use **`forward()`** to describe how data flows through layers.\n",
        "* Train using **loss.backward()** and **optimizer.step()**.\n",
        "* Evaluate with `torch.no_grad()` for inference."
      ],
      "metadata": {
        "id": "L7wmur9TuBzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 How do you define a loss function and optimizer in PyTorch2\n",
        "Great question 👍 — in **PyTorch**, defining a **loss function** and an **optimizer** is an essential step before training a neural network. Let’s go step by step 👇\n",
        "# 🔹 1. **Define a Loss Function**\n",
        "\n",
        "PyTorch provides many built-in loss functions in `torch.nn`.\n",
        "Some common ones:\n",
        "\n",
        "* **`nn.MSELoss`** → for regression (Mean Squared Error)\n",
        "* **`nn.BCELoss`** → for binary classification (Binary Cross-Entropy)\n",
        "* **`nn.CrossEntropyLoss`** → for multi-class classification\n",
        "\n",
        "👉 Example (Binary Classification):\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.BCELoss()   # Binary cross-entropy loss\n",
        "```\n",
        "👉 Example (Multi-class Classification):\n",
        "\n",
        "```python\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "```\n",
        "# 🔹 2. **Define an Optimizer**\n",
        "\n",
        "PyTorch optimizers are in `torch.optim`.\n",
        "Common ones:\n",
        "\n",
        "* **`optim.SGD`** → Stochastic Gradient Descent\n",
        "* **`optim.Adam`** → Adaptive Moment Estimation (most widely used)\n",
        "* **`optim.RMSprop`** → Adaptive learning rate\n",
        "\n",
        "👉 Example:\n",
        "\n",
        "```python\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "```\n",
        "\n",
        "Here:\n",
        "\n",
        "* `model.parameters()` → tells optimizer which parameters (weights & biases) to update.\n",
        "* `lr` → learning rate.\n",
        "# 🔹 3. **Putting It All Together**\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Simple model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(20, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# Loss function (binary classification)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Optimizer (Adam with learning rate 0.001)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Example forward pass\n",
        "x = torch.rand(10, 20)     # batch of 10 samples, 20 features each\n",
        "y = torch.randint(0, 2, (10, 1)).float()\n",
        "\n",
        "# Compute loss\n",
        "outputs = model(x)\n",
        "loss = criterion(outputs, y)\n",
        "\n",
        "print(\"Loss value:\", loss.item())\n",
        "```\n",
        "## ✅ Summary\n",
        "\n",
        "* **Loss function (`criterion`)** measures how far predictions are from actual labels.\n",
        "* **Optimizer** (like Adam or SGD) updates model parameters using gradients (`loss.backward()` + `optimizer.step()`).\n"
      ],
      "metadata": {
        "id": "7h3u1BntudhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8 How do you implement a custom loss function in PyTorch\n",
        "Great question 🙌 — PyTorch makes it very flexible to implement **custom loss functions**.\n",
        "You can do this in **two main ways**:\n",
        "# 🔹 1. Using a **Python Function**\n",
        "\n",
        "You can write a simple function that takes predictions and targets, and returns a scalar loss (as a PyTorch tensor).\n",
        "\n",
        "👉 Example: **Mean Absolute Error (L1 Loss)** implemented manually\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "def custom_mae_loss(y_pred, y_true):\n",
        "    return torch.mean(torch.abs(y_pred - y_true))\n",
        "\n",
        "# Example usage\n",
        "y_true = torch.tensor([1.0, 0.0, 1.0])\n",
        "y_pred = torch.tensor([0.9, 0.2, 0.8])\n",
        "\n",
        "loss = custom_mae_loss(y_pred, y_true)\n",
        "print(\"Custom MAE Loss:\", loss.item())\n",
        "```\n",
        "# 🔹 2. Subclassing `nn.Module`\n",
        "\n",
        "If you want your loss to integrate seamlessly with the rest of PyTorch (like built-in losses), subclass **`torch.nn.Module`** and implement `forward()`.\n",
        "\n",
        "👉 Example: **Custom Weighted MSE Loss**\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class WeightedMSELoss(nn.Module):\n",
        "    def __init__(self, weight=1.0):\n",
        "        super(WeightedMSELoss, self).__init__()\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        return self.weight * torch.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "# Example usage\n",
        "criterion = WeightedMSELoss(weight=2.0)\n",
        "\n",
        "y_true = torch.tensor([1.0, 2.0, 3.0])\n",
        "y_pred = torch.tensor([1.5, 2.5, 2.0])\n",
        "\n",
        "loss = criterion(y_pred, y_true)\n",
        "print(\"Custom Weighted MSE Loss:\", loss.item())\n",
        "```\n",
        "# 🔹 3. Use in a Training Loop\n",
        "\n",
        "Custom loss functions can be used exactly like built-in ones:\n",
        "\n",
        "```python\n",
        "# Assume you already defined model, optimizer\n",
        "criterion = custom_mae_loss   # can be function or nn.Module\n",
        "\n",
        "for epoch in range(5):\n",
        "    y_pred = model(x_train)\n",
        "    loss = criterion(y_pred, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "```\n",
        "✅ **Summary**\n",
        "\n",
        "* You can implement a custom loss in PyTorch as:\n",
        "\n",
        "  1. A **function** that returns a tensor.\n",
        "  2. A **subclass of `nn.Module`** for more flexibility and integration.\n",
        "* Use it in training just like built-in losses (`nn.MSELoss`, `nn.CrossEntropyLoss`, etc.)\n"
      ],
      "metadata": {
        "id": "T_9zdXFSvTge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9 How do you save and load a TensorFlow model?\n",
        "Excellent question 🙌 — in **TensorFlow 2.x**, saving and loading models is very straightforward using **Keras APIs**.\n",
        "\n",
        "There are **two main formats** you can use:\n",
        "# 🔹 1. Save and Load the **Entire Model**\n",
        "\n",
        "This saves:\n",
        "✔ Model architecture\n",
        "✔ Weights\n",
        "✔ Training configuration (optimizer, loss, metrics)\n",
        "✔ Training state (so you can resume training)\n",
        "\n",
        "### ✅ Save Entire Model\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Example model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(16, activation='relu', input_shape=(20,)),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Save model in HDF5 format (.h5)\n",
        "model.save(\"my_model.h5\")\n",
        "\n",
        "# Or in TensorFlow SavedModel format (recommended)\n",
        "model.save(\"my_model\")\n",
        "``\n",
        "### ✅ Load Entire Model\n",
        "\n",
        "```python\n",
        "# Load HDF5 model\n",
        "model_h5 = keras.models.load_model(\"my_model.h5\")\n",
        "\n",
        "# Load SavedModel format\n",
        "model_tf = keras.models.load_model(\"my_model\")\n",
        "```\n",
        "# 🔹 2. Save and Load Only **Model Weights**\n",
        "\n",
        "Sometimes you just want to save the **weights** (not architecture).\n",
        "\n",
        "### ✅ Save Weights\n",
        "\n",
        "```python\n",
        "# Save weights only\n",
        "model.save_weights(\"model_weights.h5\")\n",
        "```\n",
        "### ✅ Load Weights\n",
        "\n",
        "```python\n",
        "# Create the same model architecture\n",
        "new_model = keras.Sequential([\n",
        "    keras.layers.Dense(16, activation='relu', input_shape=(20,)),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Load weights into it\n",
        "new_model.load_weights(\"model_weights.h5\")\n",
        "```\n",
        "⚠️ Note: When loading only weights, you **must recreate the exact same architecture** manually.\n",
        "# 🔹 3. Save and Load Only the **Model Architecture**\n",
        "\n",
        "If you just want to store the network structure, you can use **JSON or YAML**.\n",
        "\n",
        "### ✅ Save Architecture\n",
        "\n",
        "```python\n",
        "# Convert model architecture to JSON\n",
        "json_config = model.to_json()\n",
        "with open(\"model_arch.json\", \"w\") as json_file:\n",
        "    json_file.write(json_config)\n",
        "``\n",
        "### ✅ Load Architecture\n",
        "\n",
        "```python\n",
        "# Rebuild the model from JSON\n",
        "with open(\"model_arch.json\", \"r\") as json_file:\n",
        "    json_config = json_file.read()\n",
        "\n",
        "new_model = keras.models.model_from_json(json_config)\n",
        "``\n",
        "## ✅ Summary\n",
        "\n",
        "* **`model.save()` + `keras.models.load_model()`** → saves everything (recommended).\n",
        "* **`model.save_weights()` + `load_weights()`** → saves only weights.\n",
        "* **JSON/YAML** → saves only architecture."
      ],
      "metadata": {
        "id": "s7t6Nftbv_is"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}