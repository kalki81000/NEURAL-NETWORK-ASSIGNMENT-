{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBNB4uzEMDH/g8zvm0yXr8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalki81000/NEURAL-NETWORK-ASSIGNMENT-/blob/main/Untitled126.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RCNN&YOLO\n",
        "\n"
      ],
      "metadata": {
        "id": "ys6P5jOBPMsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 What types of tasks does Detectron2 support?\n",
        "Detectron2 is a **PyTorch-based computer vision library** developed by Facebook AI Research (FAIR). It provides a flexible framework for training and deploying state-of-the-art models in **object detection, segmentation, and related tasks**.\n",
        "\n",
        "Here are the main tasks Detectron2 supports:\n",
        "### üîπ **1. Object Detection**\n",
        "\n",
        "* Identifies and localizes objects in an image using **bounding boxes**.\n",
        "* Example: Detecting cars, people, or animals in a street scene.\n",
        "### üîπ **2. Instance Segmentation**\n",
        "\n",
        "* Detects objects **and** provides a **pixel-level mask** for each instance.\n",
        "* Example: Separating each person in a crowd, not just drawing a bounding box.\n",
        "### üîπ **3. Semantic Segmentation**\n",
        "\n",
        "* Assigns a **class label to every pixel** in the image (without separating different instances).\n",
        "* Example: Labeling all road pixels as \"road,\" all sky pixels as \"sky,\" etc.\n",
        "### üîπ **4. Panoptic Segmentation**\n",
        "\n",
        "* Combines **instance segmentation** and **semantic segmentation** into a single output.\n",
        "* Example: Identifying \"stuff\" (sky, grass) and \"things\" (cars, people) together.\n",
        "### üîπ **5. Keypoint Detection (Pose Estimation)**\n",
        "\n",
        "* Detects keypoints (like joints) on objects such as human bodies.\n",
        "* Example: Detecting body pose (arms, legs, head positions) in sports analysis.\n",
        "### üîπ **6. DensePose**\n",
        "\n",
        "* Maps **all human pixels** in an image to a 3D surface of a human body.\n",
        "* Example: Useful in augmented reality and virtual try-on applications.\n",
        "### üîπ **7. Custom Tasks (Extendable)**\n",
        "\n",
        "* Since Detectron2 is modular, it can be extended to:\n",
        "\n",
        "  * Video object detection\n",
        "  * Object tracking\n",
        "  * Weakly supervised detection\n",
        "  * Few-shot learning\n",
        "‚úÖ **In summary:**\n",
        "Detectron2 supports **object detection, instance segmentation, semantic segmentation, panoptic segmentation, keypoint detection, and DensePose**, while also being extendable to many other vision tasks.\n"
      ],
      "metadata": {
        "id": "C5xGwmiFPSWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  2 Why is data annotation important when training object detection models ?\n",
        "Great question üëå.\n",
        "Data annotation is **one of the most critical steps** in training object detection models because these models are **supervised learners** ‚Äì they learn from examples provided by annotated data.\n",
        "\n",
        "Here‚Äôs why annotation is so important:\n",
        "### üîπ **1. Provides Ground Truth for Learning**\n",
        "\n",
        "* Object detection models need to know **where** an object is and **what class** it belongs to.\n",
        "* Annotations (bounding boxes, class labels, masks) serve as the **ground truth** against which the model‚Äôs predictions are compared during training.\n",
        "### üîπ **2. Improves Model Accuracy**\n",
        "\n",
        "* Accurate annotations help the model learn to correctly localize and classify objects.\n",
        "* Poor or inconsistent annotations (wrong labels, misaligned boxes) introduce **noise**, which reduces detection accuracy.\n",
        "### üîπ **3. Enables Localization (Not Just Classification)**\n",
        "\n",
        "* Unlike image classification, object detection requires both:\n",
        "\n",
        "  1. **Classification** ‚Üí What is the object? (dog, car, person)\n",
        "  2. **Localization** ‚Üí Where is it? (bounding box coordinates)\n",
        "* Without precise annotation, the model cannot learn to localize objects properly.\n",
        "### üîπ **4. Handles Multiple Objects**\n",
        "\n",
        "* Real-world images usually contain **multiple objects** of different classes.\n",
        "* Annotation ensures the model learns to detect and distinguish between multiple objects in the same image.\n",
        "### üîπ **5. Supports Advanced Tasks**\n",
        "\n",
        "* Depending on the task, annotations provide different types of supervision:\n",
        "\n",
        "  * **Bounding boxes** ‚Üí Object detection\n",
        "  * **Pixel-level masks** ‚Üí Instance/Semantic segmentation\n",
        "  * **Keypoints** ‚Üí Human pose estimation\n",
        "  * **Dense mappings** ‚Üí DensePose\n",
        "\n",
        "Without correct annotations, these advanced tasks cannot be trained effectively.\n",
        "### üîπ **6. Reduces Bias**\n",
        "\n",
        "* A well-annotated dataset that covers diverse conditions (angles, lighting, occlusion, backgrounds) prevents the model from overfitting or becoming biased.\n",
        "* Inconsistent or incomplete annotation leads to biased predictions.\n",
        "‚úÖ **In summary:**\n",
        "Data annotation is important because it **provides the ground truth labels and object locations** that guide the model during training. High-quality, consistent annotation directly determines how well the object detection model will perform in the real world.\n"
      ],
      "metadata": {
        "id": "VZoxKlrLP9-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 What does batch size refer to in the context of model training?\n",
        "Good question üëç\n",
        "\n",
        "In the context of **model training**, especially in deep learning, **batch size** refers to:\n",
        "### üîπ **Definition**\n",
        "\n",
        "The **number of training samples (images, data points, etc.) processed by the model before updating its parameters (weights)**.\n",
        "\n",
        "* Data is usually too large to fit into memory all at once.\n",
        "* So, we split the dataset into **mini-batches**.\n",
        "* Each mini-batch goes through **forward pass ‚Üí loss computation ‚Üí backward pass ‚Üí parameter update**\n",
        "### üîπ **Example**\n",
        "\n",
        "Suppose:\n",
        "\n",
        "* You have **10,000 training images**.\n",
        "* You choose a **batch size of 32**.\n",
        "\n",
        "‚û°Ô∏è The model will:\n",
        "\n",
        "1. Take 32 images ‚Üí compute predictions & loss\n",
        "2. Update weights (one optimization step)\n",
        "3. Repeat for the next 32 images\n",
        "\n",
        "It will take **10,000 √∑ 32 ‚âà 312 steps** to finish **1 epoch** (one pass through the dataset).\n",
        "### üîπ **Types**\n",
        "\n",
        "* **Batch Gradient Descent (Batch size = full dataset)**\n",
        "\n",
        "  * Updates weights once per epoch.\n",
        "  * Very stable but slow and memory-heavy.\n",
        "\n",
        "* **Stochastic Gradient Descent (Batch size = 1)**\n",
        "\n",
        "  * Updates weights after every single sample.\n",
        "  * Fast updates, but very noisy training.\n",
        "\n",
        "* **Mini-Batch Gradient Descent (Batch size = between 2 and a few hundreds)**\n",
        "\n",
        "  * Most common in practice (e.g., 16, 32, 64, 128).\n",
        "  * Balances efficiency and stability.\n",
        "### üîπ **Effect of Batch Size**\n",
        "\n",
        "1. **Small Batch Size**\n",
        "\n",
        "   * More noisy updates ‚Üí better generalization sometimes.\n",
        "   * Slower training (more updates per epoch).\n",
        "   * Uses less memory.\n",
        "\n",
        "2. **Large Batch Size**\n",
        "\n",
        "   * Smoother gradient updates ‚Üí faster convergence.\n",
        "   * Needs more GPU memory.\n",
        "   * Risk of poorer generalization if too large.\n",
        "‚úÖ **In summary:**\n",
        "**Batch size = number of samples processed before one weight update.**\n",
        "It‚Äôs a key hyperparameter that affects training speed, memory usage, and model generalization.\n"
      ],
      "metadata": {
        "id": "61ay8JNxP97r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 What is the purpose of pretrained weights in object detection models ?\n",
        "Great question üëç\n",
        "\n",
        "In **object detection models**, **pretrained weights** are model parameters that were **already trained on a large dataset** (like **ImageNet** for classification or **COCO** for detection/segmentation) before being used for your specific task.\n",
        "## üîπ **Purpose of Pretrained Weights**\n",
        "\n",
        "### 1. **Transfer Learning**\n",
        "\n",
        "* Pretrained weights act as a **starting point** instead of training a model from scratch.\n",
        "* The model has already learned **generic features** (edges, textures, shapes, patterns).\n",
        "* You only need to fine-tune it on your **specific dataset**, saving time and compute.\n",
        "### 2. **Faster Convergence**\n",
        "\n",
        "* Training from scratch requires millions of images and huge compute.\n",
        "* Using pretrained weights allows the model to converge **much faster**, since it starts with useful feature representations.\n",
        "### 3. **Better Accuracy with Limited Data**\n",
        "\n",
        "* Object detection datasets are often small or domain-specific (e.g., medical images, traffic cameras).\n",
        "* Pretrained weights help achieve **higher accuracy** because the model already \"knows\" basic visual features.\n",
        "### 4. **Reduce Overfitting**\n",
        "\n",
        "* When training from scratch on a small dataset, the model can **overfit** quickly.\n",
        "* Pretrained weights provide a **regularizing effect**, improving generalization.\n",
        "### 5. **Domain Adaptation**\n",
        "\n",
        "* You can adapt a model pretrained on a **general dataset** (like COCO) to a **specialized domain** (like satellite imagery, medical scans, or manufacturing defects).\n",
        "* Only the final layers are retrained, while the backbone remains largely intact.\n",
        "## üîπ **Where Pretrained Weights Are Used in Object Detection**\n",
        "\n",
        "1. **Backbone networks** (e.g., ResNet, VGG, Swin Transformer) ‚Üí Initialized with pretrained weights from ImageNet.\n",
        "2. **Detection heads** (e.g., Faster R-CNN, YOLO, RetinaNet) ‚Üí Often initialized from COCO-pretrained models.\n",
        "‚úÖ **In summary:**\n",
        "The purpose of pretrained weights in object detection models is to **transfer knowledge** from large, general datasets to new tasks, enabling **faster training, higher accuracy, and better generalization**‚Äîespecially when your dataset is small or limited.\n"
      ],
      "metadata": {
        "id": "XcxXaYIHP94o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 How can you verify that Detectron2 was installed correctly ?\n",
        "Good one üëç\n",
        "When you install **Detectron2**, you want to make sure it‚Äôs properly compiled and working with your PyTorch setup. Here are the standard steps to **verify that Detectron2 was installed correctly**:\n",
        "## üîπ **1. Import Detectron2 in Python**\n",
        "\n",
        "Open a Python shell (or Jupyter Notebook) and try:\n",
        "\n",
        "```python\n",
        "import detectron2\n",
        "print(\"Detectron2 is installed correctly!\")\n",
        "```\n",
        "\n",
        "If it runs without errors, the package is at least importable.\n",
        "## üîπ **2. Check Version**\n",
        "\n",
        "You can confirm the installed version:\n",
        "\n",
        "```python\n",
        "import detectron2\n",
        "print(detectron2.__version__)\n",
        "```\n",
        "## üîπ **3. Run a Simple Detectron2 Test**\n",
        "\n",
        "Detectron2 has a built-in test script to ensure things are working:\n",
        "\n",
        "```bash\n",
        "python -m detectron2.utils.collect_env\n",
        "```\n",
        "\n",
        "üëâ This command prints a **system configuration report**, including:\n",
        "\n",
        "* Python version\n",
        "* PyTorch version & CUDA availability\n",
        "* Detectron2 version\n",
        "* GPU details\n",
        "\n",
        "If everything is compatible, you should see no error messages.\n",
        "## üîπ **4. Run a Quick Inference Demo**\n",
        "\n",
        "Try running an inference example with a pretrained model:\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "# Load config and pretrained model\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.MODEL.DEVICE = \"cuda\"  # or \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# Test on a random image\n",
        "im = cv2.imread(\"input.jpg\")  # replace with any image path\n",
        "outputs = predictor(im)\n",
        "\n",
        "print(outputs)  # should show bounding boxes, scores, classes\n",
        "```\n",
        "\n",
        "üëâ If this runs successfully and prints detection results, Detectron2 is working fine.\n",
        "## üîπ **5. Run Unit Tests (Optional)**\n",
        "\n",
        "You can also run Detectron2‚Äôs built-in tests (if installed from source):\n",
        "\n",
        "```bash\n",
        "python -m unittest discover -v detectron2/tests\n",
        "```\n",
        "‚úÖ **In summary:**\n",
        "To verify Detectron2 installation:\n",
        "\n",
        "1. Import the library\n",
        "2. Run `collect_env`\n",
        "3. Do a quick inference with a pretrained model\n"
      ],
      "metadata": {
        "id": "29F-_HfHP915"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 What is TFOD2, and why is it widely used ?\n",
        "Great question üëå\n",
        "## üîπ **What is TFOD2?**\n",
        "\n",
        "**TFOD2** stands for **TensorFlow Object Detection API v2**.\n",
        "It is an **open-source framework** built on top of **TensorFlow 2.x** for building, training, and deploying **object detection models**.\n",
        "\n",
        "It comes from Google‚Äôs TensorFlow team and is designed to make object detection **easier, faster, and more modular**.\n",
        "## üîπ **Key Features**\n",
        "\n",
        "1. ‚úÖ **Pretrained Models (Model Zoo)**\n",
        "\n",
        "   * Provides a wide range of **pretrained models** (SSD, Faster R-CNN, EfficientDet, CenterNet, Mask R-CNN, etc.) trained on datasets like **COCO, Open Images, KITTI**.\n",
        "   * Users can fine-tune these models instead of training from scratch.\n",
        "\n",
        "2. ‚úÖ **Multiple Tasks Supported**\n",
        "\n",
        "   * Object detection\n",
        "   * Instance segmentation\n",
        "   * Keypoint detection (pose estimation)\n",
        "   * Tracking\n",
        "\n",
        "3. ‚úÖ **Easy Pipeline Configuration**\n",
        "\n",
        "   * Uses **config files** to define dataset, preprocessing, augmentation, model architecture, training parameters, and evaluation setup.\n",
        "\n",
        "4. ‚úÖ **Scalability**\n",
        "\n",
        "   * Works on **CPUs, GPUs, and TPUs**.\n",
        "   * Can train models at **research scale** or on small datasets.\n",
        "\n",
        "5. ‚úÖ **Deployment Ready**\n",
        "\n",
        "   * Supports exporting trained models to **TensorFlow Lite (TFLite)**, **TensorFlow\\.js**, and **TensorFlow Serving** for deployment on mobile, edge, and web.\n",
        "## üîπ **Why is TFOD2 Widely Used?**\n",
        "\n",
        "1. **Ease of Use**\n",
        "\n",
        "   * Provides **end-to-end workflows**: dataset preparation ‚Üí training ‚Üí evaluation ‚Üí deployment.\n",
        "   * Minimal coding needed; lots of functionality is handled by configs.\n",
        "\n",
        "2. **Strong Community & Documentation**\n",
        "\n",
        "   * Backed by Google, widely adopted in academia and industry.\n",
        "   * Large community, tutorials, and GitHub issues/solutions.\n",
        "\n",
        "3. **Extensive Model Zoo**\n",
        "\n",
        "   * Users can pick models based on trade-offs (speed vs accuracy).\n",
        "   * Example: SSD MobileNet (fast, lightweight) vs EfficientDet (high accuracy).\n",
        "\n",
        "4. **Transfer Learning Made Easy**\n",
        "\n",
        "   * Fine-tuning pretrained models on custom datasets is straightforward.\n",
        "   * Saves huge amounts of training time and compute.\n",
        "\n",
        "5. **Integration with TensorFlow Ecosystem**\n",
        "\n",
        "   * Works seamlessly with **TensorFlow Hub**, **Keras**, **TensorFlow Lite**, and **TensorFlow Extended (TFX)**.\n",
        "   * Easy to deploy across cloud and edge devices.\n",
        "‚úÖ **In summary:**\n",
        "**TFOD2 (TensorFlow Object Detection API v2)** is a powerful, flexible, and easy-to-use framework for object detection, segmentation, and related tasks. It‚Äôs widely used because of its **pretrained models, scalability, transfer learning support, and deployment readiness** across multiple platforms.\n"
      ],
      "metadata": {
        "id": "wF5zeQylP9y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 How does learning rate affect model training in Detectron2 ?\n",
        "Great question üëç ‚Äî this is one of the most important hyperparameters in **Detectron2** (and deep learning in general).\n",
        "# üîπ **Learning Rate (LR) in Model Training**\n",
        "\n",
        "The **learning rate** controls **how much the model‚Äôs weights are updated** during backpropagation after each batch.\n",
        "\n",
        "* A **high learning rate** ‚Üí big steps in weight updates.\n",
        "* A **low learning rate** ‚Üí small, gradual steps.\n",
        "# üîπ **Effect of Learning Rate in Detectron2**\n",
        "\n",
        "### ‚úÖ 1. **Too High Learning Rate**\n",
        "\n",
        "* Model updates weights too aggressively.\n",
        "* Training loss may **fluctuate a lot** or even **diverge (explode)**.\n",
        "* In Detectron2 logs, you‚Äôll see loss values jumping up and down instead of decreasing smoothly.\n",
        "### ‚úÖ 2. **Too Low Learning Rate**\n",
        "\n",
        "* Model updates weights too slowly.\n",
        "* Training converges **very slowly** or gets stuck in a poor local minimum.\n",
        "* Detectron2 training may look like it‚Äôs not improving even after many iterations.\n",
        "### ‚úÖ 3. **Optimal Learning Rate**\n",
        "\n",
        "* Strikes a balance between speed and stability.\n",
        "* Loss decreases steadily without exploding or flattening too early.\n",
        "* In Detectron2, you‚Äôll typically see a **smooth downward curve in training loss** and improving validation metrics.\n",
        "# üîπ **Learning Rate in Detectron2 Config**\n",
        "\n",
        "In Detectron2, learning rate is set in the config file:\n",
        "\n",
        "```python\n",
        "cfg.SOLVER.BASE_LR = 0.001  # starting learning rate\n",
        "cfg.SOLVER.MAX_ITER = 10000 # total iterations\n",
        "cfg.SOLVER.STEPS = (3000, 6000) # LR decay steps\n",
        "cfg.SOLVER.GAMMA = 0.1  # factor to reduce LR at decay steps\n",
        "```\n",
        "\n",
        "* `BASE_LR` ‚Üí Initial learning rate\n",
        "* `STEPS` + `GAMMA` ‚Üí Learning rate schedule (decays LR during training)\n",
        "# üîπ **Learning Rate Scheduling in Detectron2**\n",
        "\n",
        "Detectron2 supports different schedules to **adjust LR during training**:\n",
        "\n",
        "* **StepLR** ‚Üí Drops LR by `GAMMA` at specified `STEPS`.\n",
        "* **Warmup** ‚Üí Starts with a small LR and gradually increases to `BASE_LR` (prevents unstable training at the start).\n",
        "* **Cosine Annealing / Polynomial decay** (customizable) ‚Üí Smoothly reduces LR over time.\n",
        "# üîπ **Rule of Thumb in Detectron2**\n",
        "\n",
        "* If **loss explodes or oscillates** ‚Üí Lower LR.\n",
        "* If **training is too slow or stuck** ‚Üí Increase LR.\n",
        "* Batch size also affects LR ‚Üí in practice, **larger batch sizes allow higher learning rates**.\n",
        "‚úÖ **In summary:**\n",
        "The learning rate in Detectron2 directly controls the **speed and stability** of training.\n",
        "\n",
        "* Too high ‚Üí unstable/diverging training.\n",
        "* Too low ‚Üí slow or stuck training.\n",
        "* Optimal LR with proper scheduling ‚Üí faster convergence and better accuracy.\n"
      ],
      "metadata": {
        "id": "7QtgsJpxR0Kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8 Why might Detectron2 use PyTorch as its backend framework?\n",
        "Detectron2 uses **PyTorch as its backend** for several important reasons that make it ideal for modern object detection and segmentation tasks. Here‚Äôs a detailed explanation:\n",
        "## üîπ **1. Dynamic Computation Graphs**\n",
        "\n",
        "* PyTorch uses **dynamic (eager) computation graphs**, unlike TensorFlow 1.x which used static graphs.\n",
        "* This allows:\n",
        "\n",
        "  * **Easy debugging** with standard Python tools (`print`, `pdb`).\n",
        "  * Flexible model architectures that can **change at runtime**.\n",
        "* Detectron2 often requires **customizable models** (different backbones, heads, or RPNs), which PyTorch handles naturally.\n",
        "## üîπ **2. Strong GPU Acceleration**\n",
        "\n",
        "* PyTorch has **efficient CUDA support** for NVIDIA GPUs.\n",
        "* Detectron2 can leverage GPUs for **fast training of large models** like Faster R-CNN, Mask R-CNN, or RetinaNet.\n",
        "* It also supports multi-GPU training with **Distributed Data Parallel (DDP)**.\n",
        "## üîπ **3. Pythonic & Intuitive API**\n",
        "\n",
        "* PyTorch feels like **native Python**, which is easier for researchers and engineers.\n",
        "* Detectron2‚Äôs design emphasizes **modular, readable code** for:\n",
        "\n",
        "  * Backbones\n",
        "  * RPNs (Region Proposal Networks)\n",
        "  * ROI heads\n",
        "  * Training loops\n",
        "* Easy integration with other Python libraries (NumPy, OpenCV, PIL).\n",
        "## üîπ **4. Strong Community & Ecosystem**\n",
        "\n",
        "* PyTorch has a **large research and developer community**.\n",
        "* Pretrained models, tutorials, and extensions are widely available.\n",
        "* Detectron2 benefits from PyTorch ecosystem tools:\n",
        "\n",
        "  * **Torchvision** (models & datasets)\n",
        "  * **TorchMetrics**\n",
        "  * **TorchScript** for deployment\n",
        "## üîπ **5. Flexibility for Research & Production**\n",
        "\n",
        "* Researchers can **quickly experiment** with new architectures (e.g., transformers, novel RPNs).\n",
        "* Production engineers can **deploy PyTorch models** using TorchScript or ONNX for real-time applications.\n",
        "## üîπ **6. Easy Integration with Autograd**\n",
        "\n",
        "* PyTorch has **automatic differentiation** (autograd) built-in.\n",
        "* Detectron2 relies heavily on gradient computations for **backpropagation in complex models**.\n",
        "* Custom loss functions or ROI operations are easy to implement with autograd.\n",
        "‚úÖ **In summary:**\n",
        "Detectron2 uses PyTorch because it provides **dynamic computation graphs, GPU acceleration, Pythonic APIs, strong community support, and flexible research-to-production workflow**.\n"
      ],
      "metadata": {
        "id": "a-wEQKZ7SZHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9  What types of pretrained models does TFOD2 support ?\n",
        "TensorFlow Object Detection API v2 (**TFOD2**) supports a wide range of **pretrained models** designed for different speed-accuracy trade-offs, tasks, and deployment scenarios. These pretrained models are available in the **TFOD2 Model Zoo**.\n",
        "\n",
        "Here‚Äôs a detailed breakdown:\n",
        "## üîπ **1. Single Shot Detectors (SSD)**\n",
        "\n",
        "* **Purpose:** Lightweight, fast models for real-time detection.\n",
        "* **Characteristics:** Moderate accuracy, high inference speed, low memory footprint.\n",
        "* **Common Variants:**\n",
        "\n",
        "  * SSD MobileNet V2 / V3\n",
        "  * SSD Inception V2\n",
        "  * SSD ResNet50\n",
        "* **Use Case:** Mobile apps, real-time video processing, embedded devices.\n",
        "## üîπ **2. Faster R-CNN**\n",
        "\n",
        "* **Purpose:** Two-stage detector for higher accuracy.\n",
        "* **Characteristics:** Slower than SSD, but high precision.\n",
        "* **Common Variants:**\n",
        "\n",
        "  * Faster R-CNN with ResNet50, ResNet101\n",
        "  * Faster R-CNN with NAS backbone\n",
        "* **Use Case:** Applications where **accuracy is more important than speed**, e.g., autonomous driving, medical imaging.\n",
        "## üîπ **3. Mask R-CNN**\n",
        "\n",
        "* **Purpose:** Instance segmentation (detect objects **and** generate pixel-level masks).\n",
        "* **Characteristics:** Extends Faster R-CNN with a mask branch.\n",
        "* **Common Variants:**\n",
        "\n",
        "  * Mask R-CNN with ResNet50/101 + FPN\n",
        "* **Use Case:** Detect and segment individual objects, e.g., identifying each person in a crowd, industrial inspection.\n",
        "## üîπ **4. EfficientDet**\n",
        "\n",
        "* **Purpose:** High accuracy with optimized efficiency.\n",
        "* **Characteristics:** Scalable backbone with compound scaling (EfficientDet-D0 ‚Üí D7).\n",
        "* **Variants:** D0, D1, D2 ‚Ä¶ D7\n",
        "* **Use Case:** High-performance detection on various hardware with balanced speed and accuracy.\n",
        "## üîπ **5. CenterNet**\n",
        "\n",
        "* **Purpose:** Keypoint-based object detection (detect center points of objects).\n",
        "* **Characteristics:** Single-stage detector, can be faster than Faster R-CNN.\n",
        "* **Use Case:** Lightweight detection tasks, often in real-time pipelines.\n",
        "## üîπ **6. Other Specialized Models**\n",
        "\n",
        "* **RetinaNet:** Focuses on handling class imbalance with focal loss.\n",
        "* **Keypoint R-CNN:** Detects object keypoints (pose estimation).\n",
        "* **TFOD2 also supports Mask R-CNN with keypoints** and other custom variants.\n",
        "## üîπ **Why Use Pretrained Models in TFOD2?**\n",
        "\n",
        "1. **Transfer Learning:** Fine-tune on custom datasets.\n",
        "2. **Faster Convergence:** Already learned generic features.\n",
        "3. **Better Accuracy:** Especially on small datasets.\n",
        "‚úÖ **In summary:**\n",
        "TFOD2 supports pretrained models for **object detection and instance segmentation**, including:\n",
        "\n",
        "* **SSD (MobileNet, Inception, ResNet)** ‚Üí Fast & lightweight\n",
        "* **Faster R-CNN (ResNet, NAS)** ‚Üí Accurate\n",
        "* **Mask R-CNN (ResNet + FPN)** ‚Üí Instance segmentation\n",
        "* **EfficientDet (D0-D7)** ‚Üí Scalable & efficient\n",
        "* **CenterNet, RetinaNet, Keypoint R-CNN** ‚Üí Specialized tasks\n"
      ],
      "metadata": {
        "id": "LRy0Ivk4S182"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 How can data path errors impact Detectron2 ?\n",
        "Data path errors can have a **major impact** on Detectron2 training and inference. Since Detectron2 relies on **correctly structured datasets** and file paths, any misconfiguration can cause **training failures, crashes, or incorrect results**.\n",
        "\n",
        "Here‚Äôs a detailed breakdown:\n",
        "## üîπ **1. Dataset Not Found**\n",
        "\n",
        "* **Cause:** The path specified in the dataset registration or config file does not exist.\n",
        "* **Impact:** Detectron2 cannot load images or annotations ‚Üí raises `FileNotFoundError` or similar.\n",
        "* **Example:**\n",
        "\n",
        "```python\n",
        "DatasetCatalog.register(\"my_dataset\", lambda: load_coco_json(\"wrong_path/annotations.json\", \"wrong_path/images\"))\n",
        "```\n",
        "## üîπ **Incorrect Annotation Path**\n",
        "\n",
        "* **Cause:** Annotation file path is wrong or annotation format is invalid.\n",
        "* **Impact:** Training fails during dataset loading, or the model trains on **wrong or empty data**.\n",
        "* **Example:** Using Pascal VOC instead of COCO JSON without updating the loader.\n",
        "## üîπ **Misaligned Image and Annotation Files**\n",
        "\n",
        "* **Cause:** The image folder path and annotations don‚Äôt match.\n",
        "* **Impact:** Detectron2 may skip images, produce **empty batches**, or mismatch labels.\n",
        "* **Effect on Model:** The model may **fail to learn**, resulting in poor accuracy or NaN losses.\n",
        "## üîπ **Inference Failures**\n",
        "\n",
        "* **Cause:** During inference, the image path provided is incorrect.\n",
        "* **Impact:** Detectron2 cannot read the image ‚Üí cannot perform prediction.\n",
        "* **Error Message:** `cv2.imread(image_path) is None` or similar.\n",
        "## üîπ **Subtle Issues**\n",
        "\n",
        "* **Case sensitivity (Linux vs Windows):** `image.JPG` vs `image.jpg`.\n",
        "* **Relative vs absolute paths:** Using relative paths incorrectly may fail when running scripts from a different directory.\n",
        "* **Hidden spaces or typos** in folder names.\n",
        "## üîπ **Best Practices to Avoid Data Path Errors**\n",
        "\n",
        "1. **Always use absolute paths** in DatasetCatalog registration.\n",
        "2. **Check dataset structure** matches the loader (COCO, Pascal VOC, or custom).\n",
        "3. **Verify image readability** before training:\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "im = cv2.imread(\"/path/to/image.jpg\")\n",
        "assert im is not None, \"Image cannot be read!\"\n",
        "```\n",
        "\n",
        "4. **Test dataset registration** before training:\n",
        "\n",
        "```python\n",
        "from detectron2.data import DatasetCatalog\n",
        "data = DatasetCatalog.get(\"my_dataset\")\n",
        "print(len(data))  # Should match your number of images\n",
        "``\n",
        "5. **Use consistent naming and folder structure**.\n",
        "‚úÖ **In summary:**\n",
        "Data path errors in Detectron2 can lead to **dataset loading failures, mismatched labels, skipped images, training crashes, or incorrect model outputs**. Careful verification of **image paths, annotation paths, and dataset registration** is crucial to prevent these issues\n"
      ],
      "metadata": {
        "id": "gM41NSAvS15x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11 What is Detectron2 ?\n",
        "**Detectron2** is a **PyTorch-based open-source library** developed by Facebook AI Research (FAIR) for **state-of-the-art object detection, segmentation, and related computer vision tasks**. It is the **successor to the original Detectron** framework and is designed to be **modular, flexible, and highly efficient**.\n",
        "üîπ **Key Features**\n",
        "\n",
        "1. **Object Detection**\n",
        "\n",
        "   * Detects and localizes objects using **bounding boxes**.\n",
        "   * Supports models like **Faster R-CNN, RetinaNet, and YOLO-style detectors**.\n",
        "\n",
        "2. **Instance Segmentation**\n",
        "\n",
        "   * Detects objects **and provides pixel-level masks** for each instance.\n",
        "   * Example: Identifying each person in a crowd, not just drawing boxes.\n",
        "\n",
        "3. **Semantic Segmentation**\n",
        "\n",
        "   * Labels **every pixel** with a class but does not separate instances.\n",
        "\n",
        "4. **Panoptic Segmentation**\n",
        "\n",
        "   * Combines instance and semantic segmentation into a single output.\n",
        "\n",
        "5. **Keypoint Detection (Pose Estimation)**\n",
        "\n",
        "   * Detects keypoints on objects, such as human body joints.\n",
        "\n",
        "6. **DensePose**\n",
        "\n",
        "   * Maps human pixels to a 3D body surface for advanced applications.\n",
        "\n",
        "7. **Modular & Flexible**\n",
        "\n",
        "   * Easily swap backbones (ResNet, Swin Transformer, etc.), ROI heads, and RPNs.\n",
        "   * Supports custom datasets and tasks.\n",
        "\n",
        "8. **High Performance**\n",
        "\n",
        "   * Optimized for **GPU training and inference**, including multi-GPU setups.\n",
        "## üîπ **Why Detectron2 is Popular**\n",
        "\n",
        "* **Research-friendly:** Easy to experiment with new architectures.\n",
        "* **Production-ready:** Supports exporting models for deployment.\n",
        "* **Pretrained models:** Comes with a **Model Zoo** for COCO and other datasets.\n",
        "* **Integration with PyTorch:** Leverages PyTorch‚Äôs dynamic computation graphs, autograd, and GPU acceleration.\n",
        "‚úÖ **In short:**\n",
        "Detectron2 is a **cutting-edge framework for object detection and segmentation**, widely used in both **research** and **real-world applications** because of its **modularity, efficiency, and rich pretrained models**.\n"
      ],
      "metadata": {
        "id": "xAh815DxS128"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12 What are TFRecord files, and why are they used in TFOD2 ?\n",
        "Great question! In **TFOD2 (TensorFlow Object Detection API v2)**, **TFRecord files** are a core data format for training object detection models. Here‚Äôs a detailed explanation:\n",
        "## üîπ **What is a TFRecord File?**\n",
        "\n",
        "* **TFRecord** is a **binary file format** developed by TensorFlow for storing **sequences of serialized data**.\n",
        "\n",
        "* Each record in the file contains a **serialized `tf.train.Example` protobuf** that holds data like:\n",
        "\n",
        "  * Images (raw bytes)\n",
        "  * Labels (class IDs)\n",
        "  * Bounding box coordinates\n",
        "  * Other metadata (image height, width, filename, etc.)\n",
        "\n",
        "* Unlike raw image folders, TFRecords store **all data in a single or few files**, which is efficient for large datasets.\n",
        "## üîπ **Why TFRecord Files Are Used in TFOD2**\n",
        "\n",
        "### 1. **Efficient I/O**\n",
        "\n",
        "* Reading raw images one by one can be slow.\n",
        "* TFRecord files allow **sequential reading of serialized data**, which is **faster and optimized for TensorFlow‚Äôs input pipeline**.\n",
        "\n",
        "### 2. **Better for Large Datasets**\n",
        "\n",
        "* Large datasets (COCO, Open Images) may contain tens or hundreds of thousands of images.\n",
        "* Storing them in TFRecords reduces file system overhead and **improves training speed**.\n",
        "\n",
        "### 3. **Supports TensorFlow‚Äôs `tf.data` Pipeline**\n",
        "\n",
        "* TFRecords integrate seamlessly with **`tf.data.TFRecordDataset`**.\n",
        "* Enables:\n",
        "\n",
        "  * Efficient shuffling\n",
        "  * Prefetching\n",
        "  * Parallel reading\n",
        "* This improves **GPU utilization** during training.\n",
        "\n",
        "### 4. **Serialization and Portability**\n",
        "\n",
        "* Stores images, labels, and metadata in a **single portable file**.\n",
        "* Makes it easy to **share datasets** across systems without worrying about folder structure or filenames.\n",
        "\n",
        "### 5. **Consistency**\n",
        "\n",
        "* Ensures that each training example includes **all required fields** (image, labels, bounding boxes).\n",
        "* Reduces errors from missing or misnamed files.\n",
        "## üîπ **Typical TFRecord Structure in TFOD2**\n",
        "\n",
        "Each example usually contains:\n",
        "\n",
        "```text\n",
        "features = {\n",
        "    'image/encoded': bytes of the image,\n",
        "    'image/filename': filename string,\n",
        "    'image/height': int,\n",
        "    'image/width': int,\n",
        "    'image/object/bbox/xmin': float list,\n",
        "    'image/object/bbox/xmax': float list,\n",
        "    'image/object/bbox/ymin': float list,\n",
        "    'image/object/bbox/ymax': float list,\n",
        "    'image/object/class/text': string list,\n",
        "    'image/object/class/label': int list\n",
        "}\n",
        "```\n",
        "## üîπ **In short**\n",
        "\n",
        "TFRecord files are used in TFOD2 because they:\n",
        "\n",
        "* Enable **efficient and scalable reading** of large datasets\n",
        "* Integrate seamlessly with **TensorFlow pipelines**\n",
        "* Store images, labels, and bounding boxes in a **portable, serialized format**\n",
        "* Reduce I/O bottlenecks during training\n"
      ],
      "metadata": {
        "id": "Ow0rNpvpS1z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13 What evaluation metrics are typically used with Detectron2\n",
        "In **Detectron2**, evaluation metrics depend on the task (object detection, instance segmentation, keypoint detection, etc.), but they are mostly based on **COCO-style metrics**, which are widely used in computer vision benchmarks.\n",
        "\n",
        "Here‚Äôs a detailed breakdown:\n",
        "## üîπ **1. Object Detection Metrics**\n",
        "\n",
        "### **Mean Average Precision (mAP)**\n",
        "\n",
        "* **Definition:** Average precision across all classes and IoU thresholds.\n",
        "* **IoU Thresholds:** Commonly evaluated at 0.50 (PASCAL VOC style) and 0.50:0.95 (COCO style).\n",
        "* **Variants:**\n",
        "\n",
        "  * **AP@\\[0.5] (AP50):** IoU threshold 0.5\n",
        "  * **AP@\\[0.75] (AP75):** IoU threshold 0.75\n",
        "  * **AP (COCO):** Average over IoU thresholds 0.50 to 0.95 in steps of 0.05\n",
        "\n",
        "### **Average Recall (AR)**\n",
        "\n",
        "* Measures how many objects the model correctly detects, regardless of precision.\n",
        "* Evaluated with a fixed number of detections per image (e.g., 100).\n",
        "## üîπ **2. Instance Segmentation Metrics**\n",
        "\n",
        "* Uses the **same mAP/AR metrics** as object detection but applied to **masks** instead of bounding boxes.\n",
        "* Example: **APm (mask AP)**\n",
        "## üîπ **3. Keypoint Detection Metrics**\n",
        "\n",
        "* **OKS (Object Keypoint Similarity):** Measures similarity between predicted and ground-truth keypoints.\n",
        "* Metrics:\n",
        "\n",
        "  * **AP (OKS) @ 0.50** ‚Üí Similar to AP50\n",
        "  * **AP (OKS) @ 0.75**\n",
        "  * **AP (OKS) averaged over thresholds**\n",
        "## üîπ **4. Panoptic Segmentation Metrics**\n",
        "\n",
        "* **PQ (Panoptic Quality):** Combines segmentation quality (IoU) and detection quality (recognition).\n",
        "* **SQ (Segmentation Quality)** ‚Üí IoU of matched segments\n",
        "* **RQ (Recognition Quality)** ‚Üí Detection correctness\n",
        "## üîπ **5. Other Common Metrics**\n",
        "\n",
        "* **Precision / Recall:** Standard classification metrics at object level.\n",
        "* **F1-Score:** Harmonic mean of precision and recall.\n",
        "* **Confusion Matrix:** Useful for checking class-level errors.\n",
        "## üîπ **How Detectron2 Computes Metrics**\n",
        "\n",
        "* Built-in **`COCOEvaluator`** and **`DatasetEvaluator`** classes handle evaluation.\n",
        "* During evaluation, Detectron2:\n",
        "\n",
        "  1. Runs the model on the validation set.\n",
        "  2. Compares predictions with ground truth.\n",
        "  3. Computes mAP, AR, and other relevant metrics depending on the task.\n",
        "‚úÖ **In summary:**\n",
        "Detectron2 primarily uses **COCO-style evaluation metrics**, including:\n",
        "\n",
        "| Task                  | Main Metrics                           |\n",
        "| --------------------- | -------------------------------------- |\n",
        "| Object Detection      | mAP, AP50, AP75, AR                    |\n",
        "| Instance Segmentation | mAP (mask), AP50, AP75                 |\n",
        "| Keypoint Detection    | AP (OKS), AP50, AP75                   |\n",
        "| Panoptic Segmentation | PQ, SQ, RQ                             |\n",
        "| All tasks             | Precision, Recall, F1-score (optional) |\n"
      ],
      "metadata": {
        "id": "tKCClMVeS1w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14 How do you perform inference with a trained Detectron2 model ?\n",
        "Performing **inference with a trained Detectron2 model** is straightforward once the model is trained or a pretrained model is loaded. Here‚Äôs a detailed step-by-step guide:\n",
        "## **Step 1: Import Required Libraries**\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "import torch\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "## **Step 2: Load Model Configuration and Weights**\n",
        "\n",
        "You need to set up the **config file** and **weights** for your trained model:\n",
        "\n",
        "```python\n",
        "cfg = get_cfg()\n",
        "\n",
        "# Load a config from the model zoo (or your custom config)\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "\n",
        "# Set the path to your trained weights\n",
        "cfg.MODEL.WEIGHTS = \"path/to/your/model_final.pth\"\n",
        "\n",
        "# Set device: \"cuda\" or \"cpu\"\n",
        "cfg.MODEL.DEVICE = \"cuda\"  # or \"cpu\"\n",
        "\n",
        "# Confidence threshold for predictions\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "```\n",
        "## **Step 3: Create the Predictor**\n",
        "\n",
        "```python\n",
        "predictor = DefaultPredictor(cfg)\n",
        "```\n",
        "\n",
        "The `DefaultPredictor` handles **image preprocessing, model inference, and post-processing**.\n",
        "## **Step 4: Load an Image**\n",
        "\n",
        "```python\n",
        "image = cv2.imread(\"path/to/input_image.jpg\")\n",
        "```\n",
        "## **Step 5: Run Inference**\n",
        "\n",
        "```python\n",
        "outputs = predictor(image)\n",
        "print(outputs)\n",
        "```\n",
        "\n",
        "* `outputs` is a dictionary containing:\n",
        "\n",
        "  * `instances.pred_boxes` ‚Üí predicted bounding boxes\n",
        "  * `instances.pred_classes` ‚Üí predicted class IDs\n",
        "  * `instances.scores` ‚Üí confidence scores\n",
        "  * `instances.pred_masks` ‚Üí if using instance segmentation\n",
        "## **Step 6: Visualize the Results**\n",
        "\n",
        "```python\n",
        "# Get metadata for class names\n",
        "metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
        "\n",
        "# Visualize predictions\n",
        "v = Visualizer(image[:, :, ::-1], metadata=metadata, scale=1.2)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "# Show image\n",
        "cv2.imshow(\"Predictions\", out.get_image()[:, :, ::-1])\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "```\n",
        "## **Optional: Batch Inference**\n",
        "\n",
        "For multiple images, loop through the image folder:\n",
        "\n",
        "```python\n",
        "import os\n",
        "\n",
        "image_folder = \"path/to/images\"\n",
        "for img_file in os.listdir(image_folder):\n",
        "    img_path = os.path.join(image_folder, img_file)\n",
        "    image = cv2.imread(img_path)\n",
        "    outputs = predictor(image)\n",
        "    # visualize or save results\n",
        "```\n",
        "‚úÖ **In summary:**\n",
        "Performing inference in Detectron2 involves:\n",
        "\n",
        "1. Setting up the config and weights\n",
        "2. Creating a `DefaultPredictor`\n",
        "3. Loading the image\n",
        "4. Running `predictor(image)`\n",
        "5. Optionally visualizing the results\n"
      ],
      "metadata": {
        "id": "s4E_6JdnV1Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15 What does TFOD2 stand for, and what is it designed for ?\n",
        "**TFOD2** stands for **TensorFlow Object Detection API v2**.\n",
        "## **Purpose and Design**\n",
        "\n",
        "* It is an **open-source framework built on TensorFlow 2.x** for creating, training, and deploying **object detection models**.\n",
        "* TFOD2 is designed to make object detection tasks **easier, faster, and more modular** for researchers and developers.\n",
        "### **Key Design Goals**\n",
        "\n",
        "1. **Pretrained Models**\n",
        "\n",
        "   * Provides a **Model Zoo** with SSD, Faster R-CNN, Mask R-CNN, EfficientDet, and other models pretrained on datasets like COCO.\n",
        "   * Enables **transfer learning** for custom datasets.\n",
        "\n",
        "2. **Ease of Use**\n",
        "\n",
        "   * Uses **config files** to define dataset paths, model architecture, training parameters, and evaluation metrics.\n",
        "   * Minimizes boilerplate coding for training pipelines.\n",
        "\n",
        "3. **Scalability**\n",
        "\n",
        "   * Supports training on **CPU, GPU, or TPU**, from small custom datasets to large-scale benchmarks.\n",
        "\n",
        "4. **Deployment Ready**\n",
        "\n",
        "   * Trained models can be exported for **TensorFlow Lite, TensorFlow\\.js, or TensorFlow Serving**.\n",
        "   * Makes real-time and mobile deployment easier.\n",
        "\n",
        "5. **Multiple Task Support**\n",
        "\n",
        "   * Object detection (bounding boxes)\n",
        "   * Instance segmentation (masks)\n",
        "   * Keypoint detection (pose estimation)\n",
        "‚úÖ **In short:**\n",
        "TFOD2 is designed for **building, training, and deploying object detection models efficiently**, providing **pretrained models, configurable pipelines, and easy integration with TensorFlow‚Äôs ecosystem**\n"
      ],
      "metadata": {
        "id": "dMlDN4yMWUfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 16 What does fine-tuning pretrained weights involve ?\n",
        "**Fine-tuning pretrained weights** is a common strategy in deep learning, especially in tasks like **object detection**, to adapt a model trained on one dataset to a new dataset or task. Here‚Äôs a detailed explanation:\n",
        "## üîπ **What Fine-Tuning Means**\n",
        "\n",
        "* A **pretrained model** has already learned general features from a large dataset (like **ImageNet** or **COCO**).\n",
        "* **Fine-tuning** involves taking this pretrained model and **continuing training on a new dataset**, usually with:\n",
        "\n",
        "  * A smaller learning rate\n",
        "  * A dataset specific to your problem\n",
        "\n",
        "The goal is to **transfer the learned knowledge** (like edges, textures, object shapes) to your custom task without starting from scratch.\n",
        "## üîπ **Steps Involved in Fine-Tuning**\n",
        "\n",
        "### 1. **Load Pretrained Weights**\n",
        "\n",
        "* Initialize your model with weights from a pretrained network instead of random initialization.\n",
        "* Example (Detectron2):\n",
        "\n",
        "```python\n",
        "cfg.MODEL.WEIGHTS = \"path/to/pretrained_model.pth\"\n",
        "```\n",
        "\n",
        "### 2. **Adjust the Model for Your Task**\n",
        "\n",
        "* Replace the **head layers** to match the number of classes in your dataset.\n",
        "\n",
        "  * For object detection: change the ROI head to output your number of object classes.\n",
        "\n",
        "### 3. **Set Learning Rate Appropriately**\n",
        "\n",
        "* Use a **smaller learning rate** for the pretrained layers.\n",
        "* Optionally, set a **higher learning rate** for newly added layers.\n",
        "\n",
        "### 4. **Train on Your Dataset**\n",
        "\n",
        "* Continue training the model using your dataset.\n",
        "* The pretrained layers gradually adapt to your specific data, while retaining useful general features.\n",
        "\n",
        "### 5. **Monitor Performance**\n",
        "\n",
        "* Evaluate on validation set to ensure:\n",
        "\n",
        "  * The model is learning your dataset\n",
        "  * Not overfitting\n",
        "## üîπ **Why Fine-Tuning is Useful**\n",
        "\n",
        "1. **Faster Training**\n",
        "\n",
        "   * The model already knows general features ‚Üí fewer epochs needed.\n",
        "\n",
        "2. **Better Accuracy**\n",
        "\n",
        "   * Especially useful for **small datasets** where training from scratch may fail.\n",
        "\n",
        "3. **Efficient Use of Resources**\n",
        "\n",
        "   * Saves computational cost compared to training a large model from scratch.\n",
        "\n",
        "4. **Domain Adaptation**\n",
        "\n",
        "   * Adapts a general model (e.g., COCO-trained) to a specific domain like medical imaging, traffic cameras, or satellite images.\n",
        "‚úÖ **In short:**\n",
        "Fine-tuning pretrained weights involves **starting from a model that already knows general features** and continuing training on a new dataset, typically adjusting the output layers and using a smaller learning rate to adapt the model to the new task efficiently.\n"
      ],
      "metadata": {
        "id": "9aivR77gWUb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 17 How is training started in TFOD2 ?\n",
        "\n",
        "Training in **TFOD2 (TensorFlow Object Detection API v2)** involves a few key steps, from preparing your dataset to running the training script. Here‚Äôs a detailed breakdown:\n",
        " **Step 1: Prepare Dataset**\n",
        "\n",
        "1. **Organize your images** into a folder structure (or use TFRecords).\n",
        "2. **Create annotations**:\n",
        "\n",
        "   * For TFOD2, annotations are typically in **TFRecord format**.\n",
        "   * Include fields like image bytes, bounding boxes, class labels, and optional masks.\n",
        "3. **Label map file**:\n",
        "\n",
        "   * Maps class names to integer IDs. Example `label_map.pbtxt`:\n",
        "\n",
        "   ```text\n",
        "   item {\n",
        "       id: 1\n",
        "       name: 'cat'\n",
        "   }\n",
        "   item {\n",
        "       id: 2\n",
        "       name: 'dog'\n",
        "   }\n",
        "   ```\n",
        "## **Step 2: Choose a Model**\n",
        "\n",
        "* Select a **pretrained model** from the **TFOD2 Model Zoo** (e.g., SSD MobileNet, Faster R-CNN, EfficientDet).\n",
        "* Download the checkpoint or use the model zoo URL in the config file.\n",
        "## **Step 3: Configure Training**\n",
        "\n",
        "1. Copy a **pipeline config file** from the model zoo.\n",
        "2. Edit important parameters:\n",
        "\n",
        "   * `model` ‚Üí model architecture\n",
        "   * `train_config.batch_size` ‚Üí batch size\n",
        "   * `train_config.fine_tune_checkpoint` ‚Üí path to pretrained weights\n",
        "   * `train_input_reader` and `eval_input_reader` ‚Üí paths to TFRecords\n",
        "   * `num_classes` ‚Üí number of object classes\n",
        "   * Learning rate, number of steps, and optimizer settings\n",
        "\n",
        "Example snippet:\n",
        "\n",
        "```text\n",
        "train_config: {\n",
        "  batch_size: 4\n",
        "  fine_tune_checkpoint: \"path/to/pretrained_model.ckpt\"\n",
        "  num_steps: 10000\n",
        "  optimizer { momentum_optimizer { learning_rate { ... } } }\n",
        "}\n",
        "```\n",
        "## **Step 4: Start Training**\n",
        "\n",
        "* Use the **`model_main_tf2.py`** script provided by TFOD2:\n",
        "\n",
        "```bash\n",
        "python model_main_tf2.py \\\n",
        "    --pipeline_config_path=path/to/pipeline.config \\\n",
        "    --model_dir=training/ \\\n",
        "    --alsologtostderr\n",
        "```\n",
        "\n",
        "* Parameters:\n",
        "\n",
        "  * `pipeline_config_path`: Your config file path\n",
        "  * `model_dir`: Directory where checkpoints, logs, and summaries are saved\n",
        "  * `--alsologtostderr`: Prints logs to console\n",
        "## **Step 5: Monitor Training**\n",
        "\n",
        "1. **Check TensorBoard logs**:\n",
        "\n",
        "```bash\n",
        "tensorboard --logdir=training/\n",
        "```\n",
        "\n",
        "2. Monitor metrics like **loss, learning rate, and mAP**.\n",
        "3. Adjust learning rate or batch size if training is unstable.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 6: Export the Trained Model**\n",
        "\n",
        "Once training is complete, export the model for inference:\n",
        "\n",
        "```bash\n",
        "python exporter_main_v2.py \\\n",
        "    --input_type image_tensor \\\n",
        "    --pipeline_config_path path/to/pipeline.config \\\n",
        "    --trained_checkpoint_dir training/ \\\n",
        "    --output_directory exported_model/\n",
        "```\n",
        "\n",
        "* Outputs a **saved\\_model** folder ready for inference.\n",
        "‚úÖ **In short:**\n",
        "Training in TFOD2 involves:\n",
        "\n",
        "1. Preparing your dataset in **TFRecord** format with a label map.\n",
        "2. Choosing a pretrained model and copying its **pipeline config**.\n",
        "3. Editing config parameters for your dataset and training schedule.\n",
        "4. Running `model_main_tf2.py` to start training.\n",
        "5. Monitoring with TensorBoard and exporting the final model for inference."
      ],
      "metadata": {
        "id": "i_j9STMmWUYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 18 What does COCO format represent, and why is it popular in Detectron2 ?\n",
        "**COCO format** is a widely used **dataset annotation format** in computer vision, especially for object detection, instance segmentation, and keypoint detection tasks. It is popular in **Detectron2** because it is standardized, flexible, and compatible with many pretrained models.\n",
        "## üîπ **What COCO Format Represents**\n",
        "\n",
        "COCO stands for **Common Objects in Context**. In the context of annotations:\n",
        "\n",
        "1. **JSON File Structure**\n",
        "   A COCO dataset stores annotations in a **single JSON file** containing several key sections:\n",
        "\n",
        "   * `images`: Information about each image (file name, height, width, image ID)\n",
        "   * `annotations`: Object-level annotations per image:\n",
        "\n",
        "     * `bbox` ‚Üí bounding box coordinates `[x, y, width, height]`\n",
        "     * `category_id` ‚Üí class label\n",
        "     * `segmentation` ‚Üí polygon mask for instance segmentation (optional)\n",
        "     * `keypoints` ‚Üí for human pose estimation (optional)\n",
        "   * `categories`: List of class names and their IDs\n",
        "\n",
        "Example snippet:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"images\": [{\"id\": 1, \"file_name\": \"img1.jpg\", \"height\": 600, \"width\": 800}],\n",
        "  \"annotations\": [\n",
        "    {\"id\": 1, \"image_id\": 1, \"category_id\": 3, \"bbox\": [100, 150, 50, 80], \"area\": 4000, \"iscrowd\": 0}\n",
        "  ],\n",
        "  \"categories\": [{\"id\": 3, \"name\": \"cat\"}]\n",
        "}\n",
        "```\n",
        "\n",
        "2. **Flexible for Different Tasks**\n",
        "\n",
        "* **Object Detection:** Uses `bbox` and `category_id`\n",
        "* **Instance Segmentation:** Uses `segmentation` polygons\n",
        "* **Keypoint Detection:** Uses `keypoints` and `num_keypoints`\n",
        "## üîπ **Why COCO Format is Popular in Detectron2**\n",
        "\n",
        "1. **Standardized and Compatible**\n",
        "\n",
        "   * Detectron2‚Äôs **`COCOEvaluator`** and dataset loaders work natively with COCO format.\n",
        "   * Many pretrained models in Detectron2 Model Zoo are trained on COCO.\n",
        "\n",
        "2. **Supports Multiple Tasks**\n",
        "\n",
        "   * Can handle detection, segmentation, and keypoints in a **single JSON file**.\n",
        "   * Reduces the need for multiple dataset formats.\n",
        "\n",
        "3. **Easier Transfer Learning**\n",
        "\n",
        "   * Custom datasets can be converted to COCO format to **reuse pretrained COCO models**.\n",
        "\n",
        "4. **Widely Used Benchmark**\n",
        "\n",
        "   * COCO dataset is a standard benchmark in research, so using its format aligns with **best practices and evaluation metrics (AP, AR)**.\n",
        "\n",
        "5. **Flexible and Extensible**\n",
        "\n",
        "   * Allows adding new fields, e.g., ‚Äúiscrowd‚Äù or custom metadata, without breaking compatibility.\n",
        "‚úÖ **In short:**\n",
        "The **COCO format** represents a **JSON-based structured annotation format** with images, object annotations (bounding boxes, masks, keypoints), and class categories. It is popular in Detectron2 because it is **standardized, flexible, compatible with pretrained models, and supports multiple computer vision tasks**.\n"
      ],
      "metadata": {
        "id": "KwEckNBCWUVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 19 Why is evaluation curve plotting important in Detectron2 ?\n",
        "Plotting **evaluation curves** in Detectron2 is an essential step for **monitoring and understanding model training and performance**. It helps you make informed decisions about hyperparameters, training duration, and potential issues. Here‚Äôs a detailed explanation:\n",
        "## üîπ **1. Track Training Progress**\n",
        "\n",
        "* **Loss Curves**: Show how the model‚Äôs training and validation loss change over time (iterations or epochs).\n",
        "\n",
        "  * Helps verify if the model is **converging**.\n",
        "  * Detects issues like **diverging loss** or **stagnation**.\n",
        "\n",
        "* **Metric Curves**: For example, mAP (mean Average Precision) or AR (Average Recall) over iterations.\n",
        "\n",
        "  * Helps monitor improvements in model **accuracy and generalization**.\n",
        "## üîπ **2. Detect Overfitting or Underfitting**\n",
        "\n",
        "* **Overfitting**: Training loss decreases but validation loss stagnates or increases.\n",
        "* **Underfitting**: Both training and validation loss remain high.\n",
        "* **Solution**: Adjust learning rate, batch size, regularization, or dataset size.\n",
        "\n",
        "Plotting curves makes these issues **immediately visible**.\n",
        "## üîπ **3. Compare Hyperparameter Settings**\n",
        "\n",
        "* By plotting curves for different configurations (learning rate, batch size, optimizer), you can **visually compare which setup works best**.\n",
        "* Saves time instead of relying solely on final metrics.\n",
        "## üîπ **4. Identify Training Instabilities**\n",
        "\n",
        "* Spikes or fluctuations in loss curves can indicate:\n",
        "\n",
        "  * Learning rate too high\n",
        "  * Batch size too small\n",
        "  * Data issues (incorrect labels, path errors)\n",
        "* Early detection avoids wasting compute time.\n",
        "## üîπ **5. Monitor Evaluation Metrics**\n",
        "\n",
        "* Detectron2 tracks **COCO-style metrics** like mAP, AP50, AP75, etc.\n",
        "* Plotting them over training steps shows **how performance improves**, not just the final result.\n",
        "* Helps decide **when to stop training** (early stopping) or **adjust learning rate schedules**.\n",
        "## üîπ **6. Facilitate Reporting and Analysis**\n",
        "\n",
        "* Curves are essential for **research papers, presentations, and reports**.\n",
        "* Visualizing metrics provides **intuition about model behavior** that numbers alone cannot convey.\n",
        "### **Summary**\n",
        "\n",
        "Evaluation curve plotting in Detectron2 is important because it allows you to:\n",
        "\n",
        "1. Track training and validation progress\n",
        "2. Detect overfitting or underfitting\n",
        "3. Compare hyperparameter settings\n",
        "4. Identify instabilities in training\n",
        "5. Monitor evaluation metrics over time\n",
        "6. Aid in reporting and analysis.\n"
      ],
      "metadata": {
        "id": "NDFzsIv4Y4l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 20 How do you configure data paths in TFOD2 ?\n",
        "Configuring **data paths in TFOD2 (TensorFlow Object Detection API v2)** is an essential step to make sure the model can correctly read your training and evaluation datasets. Here‚Äôs a detailed guide:\n",
        "## **1. Prepare Your Dataset**\n",
        "\n",
        "1. **Organize Images**\n",
        "\n",
        "   * Separate **training** and **evaluation** images into folders:\n",
        "\n",
        "     ```\n",
        "     dataset/\n",
        "       train/\n",
        "         img1.jpg\n",
        "         img2.jpg\n",
        "       val/\n",
        "         img1.jpg\n",
        "         img2.jpg\n",
        "     ```\n",
        "2. **Create TFRecord Files**\n",
        "\n",
        "   * Convert images and annotations (bounding boxes, class labels) into **TFRecord format**:\n",
        "\n",
        "     ```bash\n",
        "     python create_tf_record.py \\\n",
        "       --label_map_path=label_map.pbtxt \\\n",
        "       --data_dir=dataset/train \\\n",
        "       --output_path=train.record\n",
        "     ```\n",
        "\n",
        "     ```bash\n",
        "     python create_tf_record.py \\\n",
        "       --label_map_path=label_map.pbtxt \\\n",
        "       --data_dir=dataset/val \\\n",
        "       --output_path=val.record\n",
        "     ```\n",
        "## **2. Create a Label Map**\n",
        "\n",
        "* The label map maps **class names to integer IDs** in a `.pbtxt` file:\n",
        "\n",
        "```text\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'cat'\n",
        "}\n",
        "item {\n",
        "  id: 2\n",
        "  name: 'dog'\n",
        "}\n",
        "```\n",
        "## **3. Edit the Pipeline Configuration File**\n",
        "\n",
        "TFOD2 uses a **pipeline config file** to define the model, training parameters, and data paths.\n",
        "\n",
        "### **Key Sections to Configure**\n",
        "\n",
        "```text\n",
        "train_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"path/to/train.record\"\n",
        "  }\n",
        "  label_map_path: \"path/to/label_map.pbtxt\"\n",
        "}\n",
        "\n",
        "eval_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"path/to/val.record\"\n",
        "  }\n",
        "  label_map_path: \"path/to/label_map.pbtxt\"\n",
        "  shuffle: false\n",
        "  num_readers: 1\n",
        "}\n",
        "```\n",
        "\n",
        "* `input_path` ‚Üí Path to TFRecord files for training and evaluation\n",
        "* `label_map_path` ‚Üí Path to your label map file\n",
        "## **4. Verify Paths**\n",
        "\n",
        "Before starting training:\n",
        "\n",
        "1. Make sure the **TFRecord files exist** and are readable.\n",
        "2. Use Python to test reading a record:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "raw_dataset = tf.data.TFRecordDataset(\"path/to/train.record\")\n",
        "for raw_record in raw_dataset.take(1):\n",
        "    example = tf.train.Example()\n",
        "    example.ParseFromString(raw_record.numpy())\n",
        "    print(example)\n",
        "```\n",
        "## **5. Start Training**\n",
        "\n",
        "Once paths are correctly configured in the pipeline config, start training:\n",
        "\n",
        "```bash\n",
        "python model_main_tf2.py \\\n",
        "    --pipeline_config_path=path/to/pipeline.config \\\n",
        "    --model_dir=training/ \\\n",
        "    --alsologtostderr\n",
        "```\n",
        "‚úÖ **In short:**\n",
        "To configure data paths in TFOD2:\n",
        "\n",
        "1. Convert your dataset into **TFRecord files** for training and evaluation.\n",
        "2. Create a **label map** mapping classes to IDs.\n",
        "3. Edit the **pipeline config** to point to your TFRecord files and label map.\n",
        "4. Verify paths before starting training.\n",
        "\n"
      ],
      "metadata": {
        "id": "8-oLNCZlY4DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 21 Can you run Detectron2 on a CPU ?\n",
        "Yes ‚Äî **Detectron2 can run on a CPU**, but there are some important considerations:\n",
        "## **1. How to Run on CPU**\n",
        "\n",
        "When setting up your model configuration:\n",
        "\n",
        "```python\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "cfg.MODEL.DEVICE = \"cpu\"  # ‚Üê key step to use CPU\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "```\n",
        "\n",
        "* The line `cfg.MODEL.DEVICE = \"cpu\"` forces Detectron2 to use **CPU instead of GPU**.\n",
        "## **2. Considerations When Using CPU**\n",
        "\n",
        "1. **Slower Inference and Training**\n",
        "\n",
        "   * Detectron2 is optimized for GPUs.\n",
        "   * On CPU, inference and training **can be much slower**, especially for large models like Faster R-CNN or Mask R-CNN.\n",
        "\n",
        "2. **Smaller Batch Sizes**\n",
        "\n",
        "   * Large batch sizes may not fit into memory efficiently on CPU.\n",
        "\n",
        "3. **Suitable Use Cases**\n",
        "\n",
        "   * Quick testing or debugging small images\n",
        "   * Running pretrained models for **single image inference**\n",
        "   * Edge cases where GPU is not available\n",
        "\n",
        "4. **Training on CPU**\n",
        "\n",
        "   * Technically possible but **very slow**.\n",
        "   * For serious training tasks, GPU is highly recommended.\n",
        "## **3. Testing CPU Setup**\n",
        "\n",
        "You can verify that Detectron2 is running on CPU:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "print(torch.cuda.is_available())  # Should be False if using CPU only\n",
        "```\n",
        "‚úÖ **In short:**\n",
        "\n",
        "* **Yes**, Detectron2 supports CPU execution.\n",
        "* **How:** Set `cfg.MODEL.DEVICE = \"cpu\"`.\n",
        "* **Limitations:** Training and inference will be much slower; best for testing or small-scale tasks.\n"
      ],
      "metadata": {
        "id": "su8UabFxY38j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22 Why are label maps used in TFOD2 ?\n",
        "**Label maps** in TFOD2 are essential for mapping **human-readable class names to numerical IDs** that the model can use during training and inference. Here‚Äôs a detailed explanation:\n",
        "## üîπ **What a Label Map Is**\n",
        "\n",
        "* A **label map** is usually a `.pbtxt` file that defines a mapping from **class names** to **integer IDs**.\n",
        "* Example:\n",
        "\n",
        "```text\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'cat'\n",
        "}\n",
        "item {\n",
        "  id: 2\n",
        "  name: 'dog'\n",
        "}\n",
        "```\n",
        "\n",
        "* `id`: Integer used internally by the model.\n",
        "* `name`: Human-readable class name.\n",
        "## üîπ **Why Label Maps Are Used in TFOD2**\n",
        "\n",
        "### 1. **Consistent Class Identification**\n",
        "\n",
        "* The model outputs **numerical class IDs** during training and inference.\n",
        "* The label map ensures these IDs correspond to the correct **class names**.\n",
        "\n",
        "### 2. **Supports Multiple Classes**\n",
        "\n",
        "* For datasets with multiple object types, label maps allow the model to **handle all classes correctly**.\n",
        "\n",
        "### 3. **Required by TFOD2 Pipelines**\n",
        "\n",
        "* TFOD2 **TFRecord files** store class IDs, not names.\n",
        "* The label map is needed to interpret these IDs correctly during **training, evaluation, and inference**.\n",
        "\n",
        "### 4. **Facilitates Transfer Learning**\n",
        "\n",
        "* When fine-tuning a pretrained model, you can **remap class IDs** in the label map to match your custom dataset.\n",
        "\n",
        "### 5. **Integration with Visualization and Metrics**\n",
        "\n",
        "* Tools like `visualize_boxes_and_labels_on_image_array` or evaluation metrics rely on the label map to **display class names instead of IDs**.\n",
        "## üîπ **Summary**\n",
        "\n",
        "| Purpose                 | Description                                               |\n",
        "| ----------------------- | --------------------------------------------------------- |\n",
        "| Mapping names to IDs    | Human-readable labels ‚Üí numeric IDs                       |\n",
        "| Multiple class support  | Allows handling datasets with multiple objects            |\n",
        "| Required by TFRecords   | TFRecords store numeric IDs, label map interprets them    |\n",
        "| Visualization & metrics | Shows class names in images, logs, and evaluation metrics |\n",
        "| Transfer learning       | Allows remapping classes to a new dataset                 |\n",
        "‚úÖ **In short:**\n",
        "Label maps in TFOD2 are used to **translate between human-readable class names and model-understandable numeric IDs**, ensuring consistency across **training, evaluation, and inference**, and supporting multi-class datasets.\n"
      ],
      "metadata": {
        "id": "Ia31Qhh3bn3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 23 What makes TFOD2 popular for real-time detection tasks ?\n",
        "TFOD2 (TensorFlow Object Detection API v2) is popular for **real-time detection tasks** because it combines **efficient models, a flexible framework, and easy deployment options**. Here‚Äôs why:\n",
        "## üîπ **1. Availability of Lightweight, Fast Models**\n",
        "\n",
        "* TFOD2 provides models optimized for speed, suitable for real-time applications:\n",
        "\n",
        "  * **SSD MobileNet V2/V3** ‚Üí extremely fast, suitable for mobile and embedded devices.\n",
        "  * **EfficientDet D0‚ÄìD2** ‚Üí balances speed and accuracy.\n",
        "* These models require **less computational power** while maintaining reasonable accuracy.\n",
        "## üîπ **2. Optimized for TensorFlow 2.x**\n",
        "\n",
        "* Uses **`tf.data` pipelines** for efficient dataset loading.\n",
        "* Supports **GPU and TPU acceleration**, enabling real-time inference even on large datasets.\n",
        "## üîπ **3. Easy Deployment**\n",
        "\n",
        "* Trained models can be exported to multiple formats:\n",
        "\n",
        "  * **TensorFlow SavedModel** ‚Üí standard deployment\n",
        "  * **TensorFlow Lite** ‚Üí mobile and embedded devices\n",
        "  * **TensorFlow\\.js** ‚Üí browser-based real-time applications\n",
        "* Makes it easy to integrate real-time detection in **apps, cameras, or drones**.\n",
        "## üîπ **4. Flexible Input Sizes**\n",
        "\n",
        "* Models can handle varying image sizes, making them suitable for **video streams** or **camera feeds**.\n",
        "## üîπ **5. Support for Quantization and Optimization**\n",
        "\n",
        "* TFOD2 models can be **quantized or pruned** to reduce size and increase inference speed.\n",
        "* This is especially useful for **edge devices** requiring low latency.\n",
        "## üîπ **6. Active Community and Pretrained Models**\n",
        "\n",
        "* TFOD2 Model Zoo provides **pretrained weights**, enabling **transfer learning** and **rapid prototyping**.\n",
        "* Reduces development time for real-time detection projects.\n",
        "‚úÖ **In short:**\n",
        "TFOD2 is popular for real-time detection tasks because it offers **fast, lightweight models**, **efficient TensorFlow pipelines**, **easy deployment options** (mobile, web, edge), and **pretrained models for rapid development**, making it ideal for low-latency, real-world applications\n"
      ],
      "metadata": {
        "id": "V48pRvFDdr6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#   24 How does batch size impact GPU memory usage .\n",
        "**Batch size** has a **direct impact on GPU memory usage** during model training. Here‚Äôs a detailed explanation:\n",
        "## üîπ **1. What Batch Size Is**\n",
        "\n",
        "* **Batch size** is the number of samples processed **simultaneously** during one forward and backward pass of training.\n",
        "* Example: `batch_size = 8` means the model processes 8 images at once before updating weights.\n",
        "## üîπ **2. How Batch Size Affects GPU Memory**\n",
        "\n",
        "1. **Larger Batch Size ‚Üí Higher Memory Usage**\n",
        "\n",
        "   * Each sample requires memory to store:\n",
        "\n",
        "     * Input data (images)\n",
        "     * Intermediate activations (feature maps)\n",
        "     * Gradients for backpropagation\n",
        "   * Memory usage grows roughly **linearly** with batch size.\n",
        "\n",
        "2. **Smaller Batch Size ‚Üí Lower Memory Usage**\n",
        "\n",
        "   * Can fit into limited GPU memory, but may reduce **training stability** (more noisy gradients).\n",
        "\n",
        "3. **Extreme Cases**\n",
        "\n",
        "   * If batch size is **too large**, you may get an **out-of-memory (OOM) error**.\n",
        "   * If batch size is **too small**, training may take longer and the model may converge more slowly.\n",
        "## üîπ **3. Trade-Offs**\n",
        "\n",
        "| Batch Size | Pros                                                   | Cons                                           |\n",
        "| ---------- | ------------------------------------------------------ | ---------------------------------------------- |\n",
        "| Large      | Smoother gradient updates, faster per-epoch processing | High GPU memory usage, may exceed GPU capacity |\n",
        "| Small      | Fits in limited GPU memory, lower chance of OOM        | Noisier gradients, slower convergence          |\n",
        "## üîπ **4. Strategies to Handle GPU Memory Constraints**\n",
        "\n",
        "1. **Gradient Accumulation**\n",
        "\n",
        "   * Simulate a large batch size by accumulating gradients over multiple smaller batches.\n",
        "\n",
        "2. **Reduce Input Image Size**\n",
        "\n",
        "   * Smaller images consume less memory per batch.\n",
        "\n",
        "3. **Mixed Precision Training (FP16)**\n",
        "\n",
        "   * Uses half-precision floats to **cut memory usage roughly in half**.\n",
        "\n",
        "4. **Use Smaller Models or Fewer Layers**\n",
        "\n",
        "   * Lightweight backbones reduce activation memory.\n",
        "‚úÖ **In short:**\n",
        "\n",
        "* Batch size directly determines how much GPU memory is required.\n",
        "* **Larger batches** need more memory but may improve training stability, while **smaller batches** reduce memory usage but can slow convergence.\n",
        "* Techniques like **gradient accumulation, smaller images, and mixed precision** help balance memory constraints with training efficiency.\n"
      ],
      "metadata": {
        "id": "hczV4gEJeYhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25  What‚Äôs the role of Intersection over Union (IoU) in model evaluation.\n",
        "**Intersection over Union (IoU)** is a fundamental metric in **object detection and segmentation** tasks. It measures how well a predicted bounding box or mask aligns with the ground truth. Here‚Äôs a detailed explanation:\n",
        "## üîπ **1. Definition of IoU**\n",
        "\n",
        "IoU measures the **overlap between the predicted object region and the ground-truth region**:\n",
        "\n",
        "$$\n",
        "IoU = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}}\n",
        "$$\n",
        "\n",
        "* **Area of Overlap:** The region where the predicted box and the ground-truth box intersect.\n",
        "* **Area of Union:** The total area covered by both the predicted and ground-truth boxes.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* Perfect match ‚Üí IoU = 1.0\n",
        "* No overlap ‚Üí IoU = 0.0\n",
        "## üîπ **2. Role in Model Evaluation**\n",
        "\n",
        "### **a) Determines True Positives and False Positives**\n",
        "\n",
        "* A prediction is considered **correct (true positive)** if `IoU >= threshold` (commonly 0.5 for AP50).\n",
        "* Predictions with lower IoU are counted as **false positives**, affecting precision and recall.\n",
        "### **b) Part of Average Precision (AP) Calculations**\n",
        "\n",
        "* COCO-style mAP is computed by averaging AP across **multiple IoU thresholds** (0.5 to 0.95 in steps of 0.05).\n",
        "* This evaluates **how precise the model is in localizing objects**.\n",
        "### **c) Segmentation Evaluation**\n",
        "\n",
        "* For instance segmentation, IoU is computed on **masks**, not just bounding boxes.\n",
        "* Higher IoU ‚Üí better mask prediction.\n",
        "### **d) Ranking Predictions**\n",
        "\n",
        "* When multiple predicted boxes overlap, IoU is used in **Non-Maximum Suppression (NMS)** to remove duplicates:\n",
        "\n",
        "  * Boxes with high IoU are considered redundant.\n",
        "## üîπ **3. Why IoU Is Important**\n",
        "\n",
        "1. **Localization Accuracy:** Measures how precisely the model predicts object locations.\n",
        "2. **Evaluation Consistency:** Provides a standardized metric across datasets and tasks.\n",
        "3. **Threshold Tuning:** You can adjust the IoU threshold to control precision vs recall.\n",
        "4. **Integral to Detection Metrics:** mAP, AP50, AP75, and AR all rely on IoU.\n",
        "‚úÖ **In short:**\n",
        "IoU quantifies **how well predicted boxes or masks overlap with ground truth**, serving as the basis for **true positives, false positives, and evaluation metrics** like mAP. It‚Äôs also used in **Non-Maximum Suppression** to remove duplicate predictions.\n"
      ],
      "metadata": {
        "id": "7peTko4ofweA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 26 What is Faster R-CNN, and does TFOD2 support it ?\n",
        "**Faster R-CNN** is a **popular two-stage object detection model** known for balancing accuracy and speed. TFOD2 (TensorFlow Object Detection API v2) does support it. Here‚Äôs a detailed breakdown:\n",
        "## üîπ **1. What Faster R-CNN Is**\n",
        "\n",
        "* **Full name:** Faster Region-based Convolutional Neural Network.\n",
        "* **Purpose:** Detect and localize objects in images with **bounding boxes**.\n",
        "* **Architecture:** Two main stages:\n",
        "\n",
        "### **Stage 1: Region Proposal Network (RPN)**\n",
        "\n",
        "* Generates a set of **candidate object regions** (proposals) across the image.\n",
        "* Works like a ‚Äúsearch mechanism‚Äù for objects.\n",
        "\n",
        "### **Stage 2: ROI Classification & Refinement**\n",
        "\n",
        "* Each proposed region is **cropped and passed through the classifier** to predict:\n",
        "\n",
        "  * Object class\n",
        "  * Refined bounding box coordinates\n",
        "\n",
        "### **Key Features**\n",
        "\n",
        "* Accurate because it uses two stages: proposal generation + classification.\n",
        "* Slower than single-stage detectors like SSD or YOLO, but higher precision.\n",
        "## üîπ **2. Does TFOD2 Support Faster R-CNN?**\n",
        "\n",
        "‚úÖ **Yes.**\n",
        "\n",
        "* TFOD2 provides **predefined Faster R-CNN models** in its **Model Zoo**, such as:\n",
        "\n",
        "  * Faster R-CNN with **ResNet50** backbone\n",
        "  * Faster R-CNN with **ResNet101** backbone\n",
        "  * Options with **FPN (Feature Pyramid Network)** for multi-scale detection\n",
        "\n",
        "* Example in TFOD2 pipeline config:\n",
        "\n",
        "```text\n",
        "model {\n",
        "  faster_rcnn {\n",
        "    num_classes: 3\n",
        "    ...\n",
        "    feature_extractor { type: \"faster_rcnn_resnet50\" }\n",
        "  }\n",
        "}\n",
        "```\n",
        "## üîπ **3. When to Use Faster R-CNN**\n",
        "\n",
        "* High **detection accuracy** is critical.\n",
        "* Real-time inference is **not required** (slower than YOLO/SSD).\n",
        "* Suitable for **complex datasets with small or overlapping objects**.\n",
        "## üîπ **4. Comparison to Other Models in TFOD2**\n",
        "\n",
        "| Model Type    | Speed       | Accuracy | Use Case                          |\n",
        "| ------------- | ----------- | -------- | --------------------------------- |\n",
        "| Faster R-CNN  | Medium-Slow | High     | Research, high-accuracy detection |\n",
        "| SSD MobileNet | Fast        | Medium   | Real-time, mobile                 |\n",
        "| EfficientDet  | Medium-Fast | High     | Balanced speed & accuracy         |\n",
        "‚úÖ **In short:**\n",
        "**Faster R-CNN** is a **two-stage object detector** that uses a Region Proposal Network followed by ROI classification. **TFOD2 supports Faster R-CNN**, providing pretrained models and configurable pipelines for training custom datasets."
      ],
      "metadata": {
        "id": "6-r6v_CuhfzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 27 How does Detectron2 use pretrained weights ?\n",
        "Great question! Let‚Äôs break it down clearly.\n",
        "\n",
        "Detectron2 uses **pretrained weights** mainly for **transfer learning**. Instead of training a model from scratch (which requires huge datasets and compute), you can load weights trained on large datasets like **ImageNet** (for backbones) or **COCO** (for detection tasks), and then fine-tune them on your custom dataset.\n",
        "\n",
        "Here‚Äôs how it works step by step:\n",
        "### 1. **Model Zoo & Pretrained Weights**\n",
        "\n",
        "Detectron2 provides a **Model Zoo** with pretrained models (Faster R-CNN, Mask R-CNN, RetinaNet, etc.) trained on COCO. These weights include:\n",
        "\n",
        "* **Backbone weights** (e.g., ResNet, ResNeXt pretrained on ImageNet)\n",
        "* **Full detection model weights** (trained on COCO or other datasets)\n",
        "### 2. **Configuration**\n",
        "\n",
        "When you set up a Detectron2 model, you typically load a config file from the Model Zoo:\n",
        "\n",
        "```python\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "```\n",
        "\n",
        "Here:\n",
        "\n",
        "* `merge_from_file` loads the architecture and hyperparameters.\n",
        "* `MODEL.WEIGHTS` specifies which pretrained weights to use.\n",
        "### 3. **How Pretrained Weights Are Used**\n",
        "\n",
        "* **Feature Extractor (Backbone):**\n",
        "  If you use a ResNet-50 backbone, it loads pretrained ImageNet weights to extract low- and mid-level image features (edges, textures, shapes).\n",
        "\n",
        "* **Detection Layers:**\n",
        "  The region proposal network (RPN), ROI heads, and classification layers can load pretrained COCO-trained weights.\n",
        "  If you‚Äôre training on your own dataset, you usually:\n",
        "\n",
        "  * Keep the backbone frozen (or partially frozen)\n",
        "  * Fine-tune detection heads for your dataset classes\n",
        "### 4. **Fine-Tuning on Custom Dataset**\n",
        "\n",
        "If your dataset has a different number of classes than COCO (80 classes):\n",
        "\n",
        "```python\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = <your_num_classes>\n",
        "cfg.MODEL.WEIGHTS = \"detectron2://.../model_final.pth\"  # Pretrained on COCO\n",
        "```\n",
        "\n",
        "Detectron2 automatically:\n",
        "\n",
        "* Loads pretrained weights for all matching layers\n",
        "* Randomly initializes layers that don‚Äôt match (e.g., classification head for your new classes)\n",
        "### 5. **Workflow Summary**\n",
        "\n",
        "* **Pretraining:** Model trained on ImageNet or COCO\n",
        "* **Loading weights:** Use `cfg.MODEL.WEIGHTS`\n",
        "* **Transfer learning:** Reuse backbone & detection heads, fine-tune last layers for new dataset\n",
        "* **Initialization logic:** Layers with matching shapes ‚Üí load weights; mismatched layers ‚Üí random init\n",
        "‚úÖ In short: Detectron2 uses pretrained weights as an initialization to accelerate convergence and improve accuracy, by leveraging general-purpose visual features learned from large datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "E0hvQI2SiNt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 28 What file format is typically used to store training data in TFOD2 ?\n",
        "Great question! Let‚Äôs break it down clearly.\n",
        "\n",
        "Detectron2 uses **pretrained weights** mainly for **transfer learning**. Instead of training a model from scratch (which requires huge datasets and compute), you can load weights trained on large datasets like **ImageNet** (for backbones) or **COCO** (for detection tasks), and then fine-tune them on your custom dataset.\n",
        "\n",
        "Here‚Äôs how it works step by step:\n",
        "### 1. **Model Zoo & Pretrained Weights**\n",
        "\n",
        "Detectron2 provides a **Model Zoo** with pretrained models (Faster R-CNN, Mask R-CNN, RetinaNet, etc.) trained on COCO. These weights include:\n",
        "\n",
        "* **Backbone weights** (e.g., ResNet, ResNeXt pretrained on ImageNet)\n",
        "* **Full detection model weights** (trained on COCO or other datasets)\n",
        "### 2. **Configuration**\n",
        "\n",
        "When you set up a Detectron2 model, you typically load a config file from the Model Zoo:\n",
        "\n",
        "```python\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "```\n",
        "\n",
        "Here:\n",
        "\n",
        "* `merge_from_file` loads the architecture and hyperparameters.\n",
        "* `MODEL.WEIGHTS` specifies which pretrained weights to use.\n",
        "### 3. **How Pretrained Weights Are Used**\n",
        "\n",
        "* **Feature Extractor (Backbone):**\n",
        "  If you use a ResNet-50 backbone, it loads pretrained ImageNet weights to extract low- and mid-level image features (edges, textures, shapes).\n",
        "\n",
        "* **Detection Layers:**\n",
        "  The region proposal network (RPN), ROI heads, and classification layers can load pretrained COCO-trained weights.\n",
        "  If you‚Äôre training on your own dataset, you usually:\n",
        "\n",
        "  * Keep the backbone frozen (or partially frozen)\n",
        "  * Fine-tune detection heads for your dataset classes\n",
        "### 4. **Fine-Tuning on Custom Dataset**\n",
        "\n",
        "If your dataset has a different number of classes than COCO (80 classes):\n",
        "\n",
        "```python\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = <your_num_classes>\n",
        "cfg.MODEL.WEIGHTS = \"detectron2://.../model_final.pth\"  # Pretrained on COCO\n",
        "```\n",
        "\n",
        "Detectron2 automatically:\n",
        "\n",
        "* Loads pretrained weights for all matching layers\n",
        "* Randomly initializes layers that don‚Äôt match (e.g., classification head for your new classes)\n",
        "### 5. **Workflow Summary**\n",
        "\n",
        "* **Pretraining:** Model trained on ImageNet or COCO\n",
        "* **Loading weights:** Use `cfg.MODEL.WEIGHTS`\n",
        "* **Transfer learning:** Reuse backbone & detection heads, fine-tune last layers for new dataset\n",
        "* **Initialization logic:** Layers with matching shapes ‚Üí load weights; mismatched layers ‚Üí random init\n",
        "‚úÖ In short: Detectron2 uses pretrained weights as an initialization to accelerate convergence and improve accuracy, by leveraging general-purpose visual features learned from large datasets.\n"
      ],
      "metadata": {
        "id": "6pLPvl4ELKpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 29 What is the difference between semantic segmentation and instance segmentation.\n",
        "Good question üëç Let‚Äôs break it down clearly with examples:\n",
        "## **1. Semantic Segmentation**\n",
        "\n",
        "* **Definition:** Assigns a class label to **every pixel** in an image.\n",
        "* **Goal:** Group pixels into categories such as *car, road, person, tree*, etc.\n",
        "* **Key Point:** It **does not distinguish** between different objects of the same class.\n",
        "\n",
        "üëâ Example:\n",
        "If there are 3 people in the image, semantic segmentation will label all their pixels as \"person\" without separating them as *person 1, person 2, person 3*.\n",
        "## **2. Instance Segmentation**\n",
        "\n",
        "* **Definition:** Assigns a class label **and an instance ID** to every pixel in an image.\n",
        "* **Goal:** Detect and segment **each individual object** of a class separately.\n",
        "* **Key Point:** It distinguishes between **different instances** of the same class.\n",
        "\n",
        "üëâ Example:\n",
        "If there are 3 people in the image, instance segmentation will output 3 distinct masks: *person 1, person 2, person 3*.\n",
        "## **Comparison Table**\n",
        "\n",
        "| Aspect                             | Semantic Segmentation                    | Instance Segmentation                             |\n",
        "| ---------------------------------- | ---------------------------------------- | ------------------------------------------------- |\n",
        "| Output                             | Pixel ‚Üí class label                      | Pixel ‚Üí class + object ID                         |\n",
        "| Differentiates same-class objects? | ‚ùå No                                     | ‚úÖ Yes                                             |\n",
        "| Example with 3 cars                | All pixels labeled \"car\" (no separation) | Each car labeled separately (Car 1, Car 2, Car 3) |\n",
        "| Typical Models                     | FCN, DeepLab, PSPNet                     | Mask R-CNN, YOLACT, Detectron2-based models       |\n",
        "‚úÖ **In short:**\n",
        "\n",
        "* **Semantic segmentation** answers: *‚ÄúWhat category is this pixel?‚Äù*\n",
        "* **Instance segmentation** answers: *‚ÄúWhich object (and category) is this pixel part of?‚Äù\n",
        "\n"
      ],
      "metadata": {
        "id": "5I2qzmbULyVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 30 Can Detectron2 detect custom classes during inference ?\n",
        "Yes ‚úÖ ‚Äî Detectron2 **can detect custom classes during inference**, but only if you‚Äôve trained (or fine-tuned) the model on those classes.\n",
        "\n",
        "Here‚Äôs the breakdown:\n",
        "## üîπ 1. Training / Fine-tuning for Custom Classes\n",
        "\n",
        "* Detectron2 models (e.g., Faster R-CNN, Mask R-CNN) are pretrained on COCO with **80 classes**.\n",
        "* If your dataset has, say, **3 custom classes** (`cat`, `dog`, `rabbit`), you must:\n",
        "\n",
        "  * Update the config:\n",
        "\n",
        "    ```python\n",
        "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3\n",
        "    ```\n",
        "  * Train or fine-tune the model on your dataset.\n",
        "* During training, the model learns to recognize your classes and replaces the original COCO classifier head with a new one.\n",
        "## üîπ 2. Inference on Custom Classes\n",
        "\n",
        "Once trained, you can load the trained weights for inference:\n",
        "\n",
        "```python\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "# Load config\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\"path/to/your/config.yaml\")\n",
        "cfg.MODEL.WEIGHTS = \"path/to/your/model_final.pth\"\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # custom number of classes\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# Run inference\n",
        "outputs = predictor(im)  # im = input image (BGR, as np.array)\n",
        "``\n",
        "The output dictionary will contain predictions for your custom classes:\n",
        "\n",
        "* `instances.pred_classes` ‚Üí Class IDs (`0=cat`, `1=dog`, `2=rabbit`)\n",
        "* `instances.pred_boxes` ‚Üí Bounding boxes\n",
        "* `instances.scores` ‚Üí Confidence scores\n",
        "* `instances.pred_masks` ‚Üí Segmentation masks (if using Mask R-CNN)\n",
        "## üîπ 3. Things to Remember\n",
        "\n",
        "* You **cannot just edit the class names** and expect inference to work on new categories ‚Äî the model must be trained on them.\n",
        "* If you want to reuse a COCO model but detect **only a subset of COCO classes**, you can filter results at inference instead of retraining.\n",
        "* For **completely new objects**, training or fine-tuning is mandatory.\n",
        "‚úÖ **In summary:**\n",
        "Detectron2 can absolutely detect custom classes at inference time ‚Äî but only if you have trained the model (or fine-tuned from a pretrained COCO model) with your dataset and saved the weights\n"
      ],
      "metadata": {
        "id": "jq058oVkMMba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 31. Why is pipeline.config essential in TFOD2.\n",
        "Yes ‚úÖ ‚Äî Detectron2 **can detect custom classes during inference**, but only if you‚Äôve trained (or fine-tuned) the model on those classes.\n",
        "\n",
        "Here‚Äôs the breakdown:\n",
        "## üîπ 1. Training / Fine-tuning for Custom Classes\n",
        "\n",
        "* Detectron2 models (e.g., Faster R-CNN, Mask R-CNN) are pretrained on COCO with **80 classes**.\n",
        "* If your dataset has, say, **3 custom classes** (`cat`, `dog`, `rabbit`), you must:\n",
        "\n",
        "  * Update the config:\n",
        "\n",
        "    ```python\n",
        "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3\n",
        "    ```\n",
        "  * Train or fine-tune the model on your dataset.\n",
        "* During training, the model learns to recognize your classes and replaces the original COCO classifier head with a new one.\n",
        "## üîπ 2. Inference on Custom Classes\n",
        "\n",
        "Once trained, you can load the trained weights for inference:\n",
        "\n",
        "```python\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "# Load config\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\"path/to/your/config.yaml\")\n",
        "cfg.MODEL.WEIGHTS = \"path/to/your/model_final.pth\"\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # custom number of classes\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# Run inference\n",
        "outputs = predictor(im)  # im = input image (BGR, as np.array)\n",
        "```\n",
        "\n",
        "The output dictionary will contain predictions for your custom classes:\n",
        "\n",
        "* `instances.pred_classes` ‚Üí Class IDs (`0=cat`, `1=dog`, `2=rabbit`)\n",
        "* `instances.pred_boxes` ‚Üí Bounding boxes\n",
        "* `instances.scores` ‚Üí Confidence scores\n",
        "* `instances.pred_masks` ‚Üí Segmentation masks (if using Mask R-CNN)\n",
        "## üîπ 3. Things to Remember\n",
        "\n",
        "* You **cannot just edit the class names** and expect inference to work on new categories ‚Äî the model must be trained on them.\n",
        "* If you want to reuse a COCO model but detect **only a subset of COCO classes**, you can filter results at inference instead of retraining.\n",
        "* For **completely new objects**, training or fine-tuning is mandatory.\n",
        "‚úÖ **In summary:**\n",
        "Detectron2 can absolutely detect custom classes at inference time ‚Äî but only if you have trained the model (or fine-tuned from a pretrained COCO model) with your dataset and saved the weights\n"
      ],
      "metadata": {
        "id": "KkXaAyrtMtVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 32 What type of models does TFOD2 support for object detection.\n",
        "Great question üëç ‚Äî TensorFlow Object Detection API (TFOD2) supports a **wide variety of model architectures** for object detection, instance segmentation, and keypoint detection.\n",
        "\n",
        "Here‚Äôs a structured breakdown:\n",
        "## üîπ 1. **Single-Stage Detectors (fast, real-time models)**\n",
        "\n",
        "These models predict bounding boxes and classes directly from feature maps (no proposal stage).\n",
        "\n",
        "* **SSD (Single Shot Detector) family**\n",
        "\n",
        "  * SSD with MobileNetV1/V2/V3\n",
        "  * SSD with ResNet-50/101\n",
        "  * SSD Lite (optimized for mobile/edge devices)\n",
        "* **EfficientDet family** (D0‚ÄìD7)\n",
        "\n",
        "  * Scalable and efficient, good balance of accuracy and speed\n",
        "* **YOLO family (via TFOD2 community support)**\n",
        "\n",
        "  * Some versions of YOLO (e.g., YOLOv4/YOLOv5) have unofficial support through TFOD2-compatible repos\n",
        "\n",
        "‚úÖ Best for **real-time detection** on mobile/embedded devices.\n",
        "## üîπ 2. **Two-Stage Detectors (high accuracy models)**\n",
        "\n",
        "These models first generate region proposals (RPN) and then classify/refine them.\n",
        "\n",
        "* **Faster R-CNN**\n",
        "\n",
        "  * With ResNet-50, ResNet-101, Inception-ResNet backbones\n",
        "* **R-FCN (Region-based Fully Convolutional Network)**\n",
        "* **Mask R-CNN** (for instance segmentation, in addition to detection)\n",
        "\n",
        "‚úÖ Best for **high accuracy** but slower than single-stage models.\n",
        "## üîπ 3. **Specialized Architectures**\n",
        "\n",
        "* **CenterNet**\n",
        "\n",
        "  * Detects objects as keypoints (center points + dimensions)\n",
        "* **Keypoint R-CNN**\n",
        "\n",
        "  * Detects human keypoints (pose estimation)\n",
        "* **Panoptic-DeepLab**\n",
        "\n",
        "  * For panoptic segmentation (combination of instance + semantic segmentation)\n",
        "## üîπ 4. **Supported Backbones**\n",
        "\n",
        "Many models above can be combined with different feature extractors (backbones):\n",
        "\n",
        "* **MobileNet** (V1, V2, V3 ‚Üí lightweight, mobile optimized)\n",
        "* **ResNet** (50, 101 ‚Üí more powerful, accurate)\n",
        "* **Inception** (v2, v3, ResNet variants)\n",
        "* **EfficientNet** (for EfficientDet)\n",
        "## ‚úÖ Summary\n",
        "\n",
        "TFOD2 supports:\n",
        "\n",
        "* **Single-stage detectors** ‚Üí SSD, EfficientDet (fast, real-time)\n",
        "* **Two-stage detectors** ‚Üí Faster R-CNN, R-FCN, Mask R-CNN (high accuracy)\n",
        "* **Specialized models** ‚Üí CenterNet, Keypoint R-CNN, Panoptic-DeepLab\n",
        "* **Multiple backbones** ‚Üí MobileNet, ResNet, Inception, EfficientNet"
      ],
      "metadata": {
        "id": "U7XPE-UZNbuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 33 What happens if the learning rate is too high during training ?\n",
        "Good one üëç ‚Äî The **learning rate** is one of the most important hyperparameters in training deep learning models.\n",
        "\n",
        "Here‚Äôs what happens if the **learning rate is too high**:\n",
        "## üîπ 1. **Unstable Training**\n",
        "\n",
        "* Instead of gradually minimizing the loss, the model overshoots the optimal point.\n",
        "* The loss function may **fluctuate heavily** or even **diverge to infinity**.\n",
        "\n",
        "üëâ Example: Imagine descending a hill in big steps ‚Äî you keep jumping over the valley and never settle at the bottom.\n",
        "## üîπ 2. **Failure to Converge**\n",
        "\n",
        "* The optimizer keeps bouncing around instead of converging.\n",
        "* The model never settles into a stable state where weights produce meaningful predictions.\n",
        "* Accuracy stays low or fluctuates randomly.\n",
        "## üîπ 3. **Poor Generalization**\n",
        "\n",
        "* Even if the loss decreases, a too-high learning rate may cause the model to learn **noisy, unstable weight updates**.\n",
        "* This leads to poor generalization on validation/test sets (overfitting-like behavior).\n",
        "## üîπ 4. **Exploding Gradients**\n",
        "\n",
        "* Large updates cause gradients to become very large (\"explode\"), breaking the optimization process.\n",
        "* This may result in NaN (Not-a-Number) losses.\n",
        "## üîπ 5. **Possible Signs You‚Äôll See**\n",
        "\n",
        "* Loss doesn‚Äôt decrease (stays high).\n",
        "* Loss decreases for a while, then suddenly **jumps up**.\n",
        "* Training accuracy might look random.\n",
        "* Validation accuracy remains poor.\n",
        "## ‚úÖ Visual Analogy\n",
        "\n",
        "* **Low learning rate** ‚Üí Like walking downhill in tiny steps ‚Üí slow, but safe.\n",
        "* **Good learning rate** ‚Üí Steady steps, reaching the valley efficiently.\n",
        "* **Too high learning rate** ‚Üí Giant leaps that make you overshoot or bounce around, never reaching the valley.\n",
        "üëâ **In short:**\n",
        "If the learning rate is too high, training becomes unstable, the model may fail to converge, and the final accuracy will be poor.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vv6EdR_6OHaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 34 What is COCO JSON format.\n",
        "Great question üëç ‚Äî you‚Äôll run into **COCO JSON format** a lot when working with object detection and segmentation (in Detectron2, TFOD2, YOLO, etc.).\n",
        "## üîπ What is COCO JSON Format?\n",
        "\n",
        "The **COCO JSON format** is a standardized way of storing **datasets for computer vision tasks** (object detection, segmentation, keypoints) introduced by the **COCO dataset** (*Common Objects in Context*).\n",
        "\n",
        "It is simply a JSON file (e.g., `annotations.json`) that describes:\n",
        "\n",
        "* The **images** in the dataset\n",
        "* The **annotations** (bounding boxes, masks, keypoints, etc.)\n",
        "* The **categories (classes)**\n",
        "\n",
        "This format is widely adopted because it works for **multiple tasks**: detection, segmentation, and keypoint detection.\n",
        "## üîπ Main Structure of a COCO JSON File\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"images\": [\n",
        "    {\n",
        "      \"id\": 1,\n",
        "      \"file_name\": \"image1.jpg\",\n",
        "      \"height\": 480,\n",
        "      \"width\": 640\n",
        "    }\n",
        "  ],\n",
        "  \"annotations\": [\n",
        "    {\n",
        "      \"id\": 1,\n",
        "      \"image_id\": 1,\n",
        "      \"category_id\": 3,\n",
        "      \"bbox\": [100, 200, 50, 80],\n",
        "      \"area\": 4000,\n",
        "      \"iscrowd\": 0,\n",
        "      \"segmentation\": [[100,200, 150,200, 150,280, 100,280]]\n",
        "    }\n",
        "  ],\n",
        "  \"categories\": [\n",
        "    {\n",
        "      \"id\": 1,\n",
        "      \"name\": \"person\",\n",
        "      \"supercategory\": \"human\"\n",
        "    },\n",
        "    {\n",
        "      \"id\": 2,\n",
        "      \"name\": \"car\",\n",
        "      \"supercategory\": \"vehicle\"\n",
        "    },\n",
        "    {\n",
        "      \"id\": 3,\n",
        "      \"name\": \"dog\",\n",
        "      \"supercategory\": \"animal\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "## üîπ Explanation of Fields\n",
        "\n",
        "### 1. **images**\n",
        "\n",
        "* Stores metadata about each image.\n",
        "* Fields:\n",
        "\n",
        "  * `id` ‚Üí unique identifier for the image\n",
        "  * `file_name` ‚Üí image filename\n",
        "  * `height`, `width` ‚Üí image dimensions\n",
        "\n",
        "### 2. **annotations**\n",
        "\n",
        "* Stores all bounding boxes, masks, or keypoints.\n",
        "* Fields:\n",
        "\n",
        "  * `id` ‚Üí unique annotation ID\n",
        "  * `image_id` ‚Üí links the annotation to an image\n",
        "  * `category_id` ‚Üí links to a class (from `categories`)\n",
        "  * `bbox` ‚Üí bounding box `[x, y, width, height]` (x,y = top-left corner)\n",
        "  * `area` ‚Üí area of the bbox or mask\n",
        "  * `iscrowd` ‚Üí 0 for a normal object, 1 if it‚Äôs a crowd of objects\n",
        "  * `segmentation` ‚Üí polygon points outlining the object (for segmentation tasks)\n",
        "\n",
        "### 3. **categories**\n",
        "\n",
        "* Defines the list of object classes.\n",
        "* Fields:\n",
        "\n",
        "  * `id` ‚Üí numerical class ID\n",
        "  * `name` ‚Üí class name (e.g., ‚Äúdog‚Äù)\n",
        "  * `supercategory` ‚Üí higher-level grouping (optional, e.g., ‚Äúanimal‚Äù)\n",
        "## üîπ Why Use COCO Format?\n",
        "\n",
        "* **Standardized** ‚Üí Used by COCO dataset, supported in Detectron2, TFOD2, MMDetection, YOLO.\n",
        "* **Flexible** ‚Üí Supports bounding boxes, instance segmentation, and keypoints in one format.\n",
        "* **Easy to extend** ‚Üí You can define your own custom classes.\n",
        "‚úÖ **In short:**\n",
        "The COCO JSON format is a widely used dataset annotation structure that organizes **images**, **object annotations** (bounding boxes, masks, keypoints), and **categories (classes)** in a standardized way.\n"
      ],
      "metadata": {
        "id": "eOj-aXV0PSr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 35 Why is TensorFlow Lite compatibility important in TFOD2?\n",
        "Great question üëç ‚Äî **TensorFlow Lite (TFLite) compatibility** is very important in **TensorFlow Object Detection API (TFOD2)** because it enables models to run efficiently on **mobile devices and edge hardware**.\n",
        "\n",
        "Here‚Äôs the breakdown:\n",
        "## üîπ 1. Deployment on Mobile & Edge Devices\n",
        "\n",
        "* TFOD2 models are often trained on powerful GPUs, but deployment happens on **phones, IoT devices, drones, cameras, and embedded systems**.\n",
        "* TFLite converts heavy TensorFlow models into **lightweight, optimized versions** that can run on CPUs, DSPs, and NPUs inside mobile/edge devices.\n",
        "\n",
        "üëâ Without TFLite, most TFOD2 models would be too large and slow for real-time mobile use.\n",
        "## üîπ 2. Reduced Model Size\n",
        "\n",
        "* TFLite supports **model quantization** (e.g., FP32 ‚Üí FP16 or INT8).\n",
        "* This reduces model size (sometimes by 4√ó) while maintaining reasonable accuracy.\n",
        "* Smaller models = faster downloads, less storage, lower memory usage.\n",
        "## üîπ 3. Faster Inference\n",
        "\n",
        "* TFLite is optimized for **low-latency inference**.\n",
        "* Critical for **real-time object detection** tasks like:\n",
        "\n",
        "  * Autonomous drones\n",
        "  * Smart cameras\n",
        "  * Augmented reality\n",
        "  * Mobile apps (e.g., barcode scanners, wildlife detection apps)\n",
        "## üîπ 4. Cross-Platform Support\n",
        "\n",
        "* A single TFOD2-trained model can be exported to **TFLite** and run on:\n",
        "\n",
        "  * Android (via TensorFlow Lite Interpreter)\n",
        "  * iOS\n",
        "  * Raspberry Pi, Jetson Nano\n",
        "  * Coral Edge TPU (with Edge-TPU-compiled TFLite models)\n",
        "## üîπ 5. Energy Efficiency\n",
        "\n",
        "* Mobile and IoT devices have **limited battery power**.\n",
        "* TFLite optimizations ensure inference runs faster and consumes less energy compared to running full TensorFlow.\n",
        "## ‚úÖ Summary\n",
        "\n",
        "**TensorFlow Lite compatibility in TFOD2 is important because:**\n",
        "\n",
        "* It enables **real-time object detection on mobile & edge devices**.\n",
        "* Reduces **model size** through quantization.\n",
        "* Provides **faster inference** with lower latency.\n",
        "* Supports **cross-platform deployment** (Android, iOS, Raspberry Pi, Edge TPUs).\n",
        "* Improves **energy efficiency**, making it practical for battery-powered devices.\n"
      ],
      "metadata": {
        "id": "ePYnOZ4MPSop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practical"
      ],
      "metadata": {
        "id": "9PHJjUQpRHMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1  How do you install Detectron2 using pip and check the version of Detectron2.\n",
        "Good one üëç ‚Äî here‚Äôs how you can **install Detectron2 with pip** and check its version.\n",
        "## üîπ 1. Install Detectron2 using pip\n",
        "\n",
        "Detectron2 provides pre-built wheels (binaries) for specific PyTorch + CUDA versions.\n",
        "Run this command (replace `<TORCH_VERSION>` and `<CUDA_VERSION>` accordingly):\n",
        "\n",
        "```bash\n",
        "pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu118/torch2.0/index.html\n",
        "```\n",
        "\n",
        "Where:\n",
        "\n",
        "* `cu118` = CUDA 11.8 (use `cu117` for CUDA 11.7, `cu121` for CUDA 12.1, or `cpu` if you don‚Äôt have GPU support).\n",
        "* `torch2.0` = PyTorch version 2.0 (must match your installed PyTorch).\n",
        "\n",
        "üëâ Example for **PyTorch 2.0 + CUDA 11.8**:\n",
        "\n",
        "```bash\n",
        "pip install torch==2.0 torchvision==0.15\n",
        "pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu118/torch2.0/index.html\n",
        "```\n",
        "\n",
        "üëâ Example for **CPU-only (no GPU)**:\n",
        "\n",
        "```bash\n",
        "pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch2.0/index.html\n",
        "```\n",
        "## üîπ 2. Verify Installation & Check Version\n",
        "\n",
        "Open Python and run:\n",
        "\n",
        "```python\n",
        "import detectron2\n",
        "print(detectron2.__version__)\n",
        "```\n",
        "\n",
        "This will print something like:\n",
        "\n",
        "```\n",
        "0.6\n",
        "```\n",
        "## üîπ 3. Optional: Check CUDA Availability in Detectron2\n",
        "\n",
        "```python\n",
        "from detectron2.utils import collect_env\n",
        "print(collect_env.collect_env_info())\n",
        "```\n",
        "\n",
        "This prints a full environment report (PyTorch version, CUDA, Detectron2 version, etc.).\n",
        "‚úÖ **Summary**:\n",
        "\n",
        "1. Install Detectron2 with `pip install detectron2 -f <wheel link>`.\n",
        "2. Check version with `detectron2.__version__`.\n"
      ],
      "metadata": {
        "id": "97js_K2VPSkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 How do you perform inference with Detectron2 using an online image.\n",
        "Great question üëç ‚Äî let‚Äôs walk through how to run **inference with Detectron2 on an online image** (e.g., an image from a URL instead of a local file).\n",
        "## üîπ Steps to Perform Inference on an Online Image\n",
        "\n",
        "### 1. Import Dependencies\n",
        "\n",
        "```python\n",
        "import requests\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "import matplotlib.pyplot as plt\n",
        "```\n",
        "### 2. Load an Image from a URL\n",
        "\n",
        "```python\n",
        "# Example image from the web\n",
        "url = \"https://example.com/sample.jpg\"\n",
        "resp = requests.get(url, stream=True).raw\n",
        "\n",
        "# Load image into OpenCV format (BGR)\n",
        "image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "```\n",
        "### 3. Set up Detectron2 Model (Pretrained)\n",
        "\n",
        "```python\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # confidence threshold\n",
        "predictor = DefaultPredictor(cfg)\n",
        "```\n",
        "### 4. Run Inference\n",
        "\n",
        "```python\n",
        "outputs = predictor(image)\n",
        "print(outputs[\"instances\"].pred_classes)  # class IDs\n",
        "print(outputs[\"instances\"].pred_boxes)    # bounding boxes\n",
        "```\n",
        "### 5. Visualize Results\n",
        "\n",
        "```python\n",
        "# Convert BGR ‚Üí RGB for visualization\n",
        "v = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.imshow(out.get_image())\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "```\n",
        "## ‚úÖ Summary\n",
        "\n",
        "1. Load the image from the internet using `requests`.\n",
        "2. Decode it into OpenCV format.\n",
        "3. Load a pretrained Detectron2 model (e.g., Faster R-CNN).\n",
        "4. Run inference with `predictor(image)`.\n",
        "5. Visualize results using `Visualizer\n",
        "\n"
      ],
      "metadata": {
        "id": "2Rt64c0iRcyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 How do you visualize evaluation metrics in Detectron2, such as training loss?\n",
        "Perfect question üëç ‚Äî Detectron2 gives you tools to **track and visualize training metrics** such as loss, learning rate, and evaluation scores.\n",
        "\n",
        "Here‚Äôs how you can do it step by step:\n",
        "## üîπ 1. During Training (Loss Tracking)\n",
        "\n",
        "When you train with `DefaultTrainer`, Detectron2 automatically logs metrics like:\n",
        "\n",
        "* `total_loss`\n",
        "* `loss_cls`, `loss_box_reg`, `loss_mask` (for Mask R-CNN)\n",
        "* `lr` (learning rate)\n",
        "\n",
        "By default, these logs are written to **stdout** and to an **event storage** (`events.out.tfevents...`) file.\n",
        "## üîπ 2. Using TensorBoard\n",
        "\n",
        "If you want a nice UI:\n",
        "\n",
        "```bash\n",
        "pip install tensorboard\n",
        "```\n",
        "\n",
        "Start TensorBoard:\n",
        "\n",
        "```bash\n",
        "tensorboard --logdir output\n",
        "```\n",
        "\n",
        "* `output` is the folder where Detectron2 saves training logs.\n",
        "* Open [http://localhost:6006](http://localhost:6006) in your browser ‚Üí you‚Äôll see plots of **training loss, validation metrics, learning rate, etc.**\n",
        "## üîπ 3. Using Detectron2‚Äôs Built-in Plotting Utility\n",
        "\n",
        "Detectron2 provides a simple script to visualize logs without TensorBoard:\n",
        "\n",
        "```bash\n",
        "python -m detectron2.utils.events output/metrics.json --keys loss_cls loss_box_reg total_loss lr --smooth 0.9\n",
        "```\n",
        "\n",
        "Here:\n",
        "\n",
        "* `output/metrics.json` ‚Üí JSON log file (created automatically during training)\n",
        "* `--keys` ‚Üí metrics you want to plot\n",
        "* `--smooth` ‚Üí smooths the curve\n",
        "\n",
        "This will display loss curves directly in a matplotlib window.\n",
        "## üîπ 4. Custom Plotting in Python\n",
        "\n",
        "If you prefer Python plotting:\n",
        "\n",
        "```python\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load metrics.json\n",
        "metrics = []\n",
        "with open(\"output/metrics.json\") as f:\n",
        "    for line in f:\n",
        "        metrics.append(json.loads(line))\n",
        "\n",
        "# Extract total_loss and iteration\n",
        "iterations = [x[\"iteration\"] for x in metrics if \"total_loss\" in x]\n",
        "losses = [x[\"total_loss\"] for x in metrics if \"total_loss\" in x]\n",
        "\n",
        "plt.plot(iterations, losses)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Total Loss\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.show()\n",
        "```\n",
        "## ‚úÖ Summary\n",
        "\n",
        "* **TensorBoard** ‚Üí best for interactive visualization.\n",
        "* **`detectron2.utils.events` script** ‚Üí quick way to plot from JSON logs.\n",
        "* **Custom Python script** ‚Üí if you want full control."
      ],
      "metadata": {
        "id": "tXRQaDTrR1dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 How do you run inference with TFOD2 on an online image.\n",
        "Got it üëç ‚Äî you can definitely run **inference with TensorFlow Object Detection API (TFOD2) on an online image** (i.e., from a URL instead of a local file).\n",
        "\n",
        "Here‚Äôs how you can do it step by step:\n",
        "## üîπ 1. Import Required Libraries\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import requests\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "```\n",
        "## üîπ 2. Load an Image from a URL\n",
        "\n",
        "```python\n",
        "# Example image URL\n",
        "IMAGE_URL = \"https://tensorflow.org/images/surf.jpg\"\n",
        "\n",
        "# Load image using requests\n",
        "response = requests.get(IMAGE_URL, stream=True)\n",
        "image = Image.open(response.raw)\n",
        "\n",
        "# Convert to numpy array\n",
        "image_np = np.array(image)\n",
        "```\n",
        "## üîπ 3. Load the SavedModel\n",
        "\n",
        "Assume you‚Äôve already exported a trained TFOD2 model (`saved_model` directory).\n",
        "\n",
        "```python\n",
        "# Path to exported model\n",
        "detect_fn = tf.saved_model.load(\"exported-models/my_model/saved_model\")\n",
        "\n",
        "# Path to label map (pbtxt file)\n",
        "PATH_TO_LABELS = \"annotations/label_map.pbtxt\"\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
        "```\n",
        "## üîπ 4. Run Inference\n",
        "\n",
        "```python\n",
        "# Convert image to tensor\n",
        "input_tensor = tf.convert_to_tensor(image_np)\n",
        "input_tensor = input_tensor[tf.newaxis, ...]  # Add batch dimension\n",
        "\n",
        "# Perform inference\n",
        "detections = detect_fn(input_tensor)\n",
        "\n",
        "# Process output\n",
        "num_detections = int(detections.pop('num_detections'))\n",
        "detections = {key: value[0, :num_detections].numpy()\n",
        "              for key, value in detections.items()}\n",
        "detections['num_detections'] = num_detections\n",
        "detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "```\n",
        "## üîπ 5. Visualize Results\n",
        "\n",
        "```python\n",
        "# Copy for visualization\n",
        "image_np_with_detections = image_np.copy()\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "    image_np_with_detections,\n",
        "    detections['detection_boxes'],\n",
        "    detections['detection_classes'],\n",
        "    detections['detection_scores'],\n",
        "    category_index,\n",
        "    use_normalized_coordinates=True,\n",
        "    max_boxes_to_draw=200,\n",
        "    min_score_thresh=.50,\n",
        "    agnostic_mode=False)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(image_np_with_detections)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "```\n",
        "## ‚úÖ Summary\n",
        "\n",
        "1. Load image from a URL using `requests` + `PIL`.\n",
        "2. Convert it into a NumPy array.\n",
        "3. Load your TFOD2 `saved_model`.\n",
        "4. Run inference with `detect_fn(input_tensor)`.\n",
        "5. Visualize with `visualization_utils`.\n"
      ],
      "metadata": {
        "id": "PBrFj-QTSKsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  5 How do you install TensorFlow Object Detection API in Jupyter Notebook.\n",
        "Nice one üëç ‚Äî Installing **TensorFlow Object Detection API (TFOD2)** inside a **Jupyter Notebook** can be a little tricky because it has dependencies and a protobuf compilation step. Here‚Äôs a clean step-by-step guide:\n",
        "## üîπ 1. Install Dependencies\n",
        "\n",
        "Run these in a Jupyter cell:\n",
        "\n",
        "```python\n",
        "!pip install tensorflow==2.13\n",
        "!pip install tf-slim\n",
        "!pip install pycocotools\n",
        "!pip install lxml\n",
        "!pip install Cython\n",
        "!pip install contextlib2\n",
        "!pip install jupyter matplotlib opencv-python\n",
        "```\n",
        "## üîπ 2. Clone the TensorFlow Models Repository\n",
        "\n",
        "```python\n",
        "!git clone https://github.com/tensorflow/models.git\n",
        "```\n",
        "## üîπ 3. Compile Protobufs\n",
        "\n",
        "Protobufs are used for `.proto` configuration files inside TFOD2.\n",
        "\n",
        "```python\n",
        "!apt-get install -y protobuf-compiler\n",
        "!cd models/research && protoc object_detection/protos/*.proto --python_out=.\n",
        "```\n",
        "## üîπ 4. Install the Object Detection API\n",
        "\n",
        "```python\n",
        "!cp models/research/object_detection/packages/tf2/setup.py models/research/\n",
        "!cd models/research && pip install .\n",
        "```\n",
        "## üîπ 5. Verify Installation\n",
        "\n",
        "Run:\n",
        "\n",
        "```python\n",
        "import object_detection\n",
        "print(\"TFOD2 Installed Successfully!\")\n",
        "```\n",
        "\n",
        "If no error appears, the installation worked. ‚úÖ\n",
        "## üîπ 6. (Optional) Test the Installation\n",
        "\n",
        "```python\n",
        "!python models/research/object_detection/builders/model_builder_tf2_test.py\n",
        "```\n",
        "\n",
        "If all tests pass, the API is ready.\n",
        "## ‚úÖ Summary\n",
        "\n",
        "1. Install required Python dependencies.\n",
        "2. Clone the TensorFlow `models` repo.\n",
        "3. Compile protobuf files.\n",
        "4. Install TFOD2 with `pip install .`.\n",
        "5. Verify with `import object_detection`.\n"
      ],
      "metadata": {
        "id": "3yMPiAY_ShOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 How can you load a pre-trained TensorFlow Object Detection model.\n",
        "Great question üëç ‚Äî in **TensorFlow Object Detection API (TFOD2)**, you can easily load a **pre-trained model** (e.g., Faster R-CNN, SSD, EfficientDet) from the **TF Model Zoo**.\n",
        "\n",
        "Here‚Äôs the step-by-step:\n",
        "## üîπ 1. Download a Pre-trained Model\n",
        "\n",
        "Models are available in the **TensorFlow 2 Detection Model Zoo**.\n",
        "For example, let‚Äôs use **SSD MobileNet v2**:\n",
        "\n",
        "```python\n",
        "import os\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import wget\n",
        "\n",
        "# Download pre-trained model from TF Model Zoo\n",
        "MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
        "MODEL_DATE = '20200711'\n",
        "MODEL_TAR = f'{MODEL_NAME}.tar.gz'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/tf2/'\n",
        "MODEL_DIR = f\"{MODEL_NAME}/saved_model\"\n",
        "\n",
        "if not os.path.exists(MODEL_TAR):\n",
        "    wget.download(DOWNLOAD_BASE + MODEL_DATE + '/' + MODEL_TAR)\n",
        "\n",
        "# Extract the model\n",
        "if not os.path.exists(MODEL_NAME):\n",
        "    tar = tarfile.open(MODEL_TAR)\n",
        "    tar.extractall()\n",
        "    tar.close()\n",
        "```\n",
        "## üîπ 2. Load the SavedModel\n",
        "\n",
        "Each pre-trained model includes a **`saved_model`** directory. You load it like this:\n",
        "\n",
        "```python\n",
        "# Load the pre-trained model\n",
        "detect_fn = tf.saved_model.load(MODEL_DIR)\n",
        "print(\"Model loaded successfully!\")\n",
        "```\n",
        "## üîπ 3. Load Label Map\n",
        "\n",
        "The label map (mapping class IDs ‚Üí names) is also provided in the zoo.\n",
        "\n",
        "```python\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "PATH_TO_LABELS = 'models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
        "```\n",
        "## üîπ 4. Run Inference (Quick Test)\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "# Load an example image from the web\n",
        "IMAGE_URL = \"https://tensorflow.org/images/surf.jpg\"\n",
        "image = Image.open(requests.get(IMAGE_URL, stream=True).raw)\n",
        "image_np = np.array(image)\n",
        "\n",
        "# Prepare input tensor\n",
        "input_tensor = tf.convert_to_tensor(image_np)[tf.newaxis, ...]\n",
        "\n",
        "# Run inference\n",
        "detections = detect_fn(input_tensor)\n",
        "print(\"Detections:\", detections.keys())\n",
        "```\n",
        "## ‚úÖ Summary\n",
        "\n",
        "1. Download a pre-trained model from **TF Model Zoo**.\n",
        "2. Extract the `.tar.gz` and locate `saved_model/`.\n",
        "3. Load it with `tf.saved_model.load()`.\n",
        "4. Load the label map for class names.\n",
        "5. Run inference on images."
      ],
      "metadata": {
        "id": "LdHN8ySGS36E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7  How do you preprocess an image from the web for TFOD2 inference.\n",
        "Great question üëç ‚Äî when you run inference with **TensorFlow Object Detection API (TFOD2)** on an image from the **web**, you need to **preprocess it properly** so the model can accept it.\n",
        "\n",
        "Here‚Äôs the full pipeline:\n",
        "## üîπ 1. Load Image from the Web\n",
        "\n",
        "Use `requests` + `PIL` (or OpenCV) to fetch the image:\n",
        "\n",
        "```python\n",
        "import requests\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Example image URL\n",
        "IMAGE_URL = \"https://tensorflow.org/images/surf.jpg\"\n",
        "\n",
        "# Load image\n",
        "response = requests.get(IMAGE_URL, stream=True)\n",
        "image = Image.open(response.raw)\n",
        "\n",
        "# Convert to NumPy array (H, W, 3) in RGB format\n",
        "image_np = np.array(image)\n",
        "```\n",
        "## üîπ 2. Convert to Tensor for TFOD2\n",
        "\n",
        "TFOD2 expects a **batch of images** in tensor format:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Convert to TensorFlow tensor\n",
        "input_tensor = tf.convert_to_tensor(image_np)\n",
        "\n",
        "# Add batch dimension: (1, H, W, 3)\n",
        "input_tensor = input_tensor[tf.newaxis, ...]\n",
        "```\n",
        "## üîπ 3. Run Inference\n",
        "\n",
        "Assuming you already loaded the model (`detect_fn = tf.saved_model.load(path)`):\n",
        "\n",
        "```python\n",
        "detections = detect_fn(input_tensor)\n",
        "```\n",
        "## üîπ 4. Postprocessing (Optional Visualization)\n",
        "\n",
        "You usually want to **visualize bounding boxes**. TFOD2 provides utilities:\n",
        "\n",
        "```python\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import label_map_util\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load label map\n",
        "PATH_TO_LABELS = 'models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
        "\n",
        "# Copy image for visualization\n",
        "image_np_with_detections = image_np.copy()\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "    image_np_with_detections,\n",
        "    detections['detection_boxes'][0].numpy(),\n",
        "    detections['detection_classes'][0].numpy().astype(int),\n",
        "    detections['detection_scores'][0].numpy(),\n",
        "    category_index,\n",
        "    use_normalized_coordinates=True,\n",
        "    min_score_thresh=0.5)\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.imshow(image_np_with_detections)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "```\n",
        "## ‚úÖ Summary\n",
        "\n",
        "To **preprocess an image from the web for TFOD2 inference**:\n",
        "\n",
        "1. Fetch the image ‚Üí `requests + PIL`\n",
        "2. Convert ‚Üí NumPy array\n",
        "3. Convert ‚Üí TensorFlow tensor (`tf.convert_to_tensor`)\n",
        "4. Add batch dimension (`tf.newaxis`) ‚Üí shape `(1, H, W, 3)`\n",
        "5. Feed into `detect_fn` for inference."
      ],
      "metadata": {
        "id": "kRQAC70JTKHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8 How do you visualize bounding boxes for detected objects in TFOD2 inference.\n",
        "Perfect üëç ‚Äî once you run inference in **TensorFlow Object Detection API (TFOD2)**, you‚Äôll get predictions like:\n",
        "\n",
        "* `detection_boxes` ‚Üí bounding box coordinates (normalized: ymin, xmin, ymax, xmax)\n",
        "* `detection_classes` ‚Üí class IDs\n",
        "* `detection_scores` ‚Üí confidence scores\n",
        "\n",
        "To **visualize bounding boxes** on your image, you can use the built-in **`visualization_utils`** helper.\n",
        "## üîπ Full Example: Visualize Bounding Boxes\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import label_map_util\n",
        "import numpy as np\n",
        "\n",
        "# Assume 'detections' is the output of detect_fn(input_tensor)\n",
        "# and 'image_np' is the original NumPy image (H, W, 3)\n",
        "\n",
        "# Load COCO label map (or your custom one if fine-tuned)\n",
        "PATH_TO_LABELS = 'models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
        "\n",
        "# Copy original image for visualization\n",
        "image_np_with_detections = image_np.copy()\n",
        "\n",
        "# Draw bounding boxes, class labels, and scores on the image\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "    image_np_with_detections,\n",
        "    detections['detection_boxes'][0].numpy(),      # Bounding boxes\n",
        "    detections['detection_classes'][0].numpy().astype(np.int32),  # Class IDs\n",
        "    detections['detection_scores'][0].numpy(),    # Confidence scores\n",
        "    category_index,\n",
        "    use_normalized_coordinates=True,              # Boxes are normalized [0,1]\n",
        "    max_boxes_to_draw=50,\n",
        "    min_score_thresh=0.5,                         # Draw boxes above 50% confidence\n",
        "    agnostic_mode=False)\n",
        "\n",
        "# Display the image with bounding boxes\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(image_np_with_detections)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "```\n",
        "## üîπ What This Does\n",
        "\n",
        "* Draws bounding boxes on objects detected\n",
        "* Shows **class labels** (e.g., \"person\", \"car\")\n",
        "* Shows **confidence scores** (e.g., 0.92)\n",
        "‚úÖ **In short:**\n",
        "Use `viz_utils.visualize_boxes_and_labels_on_image_array()` with the outputs of inference (`detection_boxes`, `detection_classes`, `detection_scores`) and a label map to **overlay bounding boxes and labels on your image**.\n"
      ],
      "metadata": {
        "id": "n9WM9Nh1TKDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9 How do you define classes for custom training in TFOD2.\n",
        "Great question üëç ‚Äî when you train on a **custom dataset** in **TensorFlow Object Detection API (TFOD2)**, you must explicitly define your **custom classes** so the model knows what to predict. This is done using a **label map file** (`.pbtxt`).\n",
        "## üîπ 1. Create a Label Map File\n",
        "\n",
        "The label map tells TFOD2 the **mapping between class IDs and class names**.\n",
        "\n",
        "Example: Suppose you want to detect **cats** and **dogs**.\n",
        "Create a file called `label_map.pbtxt`:\n",
        "\n",
        "```protobuf\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'cat'\n",
        "}\n",
        "item {\n",
        "  id: 2\n",
        "  name: 'dog'\n",
        "}\n",
        "``\n",
        "‚ö†Ô∏è Rules:\n",
        "\n",
        "* IDs **must start at 1** (0 is reserved for background).\n",
        "* Each class needs a unique `id`.\n",
        "* Names should match your dataset annotations.\n",
        "## üîπ 2. Reference the Label Map in Your Pipeline Config\n",
        "\n",
        "In your **pipeline.config** file, specify the path to the label map:\n",
        "\n",
        "```protobuf\n",
        "label_map_path: \"annotations/label_map.pbtxt\"\n",
        "```\n",
        "\n",
        "This ensures that during training & inference, TFOD2 maps predictions to your custom class names.\n",
        "## üîπ 3. Use the Same Label Map for Inference\n",
        "\n",
        "When visualizing results, load the same file:\n",
        "\n",
        "```python\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "PATH_TO_LABELS = \"annotations/label_map.pbtxt\"\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
        "```\n",
        "## üîπ 4. Example with 3 Custom Classes\n",
        "\n",
        "If your dataset has `apple`, `banana`, and `orange`:\n",
        "\n",
        "```protobuf\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'apple'\n",
        "}\n",
        "item {\n",
        "  id: 2\n",
        "  name: 'banana'\n",
        "}\n",
        "item {\n",
        "  id: 3\n",
        "  name: 'orange'\n",
        "}\n",
        "```\n",
        "‚úÖ **Summary:**\n",
        "\n",
        "* Define classes in a `.pbtxt` label map file with `id` and `name`.\n",
        "* Reference it in `pipeline.config` for training.\n",
        "* Use it again for inference to decode predictions.\n"
      ],
      "metadata": {
        "id": "hjw4tw1yTz6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 How do you define classes for custom training in TFOD2.\n",
        "In **TensorFlow Object Detection API (TFOD2)**, classes for **custom training** are defined through a **label map file** (`.pbtxt`). This file is what tells the model **which class IDs correspond to which object names**.\n",
        "## üîπ 1. Create a Label Map File\n",
        "\n",
        "Suppose you want to train a detector for **cats** and **dogs**. You would create a file called `label_map.pbtxt` inside your `annotations/` folder:\n",
        "\n",
        "```protobuf\n",
        "item {\n",
        "  id: 1\n",
        "  name: \"cat\"\n",
        "}\n",
        "item {\n",
        "  id: 2\n",
        "  name: \"dog\"\n",
        "}\n",
        "```\n",
        "\n",
        "### ‚ö†Ô∏è Important Rules:\n",
        "\n",
        "* Class IDs **must start at 1** (0 is reserved for background).\n",
        "* Every class must have a unique ID.\n",
        "* The names should exactly match the class names in your dataset annotations.\n",
        "## üîπ 2. Reference the Label Map in Pipeline Config\n",
        "\n",
        "Inside your **`pipeline.config`** file, point to the label map:\n",
        "\n",
        "```protobuf\n",
        "train_input_reader: {\n",
        "  label_map_path: \"annotations/label_map.pbtxt\"\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"annotations/train.record\"\n",
        "  }\n",
        "}\n",
        "\n",
        "eval_input_reader: {\n",
        "  label_map_path: \"annotations/label_map.pbtxt\"\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"annotations/test.record\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "This ensures both **training** and **evaluation** know which class IDs map to which names.\n",
        "## üîπ 3. Use the Same Label Map for Inference\n",
        "\n",
        "When you later run inference, you‚Äôll load the same label map to decode predictions:\n",
        "\n",
        "```python\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "PATH_TO_LABELS = \"annotations/label_map.pbtxt\"\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
        "```\n",
        "## üîπ 4. Example with 3 Custom Classes\n",
        "\n",
        "If your dataset has **apple, banana, orange**, your label map would look like:\n",
        "\n",
        "```protobuf\n",
        "item {\n",
        "  id: 1\n",
        "  name: \"apple\"\n",
        "}\n",
        "item {\n",
        "  id: 2\n",
        "  name: \"banana\"\n",
        "}\n",
        "item {\n",
        "  id: 3\n",
        "  name: \"orange\"\n",
        "}\n",
        "```\n",
        "‚úÖ **Summary:**\n",
        "To define classes in TFOD2:\n",
        "\n",
        "1. Create a **label map (`.pbtxt`)** listing all your classes with unique IDs.\n",
        "2. Reference it in the **pipeline.config** file for training and evaluation.\n",
        "3. Load it again during inference for visualizing predictions.\n"
      ],
      "metadata": {
        "id": "Zzf-RN-8UMes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11 How do you resize an image before detecting object1 ?\n",
        "Good question üëç ‚Äî resizing an image before object detection is often required because most detection models (in **TFOD2**, **Detectron2**, or **YOLO**) expect input images of a fixed size (e.g., 320√ó320, 640√ó640).\n",
        "\n",
        "Here‚Äôs how you can do it:\n",
        "## üîπ 1. Using **OpenCV (cv2)**\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "\n",
        "# Load an image\n",
        "image = cv2.imread(\"image.jpg\")\n",
        "\n",
        "# Resize to 320x320\n",
        "resized_image = cv2.resize(image, (320, 320))\n",
        "\n",
        "# Convert BGR to RGB (if needed for TFOD2 or Detectron2)\n",
        "resized_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n",
        "```\n",
        "## üîπ 2. Using **TensorFlow**\n",
        "\n",
        "If you‚Äôre working directly with TFOD2 inference pipeline:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load image\n",
        "image = Image.open(\"image.jpg\")\n",
        "image_np = np.array(image)\n",
        "\n",
        "# Resize to 320x320\n",
        "resized_image = tf.image.resize(image_np, (320, 320))\n",
        "resized_image = tf.cast(resized_image, tf.uint8).numpy()\n",
        "```\n",
        "## üîπ 3. Automatic Resizing in TFOD2\n",
        "\n",
        "In **TFOD2**, if you‚Äôre using a pre-trained model from the TF model zoo, resizing often happens **inside the pipeline**. For example, if you use a `ssd_mobilenet_v2_fpnlite_320x320` model, it **expects 320√ó320 input**.\n",
        "So you can just load the image and convert it into a tensor:\n",
        "\n",
        "```python\n",
        "input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, axis=0), dtype=tf.float32)\n",
        "detections = detect_fn(input_tensor)  # TFOD2 handles resizing internally\n",
        "```\n",
        "## üîπ 4. Best Practice\n",
        "\n",
        "* If you resize manually, keep the **aspect ratio** to avoid distortion. You can **pad** the image (letterboxing) instead of stretching it.\n",
        "* If you rely on the model pipeline, it automatically resizes your image to the expected size.\n",
        "‚úÖ **Summary:**\n",
        "You can resize images with OpenCV, TensorFlow, or Pillow before feeding them into your detector. In TFOD2, the model pipeline usually handles resizing automatically based on the model architecture (e.g., 320√ó320, 640√ó640).\n"
      ],
      "metadata": {
        "id": "GHh6oiVUUMat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12  How can you apply a color filter (e.g., red filter) to an image ?\n",
        "You can apply a color filter (like a red filter) to an image in multiple ways depending on the library/framework you‚Äôre using. Below are common approaches in **Python** using **OpenCV** and **PIL (Pillow)**:\n",
        "## **1. Using OpenCV**\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load image\n",
        "image = cv2.imread(\"image.jpg\")\n",
        "\n",
        "# Create a red filter mask (only keep red channel)\n",
        "red_filter = np.zeros_like(image)\n",
        "red_filter[:, :, 2] = image[:, :, 2]   # Keep only red channel\n",
        "\n",
        "# Combine with original image (adjust intensity if needed)\n",
        "red_image = cv2.addWeighted(image, 0.5, red_filter, 0.5, 0)\n",
        "\n",
        "# Show result\n",
        "cv2.imshow(\"Red Filtered Image\", red_image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "```\n",
        "\n",
        "‚úÖ This keeps the red tones dominant.\n",
        "## **2. Using Pillow (PIL)**\n",
        "\n",
        "```python\n",
        "from PIL import Image, ImageEnhance\n",
        "\n",
        "# Open image\n",
        "img = Image.open(\"image.jpg\").convert(\"RGB\")\n",
        "\n",
        "# Split channels\n",
        "r, g, b = img.split()\n",
        "\n",
        "# Suppress green and blue channels\n",
        "red_img = Image.merge(\"RGB\", (r, g.point(lambda x: 0), b.point(lambda x: 0)))\n",
        "\n",
        "# Blend with original if you want softer effect\n",
        "final_img = Image.blend(img, red_img, alpha=0.5)\n",
        "\n",
        "# Save / Show\n",
        "final_img.show()\n",
        "```\n",
        "\n",
        "‚úÖ This emphasizes the **red channel**.\n",
        "## **3. Quick Method with NumPy Tinting**\n",
        "\n",
        "If you just want to tint the whole image red (like a red transparent overlay):\n",
        "\n",
        "```python\n",
        "overlay = np.full(image.shape, (0, 0, 255), dtype=np.uint8)  # pure red\n",
        "red_tinted = cv2.addWeighted(image, 0.7, overlay, 0.3, 0)\n",
        "```\n"
      ],
      "metadata": {
        "id": "e7Q9f8q1U-0x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}