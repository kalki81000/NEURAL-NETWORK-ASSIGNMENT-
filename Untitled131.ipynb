{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPn6JOVKps613Zi9pLT9fJT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalki81000/NEURAL-NETWORK-ASSIGNMENT-/blob/main/Untitled131.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GANs"
      ],
      "metadata": {
        "id": "qgfdCbs8mFrF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv9INft5mBS6"
      },
      "outputs": [],
      "source": [
        "#1 What does GAN stand for, and what is its main purpose ?\n",
        "GAN stands for **Generative Adversarial Network**.\n",
        "Its **main purpose** is to generate new, synthetic data that is similar to real data. For example, GANs can create realistic images of people, objects, or scenes that never existed, generate audio that sounds human, or even produce text and video\n",
        "A GAN consists of two neural networks that compete with each other:.\n",
        "1. **Generator** ‚Äì Creates fake data (e.g., images, audio, etc.) that resembles real data.\n",
        "2. **Discriminator** ‚Äì Tries to distinguish between real data and the fake data produced by the generator.\n",
        "Through this adversarial process, the generator becomes better at creating realistic data, while the discriminator improves at detecting fakes.\n",
        "üëâ In short: GANs are mainly used for **data generation, image synthesis, and creative AI applications*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Explain the concept of the \"discriminator\" in GANs ?\n",
        "Great question!\n",
        "In a **Generative Adversarial Network (GAN)**, the **discriminator** is one of the two main components (the other is the **generator**).\n",
        "### üîπ Concept of the Discriminator:\n",
        "* The **discriminator** is a **neural network classifier** whose job is to **distinguish between real data (from the training set) and fake data (produced by the generator)**.\n",
        "* It outputs a probability:\n",
        "  * **Close to 1** ‚Üí if it thinks the input is *real* (from actual data).\n",
        "  * **Close to 0** ‚Üí if it thinks the input is *fake* (from the generator).\n",
        "### üîπ How it Works in Training:\n",
        "1. The **generator** produces synthetic (fake) data.\n",
        "2. The **discriminator** is given a mix of real and fake data.\n",
        "3. The discriminator tries to correctly classify each input as *real* or *fake*.\n",
        "4. It gets trained using **binary cross-entropy loss**, improving its ability to spot fakes.\n",
        "5. The **feedback (loss signal)** from the discriminator is sent back to the generator, helping the generator improve and make more realistic outputs.\n",
        "### üîπ Analogy:\n",
        "Think of the **discriminator as a detective**:\n",
        "* Real data = real money.\n",
        "* Fake data = counterfeit money created by the generator.\n",
        "* The detective (discriminator) tries to tell which is real and which is fake.\n",
        "* Over time, both the counterfeiter (generator) and the detective (discriminator) get better at their jobs.\n",
        "üëâ In summary:\n",
        "The **discriminator in GANs is a binary classifier that learns to distinguish real data from generated (fake) data, and its feedback guides the generator to produce more realistic outputs.**"
      ],
      "metadata": {
        "id": "h-14QM00nIMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 How does a GAN work ?\n",
        "Alright, let‚Äôs break it down step by step so it‚Äôs very clear\n",
        "## üîπ How a GAN Works\n",
        "\n",
        "A **Generative Adversarial Network (GAN)** is made up of two neural networks that ‚Äúcompete‚Äù against each other:\n",
        "\n",
        "1. **Generator (G)** ‚Üí creates fake data.\n",
        "2. **Discriminator (D)** ‚Üí judges whether data is real (from the training set) or fake (from the generator).\n",
        "### üåÄ Step-by-Step Process:\n",
        "\n",
        "1. **Noise Input**\n",
        "\n",
        "   * The process starts with the generator receiving a random noise vector (just numbers, usually sampled from a normal or uniform distribution).\n",
        "   * This is the ‚Äúseed‚Äù for creating fake data.\n",
        "\n",
        "2. **Generator Creates Fake Data**\n",
        "\n",
        "   * The generator transforms this random noise into data resembling the real dataset (e.g., an image, sound, or text).\n",
        "\n",
        "3. **Discriminator Evaluates Data**\n",
        "\n",
        "   * The discriminator sees both **real data** (from the dataset) and **fake data** (from the generator).\n",
        "   * It outputs a probability: ‚ÄúHow real does this look?‚Äù\n",
        "\n",
        "4. **Training Feedback**\n",
        "\n",
        "   * If the discriminator correctly identifies real vs. fake, it gets rewarded.\n",
        "   * If it makes mistakes, it updates its parameters (weights) to improve classification.\n",
        "   * At the same time, the generator learns from the discriminator‚Äôs feedback and tries to produce *more realistic* fake data to fool it next time.\n",
        "\n",
        "5. **Adversarial Learning Loop**\n",
        "\n",
        "   * Over many training rounds, the generator gets better at making realistic data, and the discriminator gets better at spotting fakes.\n",
        "   * The ‚Äúgame‚Äù continues until the generator produces data that is so convincing the discriminator can‚Äôt easily tell real from fake (50-50 chance).\n",
        "### üîπ Analogy\n",
        "\n",
        "Think of a **counterfeiter and a police officer**:\n",
        "\n",
        "* The **generator** = counterfeiter making fake currency.\n",
        "* The **discriminator** = police officer trying to catch counterfeits.\n",
        "* Over time:\n",
        "\n",
        "  * The counterfeiter improves their fake money.\n",
        "  * The police officer gets better at detecting fakes.\n",
        "* Eventually, the fakes become so good that even the police struggle to distinguish them.\n",
        "### üîπ Training Objective (Mathematics, simplified)\n",
        "\n",
        "The GAN training objective is a **minimax game**:\n",
        "\n",
        "$$\n",
        "\\min_G \\max_D \\; V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]\n",
        "$$\n",
        "\n",
        "* $D(x)$ ‚Üí probability the discriminator assigns that $x$ is real.\n",
        "* $G(z)$ ‚Üí fake data generated from noise $z$.\n",
        "* Goal:\n",
        "\n",
        "  * **Discriminator** tries to maximize correct classification.\n",
        "  * **Generator** tries to minimize discriminator success (i.e., fool it).\n",
        "‚úÖ **In short**:\n",
        "A GAN works by setting up a competition between two networks ‚Äî the **generator**, which creates fake data, and the **discriminator**, which evaluates authenticity. Through this adversarial process, the generator gradually learns to produce highly realistic synthetic data.\n",
        "\n"
      ],
      "metadata": {
        "id": "9GmcVNWBon7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 What is the generator's role in a GAN ?\n",
        "Great follow-up\n",
        "In a **Generative Adversarial Network (GAN)**, the **generator** is one of the two key players (the other is the discriminator).\n",
        "## üîπ Role of the Generator in a GAN\n",
        "* The **generator‚Äôs job** is to **create fake data that looks as close to the real data as possible**, so it can fool the discriminator.\n",
        "* It **starts from random noise** (a vector of random numbers) and learns how to transform that noise into structured, meaningful data (like an image, audio clip, or text).\n",
        "### üåÄ How the Generator Works:\n",
        "\n",
        "1. **Takes noise as input**\n",
        "\n",
        "   * Input is usually a random vector sampled from a probability distribution (e.g., Gaussian).\n",
        "\n",
        "2. **Produces synthetic data**\n",
        "\n",
        "   * The generator is a neural network that maps this noise into data space (e.g., pixels of an image).\n",
        "\n",
        "3. **Tries to fool the discriminator**\n",
        "\n",
        "   * At the beginning, the generator produces nonsense, but with training, it learns to generate realistic samples that the discriminator has trouble distinguishing from real ones.\n",
        "\n",
        "4. **Improves from feedback**\n",
        "\n",
        "   * The discriminator provides feedback (gradients) when it detects fakes.\n",
        "   * The generator uses this information to adjust its parameters and improve the realism of its outputs.\n",
        "### üîπ Analogy\n",
        "\n",
        "Think of the **generator as a counterfeiter**:\n",
        "\n",
        "* It starts by making very poor fake money.\n",
        "* The **discriminator (police officer)** easily catches the fakes.\n",
        "* Over time, the counterfeiter improves, learning what makes money look ‚Äúreal.‚Äù\n",
        "* Eventually, the fakes get so convincing that the police struggle to tell real from fake.\n",
        "‚úÖ **In short:**\n",
        "The **generator‚Äôs role in a GAN is to learn how to produce synthetic data that is indistinguishable from real data**, by constantly improving through feedback from the discriminator.\n"
      ],
      "metadata": {
        "id": "aZTPS4AWonsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 What is the loss function used in the training of GANs ?\n",
        "Excellent question üëç This goes into the **heart of how GANs learn**.\n",
        "## üîπ Loss Function in GANs\n",
        "\n",
        "GANs are trained using a **minimax game** between the **discriminator** and the **generator**.\n",
        "\n",
        "The **value function** (loss) is:\n",
        "\n",
        "$$\n",
        "\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log (1 - D(G(z)))]\n",
        "$$\n",
        "### üîπ Breaking it Down\n",
        "\n",
        "1. **Discriminator Loss**\n",
        "\n",
        "   * The discriminator wants to correctly classify **real samples** as real and **fake samples** as fake.\n",
        "   * Its loss is:\n",
        "\n",
        "   $$\n",
        "   L_D = - \\big( \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log (1 - D(G(z)))] \\big)\n",
        "   $$\n",
        "\n",
        "   * This is basically **binary cross-entropy** for real vs fake classification.\n",
        "2. **Generator Loss**\n",
        "\n",
        "   * The generator wants to fool the discriminator into thinking fake data is real.\n",
        "   * Its original minimax loss is:\n",
        "\n",
        "   $$\n",
        "   L_G = \\mathbb{E}_{z \\sim p_z(z)}[\\log (1 - D(G(z)))]\n",
        "   $$\n",
        "\n",
        "   * BUT in practice, this loss can lead to **vanishing gradients** when $D(G(z))$ is close to 0.\n",
        "   * So a common alternative is the **non-saturating loss**:\n",
        "\n",
        "   $$\n",
        "   L_G = - \\mathbb{E}_{z \\sim p_z(z)}[\\log D(G(z))]\n",
        "   $$\n",
        "\n",
        "   * This makes training more stable.\n",
        "### üîπ Summary of Loss Functions\n",
        "\n",
        "* **Discriminator**: tries to **maximize** log probability of real being real & fake being fake.\n",
        "* **Generator**: tries to **minimize** the probability that its fakes are detected (or maximize probability that fakes are classified as real).\n",
        "‚úÖ **In short:**\n",
        "\n",
        "* GANs use a **minimax loss function** based on binary cross-entropy.\n",
        "* **Discriminator** minimizes classification error between real and fake.\n",
        "* **Generator** minimizes the chance of its fakes being detected (often using the non-saturating variant for stability).\n"
      ],
      "metadata": {
        "id": "3ukQP2mzonoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 What is the difference between a WGAN and a traditional GAN ?\n",
        "Great question üôå ‚Äî this is a key topic in understanding why **Wasserstein GANs (WGANs)** were introduced!\n",
        "## üîπ Traditional GAN (Goodfellow, 2014)\n",
        "\n",
        "* Uses a **minimax loss** with **binary cross-entropy** (log loss).\n",
        "* The discriminator outputs a probability:\n",
        "\n",
        "  $$\n",
        "  D(x) \\in [0, 1]\n",
        "  $$\n",
        "* Problem: Training is **unstable** because of issues like:\n",
        "\n",
        "  * **Vanishing gradients** (when the discriminator gets too good, the generator stops learning).\n",
        "  * **Mode collapse** (generator produces limited variety).\n",
        "## üîπ Wasserstein GAN (WGAN, Arjovsky et al., 2017)\n",
        "\n",
        "* Replaces the discriminator with a **critic** (not a classifier).\n",
        "* Instead of predicting real/fake probabilities, the critic outputs a **real number (a score)** indicating how ‚Äúreal‚Äù the data looks.\n",
        "\n",
        "### Key differences:\n",
        "\n",
        "1. **Loss Function**\n",
        "\n",
        "   * **GAN:**\n",
        "\n",
        "     $$\n",
        "     \\min_G \\max_D \\; \\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]\n",
        "     $$\n",
        "   * **WGAN (Wasserstein distance):**\n",
        "\n",
        "     $$\n",
        "     L = \\mathbb{E}_{x \\sim p_{data}}[f(x)] - \\mathbb{E}_{z \\sim p_z(z)}[f(G(z))]\n",
        "     $$\n",
        "\n",
        "     where $f$ (the critic) approximates the **Earth Mover‚Äôs Distance (Wasserstein-1 distance)** between real and generated distributions.\n",
        "2. **Critic vs Discriminator**\n",
        "\n",
        "   * **GAN:** Discriminator outputs probability (real/fake).\n",
        "   * **WGAN:** Critic outputs a score (no sigmoid activation).\n",
        "3. **Gradient Stability**\n",
        "\n",
        "   * **GAN:** Often suffers from vanishing gradients ‚Üí unstable training.\n",
        "   * **WGAN:** Wasserstein distance provides smoother, more informative gradients ‚Üí stable training.\n",
        "4. **Weight Clipping / Gradient Penalty**\n",
        "\n",
        "   * WGAN enforces a **Lipschitz constraint** on the critic to make Wasserstein distance valid.\n",
        "   * Initially done with **weight clipping** (forcing weights into a small range).\n",
        "   * Improved with **WGAN-GP** (gradient penalty) ‚Üí more stable.\n",
        "5. **Mode Collapse**\n",
        "\n",
        "   * **GAN:** Can collapse (generator only produces a few samples).\n",
        "   * **WGAN:** Reduces mode collapse by providing better learning signals.\n",
        "## üîπ Simple Analogy\n",
        "\n",
        "* **Traditional GAN**: Discriminator says **‚Äúreal (1)‚Äù or ‚Äúfake (0)‚Äù** ‚Äî very black-and-white.\n",
        "* **WGAN**: Critic says **‚Äúthis looks 0.9 real, that looks -1.5 real‚Äù** ‚Äî giving a *continuous score* so the generator gets richer feedback.\n",
        "‚úÖ **In short:**\n",
        "\n",
        "* **GAN:** Uses binary cross-entropy, outputs probabilities, often unstable.\n",
        "* **WGAN:** Uses Wasserstein distance, outputs scores, provides stable gradients and reduces mode collapse.\n"
      ],
      "metadata": {
        "id": "vRaMNn1LqlPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 How does the training of the generator differ from that of the discriminator ?\n",
        "Great point to clarify üëç The **generator** and **discriminator** in a GAN are trained very differently because they have *opposite objectives*. Let‚Äôs break it down:\n",
        "## üîπ 1. **Discriminator Training**\n",
        "\n",
        "* **Goal:** Learn to distinguish **real data** from **fake data**.\n",
        "* **Input:** A batch of real samples (from dataset) + fake samples (from generator).\n",
        "* **Output:** Probability that each sample is real.\n",
        "* **Loss Function:**\n",
        "\n",
        "  $$\n",
        "  L_D = - \\Big( \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log (1 - D(G(z)))] \\Big)\n",
        "  $$\n",
        "* **Updates:**\n",
        "\n",
        "  * Minimize classification error (real ‚Üí 1, fake ‚Üí 0).\n",
        "  * Parameters updated using backpropagation (gradient descent).\n",
        "## üîπ 2. **Generator Training**\n",
        "\n",
        "* **Goal:** Learn to create **fake data that looks real enough to fool the discriminator**.\n",
        "* **Input:** Random noise vector $z$.\n",
        "* **Output:** Synthetic (fake) data sample.\n",
        "* **Loss Function (Non-saturating variant, common in practice):**\n",
        "\n",
        "  $$\n",
        "  L_G = - \\mathbb{E}_{z \\sim p_z(z)}[\\log D(G(z))]\n",
        "  $$\n",
        "* **Updates:**\n",
        "\n",
        "  * The generator does **not** get direct labels (no ‚Äúground truth‚Äù for fakes).\n",
        "  * Instead, it updates its parameters based on **feedback from the discriminator**.\n",
        "  * If the discriminator is easily spotting fakes, the generator adjusts to make more realistic data.\n",
        "## üîπ Key Differences in Training\n",
        "\n",
        "| Aspect              | **Discriminator (D)**                             | **Generator (G)**                             |\n",
        "| ------------------- | ------------------------------------------------- | --------------------------------------------- |\n",
        "| **Role**            | Classifier (real vs fake)                         | Creator (produces fake data)                  |\n",
        "| **Input**           | Real + fake data                                  | Random noise vector                           |\n",
        "| **Output**          | Probability (real/fake)                           | Synthetic sample                              |\n",
        "| **Learning Signal** | From real/fake labels                             | From discriminator‚Äôs feedback                 |\n",
        "| **Loss**            | Binary cross-entropy for real/fake classification | Trick discriminator into outputting ‚Äúreal‚Äù    |\n",
        "| **Updates**         | Directly learns from ground-truth labels          | Indirectly learns via discriminator gradients |\n",
        "‚úÖ **In short:**\n",
        "\n",
        "* The **discriminator** learns from **labeled data (real=1, fake=0)** to classify correctly.\n",
        "* The **generator** learns **only from the discriminator‚Äôs feedback**, trying to improve until its outputs are indistinguishable from real data."
      ],
      "metadata": {
        "id": "1t7eSN8TqlLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8 What is a DCGAN, and how is it different from a traditional GAN ?\n",
        "Perfect question üôå ‚Äî this is where GANs start becoming *really powerful* for images.\n",
        "# üîπ What is a **DCGAN**?\n",
        "\n",
        "**DCGAN** stands for **Deep Convolutional Generative Adversarial Network**.\n",
        "It is a special type of GAN that uses **Convolutional Neural Networks (CNNs)** in both the generator and discriminator.\n",
        "\n",
        "üìå Introduced in the paper *‚ÄúUnsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks‚Äù (Radford et al., 2015)*.\n",
        "# üîπ How DCGAN Differs from a Traditional GAN\n",
        "\n",
        "Traditional GANs (Goodfellow, 2014):\n",
        "\n",
        "* Generator and Discriminator are often just **fully connected (dense) networks**.\n",
        "* Struggle to handle **image data** efficiently ‚Üí poor quality, blurry outputs.\n",
        "\n",
        "DCGANs:\n",
        "\n",
        "* Replace dense layers with **convolutional (and transposed convolutional) layers**, making them *much better for image synthesis*.\n",
        "# üîπ Key Architectural Differences\n",
        "\n",
        "| Feature                                  | **Traditional GAN**           | **DCGAN**                                                                                                                                                        |\n",
        "| ---------------------------------------- | ----------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Generator**                            | Fully connected layers        | Uses **transposed convolutions** (a.k.a. deconvolutions) to upsample noise into an image                                                                         |\n",
        "| **Discriminator**                        | Fully connected layers        | Uses **convolutions** to extract image features                                                                                                                  |\n",
        "| **Stability**                            | Often unstable, mode collapse | More stable due to CNN inductive biases                                                                                                                          |\n",
        "| **Image Quality**                        | Blurry, unrealistic images    | Sharper, more detailed, realistic images                                                                                                                         |\n",
        "| **Design Guidelines (from DCGAN paper)** | None specific                 | - Use strided convolutions instead of pooling <br> - Remove fully connected layers <br> - Use BatchNorm <br> - Use ReLU in Generator, LeakyReLU in Discriminator |\n",
        "# üîπ Example Flow in a DCGAN\n",
        "* **Generator**: Noise vector $z \\sim \\mathcal{N}(0,1)$ ‚Üí transposed convolutions ‚Üí fake image.\n",
        "* **Discriminator**: Fake/real image ‚Üí convolution layers ‚Üí probability (real vs fake).\n",
        "# üîπ Why DCGANs Matter\n",
        "* Produced the **first high-quality synthetic images** compared to original GANs.\n",
        "* Served as the foundation for **modern image GANs** like Pix2Pix, CycleGAN, StyleGAN, etc.\n",
        "‚úÖ **In short:**\n",
        "A **DCGAN** is a GAN that uses **CNN-based architectures** (instead of dense networks), making it especially powerful for generating realistic images.\n"
      ],
      "metadata": {
        "id": "BeaLcx6rqlI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9 Explain the concept of \"controllable generation\" in the context of GAN:\n",
        "Nice one üëå ‚Äî this dives into one of the *biggest advances* in GAN research.\n",
        "## üîπ Controllable Generation in GANs\n",
        "\n",
        "In a normal GAN, the **generator takes random noise as input** and produces data (like an image).\n",
        "üëâ But this process is *uncontrolled* ‚Äî we can‚Äôt easily decide **what kind of image** will be generated.\n",
        "\n",
        "**Controllable generation** means:\n",
        "We modify GANs so that we can **guide or condition the generator** to produce outputs with specific properties (e.g., generating a smiling face, a car of a certain color, or a digit ‚Äú7‚Äù instead of random digits).\n",
        "## üîπ How Controllable Generation Works\n",
        "\n",
        "There are several approaches:\n",
        "\n",
        "### 1. **Conditional GAN (cGAN)**\n",
        "\n",
        "* Instead of just noise, we also give the generator a **label or condition** (e.g., ‚Äúdigit 5‚Äù from MNIST).\n",
        "* The discriminator also receives this label to check if the generated image **matches the condition**.\n",
        "* Example: Generate a ‚Äúred car‚Äù image instead of just any car.\n",
        "\n",
        "$$\n",
        "G(z, y) \\quad , \\quad D(x, y)\n",
        "$$\n",
        "\n",
        "(where $y$ = condition like class label, attributes, or text).\n",
        "### 2. **Latent Space Control**\n",
        "\n",
        "* GANs learn a **latent space** (compressed representation of data).\n",
        "* By moving in certain directions in this space, we can control properties of outputs.\n",
        "* Example: In a face GAN, sliding along one axis might make the person **smile more**, another axis might make them **wear glasses**.\n",
        "* Used in **StyleGAN** ‚Üí controls things like age, gender, hairstyle, etc.\n",
        "### 3. **Attribute-based Control**\n",
        "\n",
        "* GANs can be trained with additional attribute information (e.g., ‚Äúsmiling = 1, not smiling = 0‚Äù).\n",
        "* Later, we can set these attributes manually to control the output.\n",
        "## üîπ Analogy\n",
        "\n",
        "Imagine a **musical instrument**:\n",
        "\n",
        "* A normal GAN is like pressing random keys ‚Üí you get random sounds (images).\n",
        "* A controllable GAN is like playing specific notes ‚Üí you choose whether to play jazz, rock, or classical.\n",
        "## üîπ Why Controllable Generation is Important\n",
        "\n",
        "* **Image editing** (change a person‚Äôs hair color or expression).\n",
        "* **Data augmentation** (generate labeled data for rare classes).\n",
        "* **Creative applications** (art, design, video games).\n",
        "* **Medical imaging** (generate diseased vs healthy samples for training).\n",
        "‚úÖ **In short:**\n",
        "**Controllable generation in GANs** is about giving us the ability to **guide what the generator creates**, either by conditioning on labels, text, or by manipulating the latent space, instead of just producing random outputs.\n"
      ],
      "metadata": {
        "id": "9FqNdi3ersSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 What is the primary goal of training a GAN ?\n",
        "The **primary goal of training a GAN** is:\n",
        "\n",
        "üëâ **To make the generator learn the true data distribution so it can produce synthetic data that looks indistinguishable from real data.**\n",
        "### üîπ What that means:\n",
        "\n",
        "* The **generator (G)** keeps improving until the fake data it produces is so realistic that the **discriminator (D)** can no longer tell the difference between real and fake.\n",
        "* At this point:\n",
        "\n",
        "  * The generator‚Äôs distribution $p_g$ ‚âà real data distribution $p_{data}$.\n",
        "  * The discriminator‚Äôs accuracy drops to about **50%** (random guessing).\n",
        "‚úÖ **In short:**\n",
        "Training a GAN aims to reach a balance where the generator produces **realistic synthetic data**, and the discriminator **cannot distinguish it from real data**."
      ],
      "metadata": {
        "id": "Uk-6wgqFrsPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11 What are the limitations of GANs ?\n",
        "Great question üëç ‚Äî GANs are powerful but **far from perfect**. Researchers have identified several important **limitations and challenges**:\n",
        "## üîπ 1. **Training Instability**\n",
        "\n",
        "* GANs are notoriously **hard to train**.\n",
        "* The generator and discriminator are in a **minimax game**, which doesn‚Äôt always converge smoothly.\n",
        "* Training can oscillate, diverge, or collapse.\n",
        "## üîπ 2. **Mode Collapse**\n",
        "\n",
        "* The generator may learn to produce only a **limited variety** of samples, ignoring much of the data distribution.\n",
        "* Example: Instead of generating all digits (0‚Äì9), it may only generate ‚Äú3s‚Äù because that‚Äôs enough to fool the discriminator.\n",
        "## üîπ 3. **Sensitive to Hyperparameters**\n",
        "\n",
        "* GAN performance depends heavily on choices like:\n",
        "\n",
        "  * Learning rate\n",
        "  * Batch size\n",
        "  * Architecture design\n",
        "* Small mistakes can lead to failure (e.g., discriminator overpowering generator).\n",
        "## üîπ 4. **Evaluation is Difficult**\n",
        "\n",
        "* Unlike classification models, there‚Äôs no clear accuracy metric.\n",
        "* Common metrics (like Inception Score, FID) are **imperfect**.\n",
        "* Human judgment is often required to judge realism.\n",
        "## üîπ 5. **Require Large, High-Quality Datasets**\n",
        "\n",
        "* GANs need **a lot of training data** to generate realistic samples.\n",
        "* With limited or biased data, outputs may be unrealistic or biased too.\n",
        "## üîπ 6. **Computationally Expensive**\n",
        "\n",
        "* GANs (especially modern ones like **StyleGAN**) need powerful GPUs and long training times.\n",
        "* Hard for resource-limited researchers to train from scratch.\n",
        "## üîπ 7. **No Explicit Likelihood**\n",
        "\n",
        "* Unlike some generative models (like Variational Autoencoders), GANs don‚Äôt give a clear probability for generated samples.\n",
        "* This makes **theoretical analysis and comparisons harder**.\n",
        "## üîπ 8. **Ethical & Misuse Concerns**\n",
        "\n",
        "* GANs can be used for **deepfakes**, misinformation, fake identities, and copyright issues.\n",
        "* Raises **trust and ethical concerns** in society.\n",
        "‚úÖ **In short:**\n",
        "GANs are powerful tools for realistic data generation, but they suffer from **training instability, mode collapse, lack of good evaluation metrics, high data/computational needs, and potential for misuse**."
      ],
      "metadata": {
        "id": "hcxzsZMMrsLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12 What are StyleGANs, and what makes them unique ?\n",
        "Great question üî• ‚Äî **StyleGANs** are one of the most important breakthroughs in GAN research!\n",
        "## üîπ What is StyleGAN?\n",
        "\n",
        "**StyleGAN (Style-based GAN)** is a **type of GAN architecture** introduced by NVIDIA in 2018 (Karras et al.) that produces **extremely high-quality, realistic images**, often indistinguishable from real photos.\n",
        "\n",
        "It is the foundation for many **state-of-the-art image synthesis systems** (e.g., those used in face generation like *thispersondoesnotexist.com*).\n",
        "## üîπ What Makes StyleGAN Unique\n",
        "\n",
        "StyleGAN introduced several key innovations compared to earlier GANs:\n",
        "\n",
        "### 1. **Style-based Generator Architecture**\n",
        "\n",
        "* In traditional GANs: noise vector $z$ is fed directly into the generator.\n",
        "* In StyleGAN:\n",
        "\n",
        "  * Noise $z$ first passes through a **mapping network** ‚Üí produces an intermediate latent vector $w$.\n",
        "  * This $w$ controls styles (features) at each layer of the generator.\n",
        "* Result: **fine-grained control** over features like pose, hair, expression, or background.\n",
        "### 2. **Multi-scale Style Control**\n",
        "\n",
        "* StyleGAN injects styles at different layers of the generator:\n",
        "\n",
        "  * Early layers ‚Üí control **coarse features** (pose, face shape).\n",
        "  * Middle layers ‚Üí control **mid-level features** (facial features, hair).\n",
        "  * Later layers ‚Üí control **fine details** (skin texture, lighting).\n",
        "* This makes the generator **interpretable and controllable**, unlike earlier GANs.\n",
        "### 3. **Noise Injection for Stochastic Variation**\n",
        "\n",
        "* Random noise is added at different layers ‚Üí introduces **small random variations** like freckles, hair strands, skin pores.\n",
        "* Makes each generated image unique and more natural-looking.\n",
        "### 4. **Progressive Growing (StyleGAN1 ‚Üí StyleGAN2)**\n",
        "\n",
        "* Training starts with **low-resolution images**, then progressively adds layers to increase resolution (e.g., 4√ó4 ‚Üí 8√ó8 ‚Üí 1024√ó1024).\n",
        "* Improves stability and quality of high-resolution image generation.\n",
        "### 5. **State-of-the-Art Realism**\n",
        "\n",
        "* StyleGAN and its improvements (StyleGAN2, StyleGAN3) produce some of the most **photorealistic images ever generated by AI**.\n",
        "* Example: Ultra-realistic human faces that don‚Äôt actually exist.\n",
        "## üîπ Why StyleGAN is Special\n",
        "\n",
        "* **Controllable generation** ‚Üí tweak specific attributes.\n",
        "* **Photorealism** ‚Üí unmatched image quality.\n",
        "* **Versatility** ‚Üí used beyond faces (art, anime, cars, landscapes, fashion).\n",
        "* **Foundation for creative AI** ‚Üí used in design, gaming, film, and even scientific applications.\n",
        "‚úÖ **In short:**\n",
        "**StyleGAN is a GAN architecture that revolutionized image generation by introducing style-based control, multi-scale feature manipulation, and progressive training ‚Äî allowing AI to create extremely realistic and customizable images.*\n"
      ],
      "metadata": {
        "id": "ZW0kil7htRo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13 What is the role of noise in a GAN ?\n",
        "Awesome question üëç ‚Äî noise is at the very **heart of how GANs generate diversity**.\n",
        "## üîπ Role of Noise in a GAN\n",
        "\n",
        "In a **GAN**, the generator doesn‚Äôt take real data as input. Instead, it starts from a **random noise vector** $z$, sampled from a probability distribution (commonly Gaussian or uniform).\n",
        "\n",
        "üëâ This noise serves as the **seed** for generating data.\n",
        "## üîπ Why Noise is Important\n",
        "\n",
        "1. **Introduces Randomness ‚Üí Diversity**\n",
        "\n",
        "   * Without noise, the generator would always output the same thing.\n",
        "   * Noise ensures that every input $z$ produces a *different* output (different faces, digits, objects, etc.).\n",
        "\n",
        "2. **Captures the Data Distribution**\n",
        "\n",
        "   * The generator learns a mapping:\n",
        "\n",
        "     $$\n",
        "     z \\sim p_z(z) \\quad \\longrightarrow \\quad G(z) \\sim p_{data}(x)\n",
        "     $$\n",
        "   * This means the random noise is gradually ‚Äúreshaped‚Äù into samples that follow the real data distribution.\n",
        "\n",
        "3. **Encodes Latent Space Representations**\n",
        "\n",
        "   * Each dimension of the noise vector $z$ can correspond to certain properties in the generated output.\n",
        "   * Example: In a face GAN:\n",
        "\n",
        "     * One direction in latent space might control **smile vs. no smile**.\n",
        "     * Another might control **hair color**.\n",
        "\n",
        "4. **Enables Controllable Generation (in advanced GANs like StyleGAN)**\n",
        "\n",
        "   * By manipulating the noise vector in specific ways, we can generate controlled variations (e.g., turning a face gradually older or younger).\n",
        "## üîπ Analogy\n",
        "\n",
        "Think of **noise** as the **DNA blueprint**:\n",
        "\n",
        "* Each random vector $z$ is like a unique genetic code.\n",
        "* The generator is like a ‚Äúsculptor‚Äù that uses this code to create a unique individual (image, sound, etc.).\n",
        "‚úÖ **In short:**\n",
        "The **role of noise in a GAN** is to provide **randomness and diversity**, allowing the generator to produce a wide variety of synthetic data samples that collectively approximate the real data distribution.\n"
      ],
      "metadata": {
        "id": "KeHkmAZNtRla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14 How does the loss function in a WGAN improve training stability ?\n",
        "Excellent üëå ‚Äî this gets to the heart of why **Wasserstein GANs (WGANs)** were created.\n",
        "# üîπ Problem with Traditional GAN Loss\n",
        "\n",
        "In a standard GAN:\n",
        "\n",
        "* The discriminator outputs a probability (real/fake).\n",
        "* Loss is based on **binary cross-entropy**.\n",
        "\n",
        "### Issues:\n",
        "\n",
        "1. **Vanishing gradients**\n",
        "\n",
        "   * If the discriminator gets too good, it classifies fakes with probability \\~0.\n",
        "   * Generator receives almost no gradient ‚Üí training stalls.\n",
        "\n",
        "2. **Unstable training**\n",
        "\n",
        "   * Loss doesn‚Äôt correlate well with image quality.\n",
        "   * Mode collapse (generator produces limited variety).\n",
        "# üîπ WGAN‚Äôs Solution: Wasserstein Loss\n",
        "\n",
        "Instead of cross-entropy, WGAN minimizes the **Wasserstein-1 (Earth Mover‚Äôs) Distance** between the real and generated distributions:\n",
        "\n",
        "$$\n",
        "W(p_{data}, p_g) = \\inf_{\\gamma \\in \\Pi(p_{data}, p_g)} \\mathbb{E}_{(x,y) \\sim \\gamma}[\\|x-y\\|]\n",
        "$$\n",
        "\n",
        "In practice, the **critic** (replacing the discriminator) is trained with:\n",
        "\n",
        "$$\n",
        "L = \\mathbb{E}_{x \\sim p_{data}}[f(x)] - \\mathbb{E}_{z \\sim p_z(z)}[f(G(z))]\n",
        "$$\n",
        "\n",
        "where $f$ is the critic (constrained to be 1-Lipschitz).\n",
        "# üîπ How This Improves Stability\n",
        "\n",
        "### 1. **Better Gradients Everywhere**\n",
        "\n",
        "* Even if the generated samples are far from real data, the Wasserstein distance provides a **smooth, continuous gradient**.\n",
        "* The generator **always has useful feedback** (no vanishing gradients).\n",
        "### 2. **Loss Meaningfully Correlates with Sample Quality**\n",
        "\n",
        "* In standard GANs, discriminator loss doesn‚Äôt tell you if generated samples are improving.\n",
        "* In WGANs, critic loss **decreases as generated samples get closer to the real distribution** ‚Üí stable convergence.\n",
        "### 3. **Reduced Mode Collapse**\n",
        "\n",
        "* Since Wasserstein distance measures the actual distance between distributions, the generator is encouraged to cover **all modes** of the data distribution, not just a few.\n",
        "### 4. **Training More Stable**\n",
        "\n",
        "* The Lipschitz constraint (via weight clipping or gradient penalty in WGAN-GP) ensures the critic behaves smoothly.\n",
        "* This prevents instability that arises from overly sharp decision boundaries in traditional discriminators.\n",
        "# üîπ Intuition (Analogy)\n",
        "\n",
        "* **Traditional GAN:** Discriminator says only ‚Äúyes‚Äù or ‚Äúno.‚Äù If it becomes too confident, it stops teaching the generator.\n",
        "* **WGAN:** Critic gives a **score** (e.g., ‚Äúthis fake looks -2.5 real, that one looks -0.7 real‚Äù).\n",
        "\n",
        "  * The generator gets **graded feedback** instead of pass/fail.\n",
        "  * This continuous signal keeps training moving forward.\n",
        "‚úÖ **In short:**\n",
        "The WGAN loss improves stability by replacing binary classification with a **distance-based objective (Wasserstein distance)**, giving **smooth, meaningful gradients**, reducing **mode collapse**, and ensuring the loss correlates with sample quality.\n"
      ],
      "metadata": {
        "id": "cM8hgvWmtRiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15 Describe the architecture of a typical GANB ?\n",
        "I think you meant **‚ÄúGAN‚Äù** (maybe a small typo at the end with ‚ÄúB‚Äù üôÇ). Let‚Äôs go over the **typical architecture of a GAN**.\n",
        "# üîπ Architecture of a Typical GAN\n",
        "\n",
        "A **Generative Adversarial Network (GAN)** has **two neural networks** that compete against each other:\n",
        "\n",
        "1. **Generator (G)** ‚Üí creates fake data.\n",
        "2. **Discriminator (D)** ‚Üí evaluates whether data is real or fake.\n",
        "## 1. **Generator Architecture**\n",
        "\n",
        "* **Input:** A random noise vector $z$ (sampled from a simple distribution, e.g., Gaussian).\n",
        "* **Layers:**\n",
        "\n",
        "  * Dense (fully connected) ‚Üí reshapes noise into a structured representation.\n",
        "  * Upsampling layers (e.g., transposed convolutions in image GANs).\n",
        "  * Batch Normalization ‚Üí stabilizes training.\n",
        "  * Activation functions: ReLU (hidden layers), Tanh (output).\n",
        "* **Output:** Fake sample (e.g., an image).\n",
        "\n",
        "üëâ **Purpose:** Learn to transform random noise into data that looks like the real dataset.\n",
        "## 2. **Discriminator Architecture**\n",
        "\n",
        "* **Input:** A data sample (either real or fake).\n",
        "* **Layers:**\n",
        "\n",
        "  * Convolutional (for images) or Dense layers (for simple data).\n",
        "  * Dropout (to prevent overfitting).\n",
        "  * Activation: LeakyReLU (common in GANs).\n",
        "* **Output:** A probability $D(x) \\in [0,1]$ (real vs fake).\n",
        "\n",
        "üëâ **Purpose:** Learn to classify inputs as **real (1)** or **fake (0)**.\n",
        "## 3. **Adversarial Setup**\n",
        "\n",
        "* **Generator tries to fool the discriminator** by producing realistic samples.\n",
        "* **Discriminator tries to detect fakes** while correctly identifying real samples.\n",
        "* Both are trained alternately in a **minimax game**:\n",
        "\n",
        "$$\n",
        "\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]\n",
        "$$\n",
        "## üîπ Typical GAN Workflow\n",
        "\n",
        "1. Sample random noise $z$.\n",
        "2. Generator produces fake sample $G(z)$.\n",
        "3. Discriminator sees both real samples and fake samples.\n",
        "4. Discriminator learns to classify correctly.\n",
        "5. Generator learns from discriminator‚Äôs feedback to make better fakes.\n",
        "## üîπ Visual Analogy\n",
        "\n",
        "* **Generator = Counterfeiter** (making fake money).\n",
        "* **Discriminator = Police officer** (detecting fake vs real money).\n",
        "* Over time, both improve, and the generator produces **realistic fakes**.\n",
        "‚úÖ **In short:**\n",
        "A typical GAN has a **generator** (that maps noise to fake data) and a **discriminator** (that classifies real vs fake). They are trained together in a minimax game until the generator produces data indistinguishable from real samples.\n"
      ],
      "metadata": {
        "id": "MlHpjMtYuy8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 16 What challenges do GANs face during training, and how can they be addressed ?\n",
        "Great question üôå ‚Äî GANs are **powerful but notoriously hard to train**. Let‚Äôs break this down.\n",
        "# üîπ Challenges GANs Face During Training\n",
        "\n",
        "### 1. **Mode Collapse**\n",
        "\n",
        "* The generator produces only a few varieties of samples (e.g., always the same digit in MNIST).\n",
        "* It finds a ‚Äúshortcut‚Äù that fools the discriminator but doesn‚Äôt capture the full data distribution.\n",
        "### 2. **Vanishing Gradients**\n",
        "\n",
        "* If the discriminator becomes too strong, it easily rejects fake samples.\n",
        "* The generator then receives **very weak gradients** ‚Üí can‚Äôt improve.\n",
        "### 3. **Training Instability**\n",
        "\n",
        "* GANs are trained in a **minimax game** (two networks competing).\n",
        "* Sometimes one network learns much faster than the other ‚Üí oscillations or divergence.\n",
        "### 4. **Lack of Convergence**\n",
        "\n",
        "* Unlike standard supervised learning, GANs don‚Äôt optimize a clear, single objective.\n",
        "* Training may not converge; instead, generator and discriminator keep chasing each other.\n",
        "### 5. **Evaluation Difficulty**\n",
        "\n",
        "* GANs don‚Äôt have an explicit likelihood function.\n",
        "* Hard to measure ‚Äúhow good‚Äù the generated samples are quantitatively.\n",
        "# üîπ How These Challenges Can Be Addressed\n",
        "\n",
        "### ‚úÖ 1. Tackling Mode Collapse\n",
        "\n",
        "* Use **minibatch discrimination** ‚Üí discriminator looks at groups of samples to detect lack of diversity.\n",
        "* Feature matching ‚Üí generator tries to match statistics of real data, not just fool D.\n",
        "* Use **Wasserstein GAN (WGAN)** ‚Üí encourages learning the full distribution.\n",
        "### ‚úÖ 2. Avoiding Vanishing Gradients\n",
        "\n",
        "* Use **WGAN with gradient penalty (WGAN-GP)** ‚Üí gives smoother gradients.\n",
        "* Replace sigmoid with **linear outputs in critic** (WGAN).\n",
        "* Balance training (don‚Äôt let discriminator get *too* strong).\n",
        "### ‚úÖ 3. Improving Stability\n",
        "\n",
        "* Apply **spectral normalization** ‚Üí stabilizes discriminator.\n",
        "* Use **batch normalization / layer normalization** in generator ‚Üí avoids exploding/vanishing activations.\n",
        "* Careful learning rate tuning (often different for G and D).\n",
        "### ‚úÖ 4. Encouraging Convergence\n",
        "\n",
        "* Two-time-scale update rule (**TTUR**) ‚Üí use different learning rates for G and D.\n",
        "* Use **progressive growing of GANs (PGGANs)** ‚Üí start with low-res images, increase resolution gradually.\n",
        "### ‚úÖ 5. Better Evaluation\n",
        "\n",
        "* Use metrics like:\n",
        "\n",
        "  * **Inception Score (IS)**\n",
        "  * **Fr√©chet Inception Distance (FID)**\n",
        "  * **Precision & Recall for GANs**\n",
        "* Visual inspection still important for many domains.\n",
        "# üîπ Analogy\n",
        "\n",
        "Training a GAN is like a **cat-and-mouse game**:\n",
        "\n",
        "* If the cat (discriminator) is too fast, the mouse (generator) starves.\n",
        "* If the mouse is too sneaky, the cat can‚Äôt learn.\n",
        "* Balance + rules (WGAN, normalization, penalties) keep the game fair.\n",
        "‚úÖ **In short:**\n",
        "GAN training is hard because of **mode collapse, vanishing gradients, instability, and lack of convergence**. These can be addressed with improved loss functions (WGAN, WGAN-GP), normalization techniques, balancing learning rates, and smarter evaluation metrics."
      ],
      "metadata": {
        "id": "g4OfPZyEuypB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 17 How does DCGAN help improve image generation in GANs ?\n",
        "Perfect follow-up üëå ‚Äî DCGAN (**Deep Convolutional GAN**) is one of the biggest breakthroughs in making GANs *actually generate high-quality images*. Let‚Äôs break it down.\n",
        "# üîπ How DCGAN Improves Image Generation\n",
        "## 1. **Uses Convolutional Layers Instead of Fully Connected Layers**\n",
        "* Traditional GANs used dense (fully connected) layers, which don‚Äôt capture spatial structure well.\n",
        "* DCGAN replaces them with **convolutional (Conv) and transposed convolutional (Deconv) layers** ‚Üí much better at learning local patterns in images (edges, textures, shapes).\n",
        "* Result: **Sharper and more realistic images.**\n",
        "## 2. **Upsampling with Transposed Convolutions*\n",
        "* In the generator, instead of ‚Äúreshaping‚Äù noise with dense layers, DCGAN uses **fractionally-strided convolutions (a.k.a. deconvolutions)** to upsample step by step.\n",
        "* This produces **high-resolution details** more naturally.\n",
        "## 3. **Downsampling with Strided Convolutions**\n",
        "* In the discriminator, DCGAN avoids pooling layers (like max-pooling) and instead uses **strided convolutions**.\n",
        "* This lets the network **learn its own downsampling**, improving feature extraction.\n",
        "## 4. **Batch Normalization**\n",
        "* Both generator and discriminator use **batch normalization**, which stabilizes training and prevents gradients from exploding/vanishing.\n",
        "* Leads to **faster and more stable convergence.**\n",
        "## 5. **Better Activation Functions**\n",
        "* **Generator:** ReLU in hidden layers, Tanh at the output (to normalize pixel values between ‚Äì1 and 1).\n",
        "* **Discriminator:** LeakyReLU instead of ReLU ‚Üí avoids ‚Äúdying ReLUs‚Äù and keeps gradients flowing.\n",
        "* This balance improves **image realism**.\n",
        "# üîπ Why DCGAN is Better than a Traditional GAN\n",
        "\n",
        "| Feature          | Traditional GAN          | DCGAN                                    |\n",
        "| ---------------- | ------------------------ | ---------------------------------------- |\n",
        "| Layers           | Fully connected (dense)  | Convolutional & transposed convolutional |\n",
        "| Image quality    | Blurry, unrealistic      | Sharp, structured, realistic             |\n",
        "| Stability        | Prone to collapse        | More stable (with batch norm)            |\n",
        "| Scalability      | Poor for high-res images | Scales to higher resolutions             |\n",
        "| Feature learning | Limited                  | Learns rich image features               |\n",
        "# üîπ Real-World Impact\n",
        "\n",
        "DCGANs showed that GANs could:\n",
        "\n",
        "* Generate **high-quality faces, objects, and scenes**.\n",
        "* Learn useful **unsupervised feature representations** (discriminator‚Äôs Conv layers can be reused for classification tasks).\n",
        "* Serve as a foundation for **advanced GANs** (StyleGAN, BigGAN, CycleGAN, etc.).\n",
        "‚úÖ **In short:**\n",
        "DCGAN improves image generation in GANs by using **deep convolutional architectures, batch normalization, and better activations**, which lead to **sharper, more realistic images and more stable training**.\n"
      ],
      "metadata": {
        "id": "s51uuUZiv_0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 18 What are the key differences between a traditional GAN and a StyleGAN ?\n",
        "Great question üî• ‚Äî StyleGAN is one of the most influential advances in GANs, especially for generating **highly realistic faces and images**. Let‚Äôs compare it directly to a **traditional GAN**.\n",
        "# üîπ Key Differences: Traditional GAN vs StyleGAN\n",
        "\n",
        "| Aspect                     | **Traditional GAN**                                                                  | **StyleGAN**                                                                                                            |\n",
        "| -------------------------- | ------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Input Noise (z)**        | Generator directly takes a random noise vector $z$ and maps it to an image.          | Noise vector $z$ first passes through a **mapping network** ‚Üí produces an intermediate latent vector $w$.               |\n",
        "| **Latent Space**           | Single latent space $z$.                                                             | **Intermediate latent space (W space)** provides more disentanglement (better control over features).                   |\n",
        "| **Generator Architecture** | Straightforward stack of layers (dense ‚Üí convolutional).                             | **Style-based generator**: applies styles (from $w$) at each layer via *Adaptive Instance Normalization (AdaIN)*.       |\n",
        "| **Control over Features**  | Limited ‚Äî hard to control individual attributes (e.g., smile, age, hair).            | Fine-grained control: each layer controls different features (coarse: face shape, mid-level: eyes, fine: skin texture). |\n",
        "| **Noise Injection**        | Only the input noise vector influences randomness.                                   | Extra **per-pixel noise injection** at different layers ‚Üí adds stochastic variation (e.g., freckles, hair strands).     |\n",
        "| **Image Quality**          | Can generate realistic images but often blurry, less consistent at high resolutions. | Produces **photorealistic, high-resolution images** (e.g., FFHQ faces).                                                 |\n",
        "| **Disentanglement**        | Latent space is entangled (changing one aspect affects many others).                 | Better **disentanglement** of factors ‚Üí smoother and more controlled edits.                                             |\n",
        "| **Training Stability**     | Can suffer from mode collapse and instability.                                       | Uses progressive growing + better normalization ‚Üí more stable and scalable training.                                    |\n",
        "# üîπ Intuition\n",
        "\n",
        "* **Traditional GAN**:\n",
        "\n",
        "  * Like asking an artist to paint from random noise.\n",
        "  * Hard to control what comes out.\n",
        "\n",
        "* **StyleGAN**:\n",
        "\n",
        "  * First, you give the artist a *blueprint (mapping network)*.\n",
        "  * Then you add **style controls at each stage** ‚Üí overall structure (face shape), then mid-level (eyes, hair), then fine textures (skin pores, freckles).\n",
        "  * This makes images **much more realistic and controllable**.\n",
        "# üîπ Why StyleGAN is Unique\n",
        "\n",
        "1. **Style-based architecture** (AdaIN) ‚Üí allows intuitive editing.\n",
        "2. **Noise injection** ‚Üí adds fine randomness without affecting global structure.\n",
        "3. **Disentangled latent space** ‚Üí makes feature-level control possible.\n",
        "4. **State-of-the-art realism** ‚Üí some StyleGAN outputs are indistinguishable from real photos.\n",
        "‚úÖ **In short:**\n",
        "While a **traditional GAN** just maps noise directly to images, **StyleGAN adds a style-based generator, disentangled latent space, and noise injection**, enabling **highly realistic, high-res, and controllable image generation**.\n"
      ],
      "metadata": {
        "id": "zUZTaBi4wjJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 19 How does the discriminator decide whether an image is real or fake in a GAN ?\n",
        "Good one üëç ‚Äî the **discriminator** is basically the \"critic\" in a GAN. Its job is to tell whether an image is from the real dataset or generated by the generator. Let‚Äôs break it down step by step:\n",
        "# üîπ How the Discriminator Works in a GAN\n",
        "1. **Input**\n",
        "\n",
        "   * The discriminator takes an image as input.\n",
        "   * This image could be:\n",
        "\n",
        "     * A **real image** from the dataset.\n",
        "     * A **fake image** created by the generator.\n",
        "2. **Feature Extraction**\n",
        "\n",
        "   * The discriminator is usually a **deep neural network** (often convolutional for images).\n",
        "   * It applies **convolutions, pooling, and activations** to learn features:\n",
        "\n",
        "     * Edges, textures, shapes (low-level features).\n",
        "     * Higher-level structures (faces, digits, objects).\n",
        "3. **Decision Making**\n",
        "\n",
        "   * After feature extraction, the discriminator outputs a single number (often through a **sigmoid activation function**).\n",
        "   * This number is interpreted as a probability:\n",
        "\n",
        "     $$\n",
        "     D(x) = P(\\text{image is real})\n",
        "     $$\n",
        "\n",
        "     * Close to **1** ‚Üí the image is real.\n",
        "     * Close to **0** ‚Üí the image is fake.\n",
        "4. **Training**\n",
        "\n",
        "   * **For real images**: The discriminator is trained to output **1**.\n",
        "   * **For fake images**: The discriminator is trained to output **0**.\n",
        "   * Loss function (for vanilla GANs):\n",
        "\n",
        "     $$\n",
        "     L_D = - \\Big[ \\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log (1 - D(G(z)))] \\Big]\n",
        "     $$\n",
        "   * This makes the discriminator better at spotting fakes.\n",
        "# üîπ Analogy\n",
        "\n",
        "Think of the **discriminator as an art expert**:\n",
        "\n",
        "* **Real paintings** = masterpieces.\n",
        "* **Fake paintings** = forgeries.\n",
        "* The expert studies brush strokes, textures, proportions.\n",
        "* Then decides whether it‚Äôs authentic or fake.\n",
        "‚úÖ **In short:**\n",
        "The **discriminator decides whether an image is real or fake by learning features through a neural network and outputting a probability score.** During training, it gets better at detecting fakes, while the generator simultaneously gets better at fooling it.\n"
      ],
      "metadata": {
        "id": "GZxpGCEmw_vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 20 What is the main advantage of using GANs in image generation ?\n",
        "Great question üåü ‚Äî GANs have many strengths, but the **main advantage of using GANs in image generation** is that they can produce **highly realistic, high-quality images** that closely resemble real data ‚Äî even without explicitly modeling the underlying probability distribution.\n",
        "# üîπ Why This Is a Big Advantage\n",
        "\n",
        "1. **Realism**\n",
        "\n",
        "   * GANs learn directly from data and generate samples that are often indistinguishable from real images.\n",
        "   * Unlike older generative models, GANs don‚Äôt just blur or average features ‚Äî they create **sharp, detailed, and natural-looking images**.\n",
        "\n",
        "2. **Unsupervised Learning**\n",
        "\n",
        "   * GANs don‚Äôt require labeled data.\n",
        "   * They learn from raw, unlabeled datasets (e.g., just a folder of images), which makes them very practical.\n",
        "\n",
        "3. **High Diversity**\n",
        "\n",
        "   * By sampling different random noise vectors, GANs can produce a wide variety of images, not just duplicates of training data.\n",
        "\n",
        "4. **Latent Space Control**\n",
        "\n",
        "   * GANs (especially advanced ones like StyleGAN) allow manipulation of image attributes (e.g., age, hair color, facial expression).\n",
        "   * This enables **controllable and creative image generation.*\n",
        "# üîπ Example Applications\n",
        "\n",
        "* **Art & design**: generating paintings, textures, or clothing designs.\n",
        "* **Face synthesis**: creating realistic human faces that don‚Äôt exist.\n",
        "* **Data augmentation**: generating synthetic medical images or rare cases to improve ML models.\n",
        "* **Super-resolution & inpainting**: filling missing parts of images or enhancing details.\n",
        "‚úÖ **In short:**\n",
        "The main advantage of GANs in image generation is their ability to produce **realistic, diverse, and high-quality images** that are often indistinguishable from real ones ‚Äî all without needing labeled data.\n"
      ],
      "metadata": {
        "id": "Z10tfXYPxXDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 21 How can GANs be used in real-world applications ?\n",
        "Excellent question üôå ‚Äî GANs aren‚Äôt just theoretical; they‚Äôve become **hugely impactful in real-world applications** across many industries.\n",
        "# üîπ Real-World Applications of GANs\n",
        "\n",
        "### 1. **Image & Media Generation**\n",
        "\n",
        "* **Face synthesis** ‚Üí e.g., generating realistic faces that don‚Äôt exist (ThisPersonDoesNotExist.com).\n",
        "* **Art & design** ‚Üí GANs assist artists in creating paintings, music album covers, or digital effects.\n",
        "* **Video game assets** ‚Üí auto-generating textures, characters, or environments.\n",
        "### 2. **Image Enhancement**\n",
        "\n",
        "* **Super-resolution** ‚Üí increasing image resolution (e.g., turning a blurry photo into a sharp one).\n",
        "* **Image inpainting** ‚Üí filling missing or corrupted regions in images (like restoring old photos).\n",
        "* **Style transfer** ‚Üí converting images into different artistic styles (e.g., Van Gogh filter).\n",
        "### 3. **Healthcare & Medicine**\n",
        "\n",
        "* **Medical image synthesis** ‚Üí generating synthetic MRI/CT scans for rare diseases to help train diagnostic models.\n",
        "* **Data augmentation** ‚Üí balancing medical datasets where real data is scarce.\n",
        "* **Drug discovery** ‚Üí generating molecular structures for potential new drugs.\n",
        "### 4. **Security & Privacy**\n",
        "\n",
        "* **Deepfake detection & creation** ‚Üí GANs generate realistic fake videos, but they are also used to train detectors against such fakes.\n",
        "* **Anonymization** ‚Üí replacing real faces with synthetic ones to protect privacy in datasets.\n",
        "### 5. **Business & Industry**\n",
        "\n",
        "* **Fashion design** ‚Üí generating new clothing designs or outfits for e-commerce.\n",
        "* **Interior & architecture** ‚Üí visualizing furniture or floor plans in different styles.\n",
        "* **Marketing** ‚Üí creating product mockups, ads, or synthetic influencers.\n",
        "### 6. **Science & Research**\n",
        "\n",
        "* **Astronomy** ‚Üí reconstructing high-quality telescope images.\n",
        "* **Climate science** ‚Üí filling gaps in satellite data.\n",
        "* **Robotics** ‚Üí generating synthetic training environments.\n",
        "# üîπ Why GANs Are Useful in These Areas\n",
        "\n",
        "* They **reduce cost and time** (e.g., synthetic data instead of collecting real data).\n",
        "* They **enable creativity** (new designs, art, content).\n",
        "* They **help where data is scarce** (rare medical conditions, expensive experiments).\n",
        "‚úÖ **In short:**\n",
        "GANs are used in the real world for **image synthesis, data augmentation, medical imaging, super-resolution, design, and even deepfake creation/detection**. Their ability to generate **realistic synthetic data** makes them powerful across many fields.\n"
      ],
      "metadata": {
        "id": "EtM7TwYFxW_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 22 What is Mode Collapse in GANs, and how can it be prevented?\n",
        "Great question üöÄ ‚Äî **mode collapse** is one of the most common and frustrating issues in training GANs. Let‚Äôs unpack it step by step:\n",
        "# üîπ What is Mode Collapse?\n",
        "\n",
        "* **Mode collapse** happens when the **generator produces limited types of outputs**, even though the real data distribution has many variations.\n",
        "* Instead of covering all the \"modes\" (variety) of the real data, the generator gets stuck producing a few patterns that successfully fool the discriminator.\n",
        "\n",
        "üëâ Example:\n",
        "\n",
        "* Suppose we train a GAN on handwritten digits (0‚Äì9).\n",
        "* Instead of generating all digits, the generator may only produce \"3\" and \"7\", because it found those are enough to trick the discriminator.\n",
        "* The result: **low diversity** in generated samples.\n",
        "# üîπ Why Does Mode Collapse Happen?\n",
        "\n",
        "1. **Generator optimization imbalance** ‚Äì the generator finds a shortcut that fools the discriminator but doesn‚Äôt represent the full data distribution.\n",
        "2. **Discriminator too strong** ‚Äì the generator learns to \"cheat\" with limited outputs.\n",
        "3. **Unstable adversarial training** ‚Äì since GAN training is a minimax game, oscillations or poor convergence can cause collapse.\n",
        "# üîπ How to Prevent Mode Collapse\n",
        "\n",
        "### ‚úÖ 1. Architectural Improvements\n",
        "\n",
        "* **Mini-batch discrimination** ‚Üí discriminator compares samples in a batch instead of individually, forcing generator to produce diverse outputs.\n",
        "* **Unrolled GANs** ‚Üí let the generator see several steps ahead in discriminator updates, reducing collapse.\n",
        "* **WGAN / WGAN-GP** ‚Üí using the Wasserstein loss improves stability and reduces collapse.\n",
        "\n",
        "### ‚úÖ 2. Training Techniques\n",
        "\n",
        "* **Feature matching** ‚Üí train generator to match internal features of real data, not just fool discriminator.\n",
        "* **Label smoothing** ‚Üí prevent discriminator from being overly confident by softening labels (e.g., real = 0.9 instead of 1).\n",
        "* **Noise injection** ‚Üí adding noise to inputs or labels helps maintain variability.\n",
        "\n",
        "### ‚úÖ 3. Balancing G and D\n",
        "\n",
        "* Ensure neither generator nor discriminator dominates.\n",
        "* Use careful **learning rate tuning** and **update frequency balance**.\n",
        "# üîπ Analogy\n",
        "\n",
        "Think of a **student (generator)** writing essays for a **teacher (discriminator)**:\n",
        "\n",
        "* If the teacher is easy to fool, the student might keep copying one essay (\"mode collapse\").\n",
        "* To prevent this, the teacher should demand variety and consistency across topics (mini-batch discrimination, feature matching).\n",
        "‚úÖ **In short:**\n",
        "**Mode collapse** = generator produces only a few types of outputs, reducing diversity.\n",
        "**Prevention** = use improved loss functions (WGAN), architectural tricks (mini-batch discrimination), and careful training strategies (feature matching, balancing learning rates).\n"
      ],
      "metadata": {
        "id": "oYv81ebyxW8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practical"
      ],
      "metadata": {
        "id": "H_IYdiPPyaV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Implement a simple GAN architecture to generate random images (like noise or basic shapes) using\n",
        "# TensorFlow/Keras\n",
        "Perfect üëç Let‚Äôs build a **very simple GAN** in TensorFlow/Keras that learns to generate random images.\n",
        "To keep things light, we‚Äôll use **MNIST digits (28√ó28 grayscale images)** as our training dataset, but the same GAN can be extended to generate other shapes or even just structured noise.\n",
        "Here‚Äôs the full implementation:\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Load dataset (MNIST as example)\n",
        "# -----------------------------\n",
        "(X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.astype(\"float32\") / 255.0\n",
        "X_train = np.expand_dims(X_train, axis=-1)  # shape: (60000, 28, 28, 1)\n",
        "\n",
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 128\n",
        "LATENT_DIM = 100  # size of random noise vector\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Build Generator\n",
        "# -----------------------------\n",
        "def build_generator():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Dense(7*7*256, use_bias=False, input_shape=(LATENT_DIM,)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        layers.Reshape((7, 7, 256)),  # -> (7,7,256)\n",
        "\n",
        "        layers.Conv2DTranspose(128, (5,5), strides=(1,1), padding='same', use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        layers.Conv2DTranspose(64, (5,5), strides=(2,2), padding='same', use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        layers.Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', use_bias=False, activation='tanh')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Build Discriminator\n",
        "# -----------------------------\n",
        "def build_discriminator():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=[28,28,1]),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Conv2D(128, (5,5), strides=(2,2), padding='same'),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(1)  # no activation, use logits\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Losses & Optimizers\n",
        "# -----------------------------\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)   # real -> 1\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output) # fake -> 0\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)  # want fake -> 1\n",
        "\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Training Loop\n",
        "# -----------------------------\n",
        "EPOCHS = 30\n",
        "noise_dim = LATENT_DIM\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
        "\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "    predictions = model(test_input, training=False)\n",
        "    fig = plt.figure(figsize=(4,4))\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(4, 4, i+1)\n",
        "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.suptitle(f'Epoch {epoch}')\n",
        "    plt.show()\n",
        "\n",
        "def train(dataset, epochs):\n",
        "    for epoch in range(1, epochs+1):\n",
        "        for image_batch in dataset:\n",
        "            gen_loss, disc_loss = train_step(image_batch)\n",
        "\n",
        "        print(f'Epoch {epoch}, Gen Loss: {gen_loss.numpy():.4f}, Disc Loss: {disc_loss.numpy():.4f}')\n",
        "        generate_and_save_images(generator, epoch, seed)\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Train the GAN\n",
        "# -----------------------------\n",
        "train(train_dataset, EPOCHS)\n",
        "```\n",
        "# üîπ What this does:\n",
        "\n",
        "1. **Generator** takes random noise ‚Üí outputs a fake 28√ó28 image.\n",
        "2. **Discriminator** tries to classify real vs fake.\n",
        "3. They train together in an adversarial loop.\n",
        "4. After a few epochs, the generator starts producing **digit-like images** (instead of random noise)."
      ],
      "metadata": {
        "id": "tCVyTQa2yZ9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Implement the discriminator for a GAN with an image input of shape (28, 28) ?\n",
        "Got it üëç You want a **discriminator** for a GAN where the input is a **28√ó28 image** (grayscale).\n",
        "That means the input shape will be `(28, 28, 1)` if we explicitly keep the channel dimension.\n",
        "Here‚Äôs a simple **discriminator implementation in Keras**:\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_discriminator():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Input(shape=(28, 28, 1)),   # input is 28x28 grayscale image\n",
        "\n",
        "        # First Conv block\n",
        "        layers.Conv2D(64, kernel_size=5, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        # Second Conv block\n",
        "        layers.Conv2D(128, kernel_size=5, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        # Flatten + output\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(1)   # No activation (use logits with BinaryCrossEntropy)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create the discriminator\n",
        "discriminator = build_discriminator()\n",
        "discriminator.summary()\n",
        "```\n",
        "### üîπ Explanation:\n",
        "\n",
        "* **Input shape:** `(28, 28, 1)` (grayscale image).\n",
        "* **Conv2D layers:** extract spatial features and reduce image size (downsampling).\n",
        "* **LeakyReLU:** avoids dead neurons compared to normal ReLU.\n",
        "* **Dropout:** helps prevent overfitting.\n",
        "* **Dense(1):** outputs a single scalar (logit), where positive ‚Üí \"real\" and negative ‚Üí \"fake\".\n",
        "\n",
        "---\n",
        "\n",
        "üëâ If your dataset is **just `(28, 28)` without a channel dimension**, you can fix it like this:\n",
        "\n",
        "```python\n",
        "layers.Input(shape=(28, 28, 1))\n",
        "```\n",
        "\n",
        "or simply expand dimensions when loading data:\n",
        "\n",
        "```python\n",
        "X = np.expand_dims(X, axis=-1)\n",
        "```"
      ],
      "metadata": {
        "id": "oXSuILleyZ6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 Train the generator to produce simple digits (using noise as input) and plot the generated images ?\n",
        "Perfect üëå Let‚Äôs put everything together:\n",
        "We‚Äôll build a **generator**, train it against a discriminator on **MNIST digits**, and periodically **plot the generated digits** from noise.\n",
        "Here‚Äôs a **self-contained example** in TensorFlow/Keras:\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Load MNIST dataset\n",
        "# -----------------------------\n",
        "(X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.astype(\"float32\") / 127.5 - 1.0   # scale to [-1, 1]\n",
        "X_train = np.expand_dims(X_train, axis=-1)          # (60000, 28, 28, 1)\n",
        "\n",
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 128\n",
        "LATENT_DIM = 100  # noise vector length\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Generator\n",
        "# -----------------------------\n",
        "def build_generator():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Input(shape=(LATENT_DIM,)),\n",
        "        layers.Dense(7*7*256, use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        layers.Reshape((7, 7, 256)),\n",
        "\n",
        "        layers.Conv2DTranspose(128, (5,5), strides=(1,1), padding=\"same\", use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        layers.Conv2DTranspose(64, (5,5), strides=(2,2), padding=\"same\", use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        layers.Conv2DTranspose(1, (5,5), strides=(2,2), padding=\"same\", use_bias=False, activation=\"tanh\")\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Discriminator\n",
        "# -----------------------------\n",
        "def build_discriminator():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(64, (5,5), strides=(2,2), padding=\"same\"),\n",
        "        layers.LeakyReLU(0.2),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Conv2D(128, (5,5), strides=(2,2), padding=\"same\"),\n",
        "        layers.LeakyReLU(0.2),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(1)  # logit\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Loss + Optimizers\n",
        "# -----------------------------\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)  # want fakes classified as real\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "gen_opt = tf.keras.optimizers.Adam(1e-4)\n",
        "disc_opt = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Training Loop\n",
        "# -----------------------------\n",
        "EPOCHS = 20\n",
        "noise_dim = LATENT_DIM\n",
        "num_examples_to_generate = 16\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
        "\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    gen_opt.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    disc_opt.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n",
        "\n",
        "def generate_and_plot(epoch, test_input):\n",
        "    predictions = generator(test_input, training=False)\n",
        "    predictions = (predictions + 1) / 2.0  # scale back to [0,1]\n",
        "\n",
        "    fig, axs = plt.subplots(4, 4, figsize=(4,4))\n",
        "    for i, ax in enumerate(axs.flat):\n",
        "        ax.imshow(predictions[i, :, :, 0], cmap=\"gray\")\n",
        "        ax.axis(\"off\")\n",
        "    plt.suptitle(f\"Epoch {epoch}\")\n",
        "    plt.show()\n",
        "\n",
        "def train(dataset, epochs):\n",
        "    for epoch in range(1, epochs+1):\n",
        "        for image_batch in dataset:\n",
        "            g_loss, d_loss = train_step(image_batch)\n",
        "\n",
        "        print(f\"Epoch {epoch}, Gen Loss: {g_loss.numpy():.4f}, Disc Loss: {d_loss.numpy():.4f}\")\n",
        "        generate_and_plot(epoch, seed)\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Run training\n",
        "# -----------------------------\n",
        "train(train_dataset, EPOCHS)\n",
        "```\n",
        "### üîπ What happens here\n",
        "\n",
        "* The **generator** takes random noise (`z ‚àº N(0,1)`) and outputs a fake MNIST-style digit.\n",
        "* The **discriminator** learns to classify real vs fake digits.\n",
        "* During training, both improve together.\n",
        "* After each epoch, we **plot 16 generated digits**.\n",
        "\n"
      ],
      "metadata": {
        "id": "YheEBa0iyZ3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 Implement WGAN by modifying the loss function in the GAN ?\n",
        "Great question üöÄ\n",
        "To implement a **Wasserstein GAN (WGAN)** we mainly need to **modify the loss function** and the way the discriminator (called the **critic**) is trained.\n",
        "## üîë Key differences from a vanilla GAN\n",
        "\n",
        "1. **Loss function**:\n",
        "\n",
        "   * GAN uses Binary Cross-Entropy (BCE).\n",
        "   * WGAN uses **Wasserstein loss** (Earth Mover‚Äôs distance).\n",
        "\n",
        "     * Critic output is a **real number** (no sigmoid).\n",
        "     * Loss is the **difference of means** between real and fake scores.\n",
        "\n",
        "   $$\n",
        "   L_{critic} = \\mathbb{E}[D(fake)] - \\mathbb{E}[D(real)]\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   L_{generator} = - \\mathbb{E}[D(fake)]\n",
        "   $$\n",
        "\n",
        "2. **Discriminator ‚Üí Critic**\n",
        "\n",
        "   * Outputs a **score**, not a probability.\n",
        "   * No `sigmoid`.\n",
        "\n",
        "3. **Weight clipping** (basic WGAN):\n",
        "\n",
        "   * Critic weights are clamped to a range (e.g. $[-0.01, 0.01]$).\n",
        "   * (WGAN-GP replaces this with a gradient penalty for stability, but here we‚Äôll keep it simple).\n",
        "\n",
        "4. **Training**:\n",
        "\n",
        "   * Critic is trained **more times** per generator update (e.g., 5 critic steps per generator step).\n",
        "## üìù Implementation (TensorFlow/Keras)\n",
        "\n",
        "Here‚Äôs how you can **modify the GAN code to a WGAN**:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# Generator\n",
        "# -----------------------------\n",
        "def build_generator(latent_dim=100):\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(7*7*128, use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Reshape((7, 7, 128)),\n",
        "\n",
        "        layers.Conv2DTranspose(64, (5,5), strides=(2,2), padding=\"same\", use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "\n",
        "        layers.Conv2DTranspose(1, (5,5), strides=(2,2), padding=\"same\", use_bias=False, activation=\"tanh\")\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# -----------------------------\n",
        "# Critic (Discriminator without sigmoid)\n",
        "# -----------------------------\n",
        "def build_critic():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(64, (5,5), strides=(2,2), padding=\"same\"),\n",
        "        layers.LeakyReLU(0.2),\n",
        "\n",
        "        layers.Conv2D(128, (5,5), strides=(2,2), padding=\"same\"),\n",
        "        layers.LeakyReLU(0.2),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(1)  # no sigmoid\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# -----------------------------\n",
        "# WGAN Loss functions\n",
        "# -----------------------------\n",
        "def generator_loss(fake_output):\n",
        "    return -tf.reduce_mean(fake_output)\n",
        "\n",
        "def critic_loss(real_output, fake_output):\n",
        "    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
        "\n",
        "# -----------------------------\n",
        "# Training Setup\n",
        "# -----------------------------\n",
        "LATENT_DIM = 100\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "N_CRITIC = 5        # train critic multiple times\n",
        "CLIP_VALUE = 0.01   # weight clipping\n",
        "\n",
        "# Data\n",
        "(X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.astype(\"float32\") / 127.5 - 1\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(60000).batch(BATCH_SIZE)\n",
        "\n",
        "generator = build_generator(LATENT_DIM)\n",
        "critic = build_critic()\n",
        "\n",
        "gen_opt = tf.keras.optimizers.RMSprop(5e-5)\n",
        "critic_opt = tf.keras.optimizers.RMSprop(5e-5)\n",
        "\n",
        "# -----------------------------\n",
        "# Training Step\n",
        "# -----------------------------\n",
        "@tf.function\n",
        "def train_critic(real_images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, LATENT_DIM])\n",
        "    with tf.GradientTape() as tape:\n",
        "        fake_images = generator(noise, training=True)\n",
        "        real_output = critic(real_images, training=True)\n",
        "        fake_output = critic(fake_images, training=True)\n",
        "        loss = critic_loss(real_output, fake_output)\n",
        "    grads = tape.gradient(loss, critic.trainable_variables)\n",
        "    critic_opt.apply_gradients(zip(grads, critic.trainable_variables))\n",
        "\n",
        "    # Weight clipping\n",
        "    for var in critic.trainable_variables:\n",
        "        var.assign(tf.clip_by_value(var, -CLIP_VALUE, CLIP_VALUE))\n",
        "\n",
        "    return loss\n",
        "\n",
        "@tf.function\n",
        "def train_generator():\n",
        "    noise = tf.random.normal([BATCH_SIZE, LATENT_DIM])\n",
        "    with tf.GradientTape() as tape:\n",
        "        fake_images = generator(noise, training=True)\n",
        "        fake_output = critic(fake_images, training=True)\n",
        "        loss = generator_loss(fake_output)\n",
        "    grads = tape.gradient(loss, generator.trainable_variables)\n",
        "    gen_opt.apply_gradients(zip(grads, generator.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "# -----------------------------\n",
        "# Training Loop\n",
        "# -----------------------------\n",
        "def train(dataset, epochs):\n",
        "    for epoch in range(1, epochs+1):\n",
        "        for i, real_images in enumerate(dataset):\n",
        "            # Train critic N_CRITIC times\n",
        "            for _ in range(N_CRITIC):\n",
        "                d_loss = train_critic(real_images)\n",
        "\n",
        "            # Train generator\n",
        "            g_loss = train_generator()\n",
        "\n",
        "        print(f\"Epoch {epoch}, Critic Loss: {d_loss.numpy():.4f}, Generator Loss: {g_loss.numpy():.4f}\")\n",
        "        plot_generated_images(epoch)\n",
        "\n",
        "def plot_generated_images(epoch, n=16):\n",
        "    noise = tf.random.normal([n, LATENT_DIM])\n",
        "    gen_images = generator(noise, training=False)\n",
        "    gen_images = (gen_images + 1) / 2.0\n",
        "\n",
        "    plt.figure(figsize=(4,4))\n",
        "    for i in range(n):\n",
        "        plt.subplot(4,4,i+1)\n",
        "        plt.imshow(gen_images[i,:,:,0], cmap=\"gray\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(f\"Epoch {epoch}\")\n",
        "    plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Run training\n",
        "# -----------------------------\n",
        "train(dataset, EPOCHS)\n",
        "```\n",
        "‚úÖ This is a **WGAN** (with weight clipping).\n",
        "\n",
        "* The **critic** outputs scores instead of probabilities.\n",
        "* The **loss function** uses Wasserstein distance.\n",
        "* The **weights are clipped** after each critic update.\n",
        "* The **critic is updated more times per generator step**.\n"
      ],
      "metadata": {
        "id": "s3RjMD1Hz1wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 Use a trained generator to generate a batch of fake images and display them ?\n",
        "Perfect üëç Since we already have a trained **generator** in the WGAN (or GAN) setup, we can easily sample random noise and use it to generate a batch of fake images.\n",
        "Here‚Äôs a simple snippet to **generate and display fake images** from the trained generator:\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Parameters\n",
        "LATENT_DIM = 100\n",
        "NUM_IMAGES = 16  # how many fake images to generate\n",
        "\n",
        "# Generate random noise\n",
        "noise = tf.random.normal([NUM_IMAGES, LATENT_DIM])\n",
        "\n",
        "# Use trained generator\n",
        "fake_images = generator(noise, training=False)\n",
        "\n",
        "# Rescale from [-1,1] to [0,1] for visualization\n",
        "fake_images = (fake_images + 1) / 2.0\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i in range(NUM_IMAGES):\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(fake_images[i, :, :, 0], cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"Generated Fake Images\")\n",
        "plt.show()\n",
        "```\n",
        "‚úÖ This will display a **grid of generated images** (e.g., fake MNIST-like digits if trained on MNIST, or random patterns if only trained briefly).\n"
      ],
      "metadata": {
        "id": "IrG0X3nbz1r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 Create a StyleGAN-inspired architecture that outputs high-resolution images ?\n",
        "Got it ‚úÖ\n",
        "Let‚Äôs design a **StyleGAN-inspired architecture** in **TensorFlow/Keras**.\n",
        "We won‚Äôt implement the *full StyleGAN2* pipeline (that‚Äôs huge and complex), but I‚Äôll give you a **simplified but modular version** that captures its **core ideas**:\n",
        "\n",
        "* **Mapping network**: Convert latent vector $z$ ‚Üí intermediate latent vector $w$.\n",
        "* **Style modulation**: Inject $w$ into each convolutional block (controls style).\n",
        "* **Progressive upsampling**: Start from a small $4√ó4$ image and grow to higher resolution (e.g., $64√ó64$ or $128√ó128$).\n",
        "* **Noise injection**: Add per-pixel noise at each layer for stochastic variation.\n",
        "## üß© Simplified StyleGAN Generator (Keras)\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# -----------------------------\n",
        "# Mapping Network (z -> w)\n",
        "# -----------------------------\n",
        "def mapping_network(latent_dim=512, dlatent_dim=512, num_layers=8):\n",
        "    model = tf.keras.Sequential()\n",
        "    for _ in range(num_layers):\n",
        "        model.add(layers.Dense(dlatent_dim, activation=\"relu\"))\n",
        "    return model\n",
        "\n",
        "# -----------------------------\n",
        "# Noise injection + style modulation\n",
        "# -----------------------------\n",
        "class StyleBlock(layers.Layer):\n",
        "    def __init__(self, filters, kernel_size=3):\n",
        "        super(StyleBlock, self).__init__()\n",
        "        self.conv = layers.Conv2D(filters, kernel_size, padding=\"same\")\n",
        "        self.noise_weight = self.add_weight(shape=(1,), initializer=\"zeros\", trainable=True)\n",
        "        self.act = layers.LeakyReLU(0.2)\n",
        "        self.norm = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, x, w, noise):\n",
        "        # Convolution\n",
        "        x = self.conv(x)\n",
        "\n",
        "        # Add per-pixel noise\n",
        "        x = x + self.noise_weight * noise\n",
        "\n",
        "        # Modulate with style vector (scale & shift)\n",
        "        style_scale = tf.expand_dims(tf.expand_dims(w, 1), 1)\n",
        "        x = x * style_scale\n",
        "\n",
        "        return self.act(self.norm(x))\n",
        "\n",
        "# -----------------------------\n",
        "# Generator inspired by StyleGAN\n",
        "# -----------------------------\n",
        "def build_stylegan_generator(resolution=64, latent_dim=512):\n",
        "    # Mapping network\n",
        "    mapping = mapping_network(latent_dim)\n",
        "\n",
        "    # Input constant (learnable 4x4x512 tensor)\n",
        "    const_input = tf.Variable(tf.random.normal([1, 4, 4, 512]), trainable=True, name=\"const_input\")\n",
        "\n",
        "    # StyleGAN generator\n",
        "    z = layers.Input(shape=(latent_dim,))\n",
        "    w = mapping(z)  # z ‚Üí w\n",
        "\n",
        "    # Broadcast w (for multiple layers)\n",
        "    w_broadcast = layers.Dense(512)(w)\n",
        "\n",
        "    # Start with constant input\n",
        "    x = tf.tile(const_input, [tf.shape(z)[0], 1, 1, 1])\n",
        "\n",
        "    # Resolution blocks (progressively upsample)\n",
        "    for filters in [512, 256, 128, 64, 32]:\n",
        "        noise = tf.random.normal(tf.shape(x))\n",
        "        x = StyleBlock(filters)(x, w_broadcast, noise)\n",
        "        x = layers.UpSampling2D()(x)\n",
        "\n",
        "        if x.shape[1] >= resolution:  # stop when target resolution reached\n",
        "            break\n",
        "\n",
        "    # Final RGB image\n",
        "    img = layers.Conv2D(3, (1, 1), padding=\"same\", activation=\"tanh\")(x)\n",
        "\n",
        "    return tf.keras.Model(z, img, name=\"StyleGAN_Generator\")\n",
        "```\n",
        "## ‚úÖ Usage Example\n",
        "\n",
        "```python\n",
        "# Build generator\n",
        "gen = build_stylegan_generator(resolution=64, latent_dim=512)\n",
        "\n",
        "# Generate images\n",
        "latent_vectors = tf.random.normal([4, 512])\n",
        "fake_images = gen(latent_vectors, training=False)\n",
        "\n",
        "print(fake_images.shape)  # (4, 64, 64, 3)\n",
        "```\n",
        "## üîπ Notes:\n",
        "\n",
        "* This is **StyleGAN-inspired**, not the full official version.\n",
        "* Key StyleGAN features included:\n",
        "  ‚úî Mapping network (z‚Üíw)\n",
        "  ‚úî Constant learned input\n",
        "  ‚úî Style modulation\n",
        "  ‚úî Noise injection\n",
        "  ‚úî Progressive upsampling\n"
      ],
      "metadata": {
        "id": "JB8Ivle1z1oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 Implement the Wasserstein loss function for GAN training ?\n",
        "Great question\n",
        "In a **Wasserstein GAN (WGAN)**, we don‚Äôt use the usual binary cross-entropy loss.\n",
        "Instead, the **Wasserstein loss** (Earth Mover‚Äôs Distance) is used, which provides smoother gradients and improves training stability.\n",
        "## üîπ WGAN Loss Functions\n",
        "\n",
        "* **Discriminator (Critic) loss**:\n",
        "\n",
        "$$\n",
        "L_D = \\mathbb{E}[D(x_{real})] - \\mathbb{E}[D(x_{fake})]\n",
        "$$\n",
        "\n",
        "The critic tries to **maximize real scores** and **minimize fake scores**.\n",
        "\n",
        "* **Generator loss**:\n",
        "\n",
        "$$\n",
        "L_G = -\\mathbb{E}[D(x_{fake})]\n",
        "$$\n",
        "\n",
        "The generator tries to **fool the critic** by maximizing fake scores.\n",
        "## üß© TensorFlow/Keras Implementation\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Critic (Discriminator) loss\n",
        "def wasserstein_discriminator_loss(real_output, fake_output):\n",
        "    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
        "\n",
        "# Generator loss\n",
        "def wasserstein_generator_loss(fake_output):\n",
        "    return -tf.reduce_mean(fake_output)\n",
        "```\n",
        "## üîπ Usage in Training Step\n",
        "\n",
        "```python\n",
        "@tf.function\n",
        "def train_step(real_images):\n",
        "    batch_size = tf.shape(real_images)[0]\n",
        "    noise = tf.random.normal([batch_size, LATENT_DIM])\n",
        "\n",
        "    # Train Critic\n",
        "    with tf.GradientTape() as tape_d:\n",
        "        fake_images = generator(noise, training=True)\n",
        "        real_output = discriminator(real_images, training=True)\n",
        "        fake_output = discriminator(fake_images, training=True)\n",
        "        d_loss = wasserstein_discriminator_loss(real_output, fake_output)\n",
        "    grads_d = tape_d.gradient(d_loss, discriminator.trainable_variables)\n",
        "    optimizer_d.apply_gradients(zip(grads_d, discriminator.trainable_variables))\n",
        "\n",
        "    # Train Generator\n",
        "    noise = tf.random.normal([batch_size, LATENT_DIM])\n",
        "    with tf.GradientTape() as tape_g:\n",
        "        fake_images = generator(noise, training=True)\n",
        "        fake_output = discriminator(fake_images, training=True)\n",
        "        g_loss = wasserstein_generator_loss(fake_output)\n",
        "    grads_g = tape_g.gradient(g_loss, generator.trainable_variables)\n",
        "    optimizer_g.apply_gradients(zip(grads_g, generator.trainable_variables))\n",
        "\n",
        "    return d_loss, g_loss\n",
        "``\n",
        "## üîπ Important WGAN Notes:\n",
        "\n",
        "1. **No sigmoid activation** in discriminator output (keep it linear).\n",
        "2. **Weight clipping** (original WGAN) or **gradient penalty** (WGAN-GP, more stable).\n",
        "3. Optimizers usually **RMSProp** or **Adam with low Œ≤1 (0.5, 0.9)**."
      ],
      "metadata": {
        "id": "kce44CEkz1lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.Write a function to modify the discriminator to include a dropout layer with a rate of 0.4 and print the\n",
        "# configurations ?\n",
        "Perfect\n",
        "We‚Äôll create a function that **modifies the discriminator architecture** by adding a **dropout layer (rate=0.4)**.\n",
        "This helps reduce overfitting and improves stability in GAN training.\n",
        "## üß© Implementation in TensorFlow/Keras\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_discriminator_with_dropout(input_shape=(28, 28, 1), dropout_rate=0.4):\n",
        "    model = models.Sequential(name=\"Discriminator\")\n",
        "\n",
        "    model.add(layers.Input(shape=input_shape))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(512, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(dropout_rate))   # Dropout layer added\n",
        "    model.add(layers.Dense(256, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(dropout_rate))   # Dropout again\n",
        "    model.add(layers.Dense(1))  # Linear output for WGAN (no sigmoid)\n",
        "\n",
        "    # Print model summary\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "discriminator = build_discriminator_with_dropout()\n",
        "```\n",
        "### ‚úÖ Output (Summary Excerpt)\n",
        "\n",
        "When you run `model.summary()`, you‚Äôll see something like:\n",
        "\n",
        "```\n",
        "Model: \"Discriminator\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #\n",
        "=================================================================\n",
        " flatten (Flatten)           (None, 784)               0\n",
        " dense (Dense)               (None, 512)               401920\n",
        " dropout (Dropout)           (None, 512)               0\n",
        " dense_1 (Dense)             (None, 256)               131328\n",
        " dropout_1 (Dropout)         (None, 256)               0\n",
        " dense_2 (Dense)             (None, 1)                 257\n",
        "=================================================================\n",
        "Total params: 533,505\n",
        "Trainable params: 533,505\n",
        "Non-trainable params: 0\n",
        "```"
      ],
      "metadata": {
        "id": "qb3GGZx-1-1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  9.Write a function to modify the discriminator to include a dropout layer with a rate of 0.4 and print the configuration ?\n",
        "Got it üëç\n",
        "I‚Äôll give you a **function** that modifies/creates the discriminator with a **Dropout layer (rate=0.4)** and prints its full configuration (summary + layer details).\n",
        "## üß© Code: Discriminator with Dropout (0.4)\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "def build_discriminator_with_dropout(input_shape=(28, 28, 1), dropout_rate=0.4):\n",
        "    \"\"\"\n",
        "    Builds a discriminator model with Dropout layers and prints its configuration.\n",
        "\n",
        "    Args:\n",
        "        input_shape (tuple): Shape of input images.\n",
        "        dropout_rate (float): Dropout rate to apply.\n",
        "\n",
        "    Returns:\n",
        "        model (tf.keras.Model): Discriminator model with dropout.\n",
        "    \"\"\"\n",
        "    model = models.Sequential(name=\"Discriminator\")\n",
        "\n",
        "    # Input\n",
        "    model.add(layers.Input(shape=input_shape))\n",
        "\n",
        "    # Layers\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(512, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.Dense(256, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.Dense(1))  # No sigmoid (for WGAN)\n",
        "\n",
        "    # Print configuration\n",
        "    model.summary()  # prints table of layers, shapes, params\n",
        "    print(\"\\nModel Configurations:\")\n",
        "    print(model.get_config())  # prints dictionary of layer settings\n",
        "\n",
        "    return model\n",
        "# Example usage\n",
        "discriminator = build_discriminator_with_dropout()\n",
        "```\n",
        "### ‚úÖ This will output:\n",
        "\n",
        "1. **`model.summary()`** ‚Üí Layer-by-layer details (shapes, params).\n",
        "2. **`model.get_config()`** ‚Üí A Python dictionary showing the internal configuration of the Sequential model and its layers."
      ],
      "metadata": {
        "id": "KlZIMidl1-sl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}